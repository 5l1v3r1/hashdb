Note: made changes to xbtree to remove compiler errors/warnings.
This will be replaced with a new version of boost btree in the future.

Note: removed use of iterator_facade because it causes many compiler warnings like this:
warning: base class ‘class boost::iterator_facade<burst_manager_map<md5_t, source_lookup_record_t>::manager_iterator, const std::pair<const md5_t, source_lookup_record_t>, boost::forward_traversal_tag, const std::pair<const md5_t, source_lookup_record_t>&, long int>’ has a non-virtual destructor [-Weffc++]
This change helped ensure efficient usage (prefix increment), reduces the call chain, and allows control over what the compiler generates, specifically, operator= and the copy constructor.

Note: 
Removed the dependency on libtoolize by porting libxml++ to libxml2.  libxml++ requires libtoolize because it requires PKG_CHECK_MODULES, requiring the use of AM_PROG_CC_C_O and requiring makefiles to define program_CPPFLAGS and program_LIBS.  Additional side effects are:

1)  the libtoolize comment libtoolize: `AC_PROG_RANLIB' is rendered obsolete by `LT_INIT'.
2)  all the generated .o files are prefixed with the primary name, which is annoying.
3)  Makefiles that need libxml++ require the lines:
          hashdb_CPPFLAGS = $(LIBXMLXX_CFLAGS) $(AM_CPPFLAGS)
          hashdb_LDFLAGS = $(LIBXMLXX_LIBS)
     which makes it so I can't readily compile with flag -lsystem for libxml++, which would make all those libxml++ warnings go away.

Here are some ramifications of this effort:
hashdb_settings_t now includes hashdigest_type.

When parsing DFXML, hashdigest_type and hash_block_size are passed to hashdb_manager.
This way, hashdb_manager can manage all checking and report all failures at the same place.

There were 3 similar DFXML parsers because the action is embedded in the parser:
  Add hash
  remove hash
  hashdb lookup

Now, the action is still embedded, but a consumer callback is used with a Template so there is only one DFXML scanner for all three.

Also, the XML settings parser has been ported to use libxml2, not libxml++.


xbtree modified for Windows port: include winsock2.h before windows.h
to avoid winsock2.h warning.

------------------------------------------------------------
Windows Port:
zmq for mingw is not static, so don't compile with --enable-static --disable-shared.

Later, may change zmq to static.  A code change recommendation is provided at
http://comments.gmane.org/gmane.network.zeromq.devel/17178

In configure.ac, changed boost to use macro boost.m4
because macros ax_boost_base.m4, ax_boost_system.m4, ax_boost_filesystem.m4
failed to work with mingw.  Specifically, ax_boost_system.m4 failed
with "Could not find a version of the library!".

------------------------------------------------------------
Issues with scan_hashid:

--------------------
(1)
These files moved from be13_api to dfxml/src:
    md5.c
    md5.h
    xml.c
    xml.h

The bulk_extractor git archive needs updated to include the dfxml package.
File bulk_extractor/src/Makefile needs to be changed so that these files
are found in dfxml/src instead of in be13_api.


--------------------
2)
I introduced HAVE_HASHID as the switch for generating the hashid scanner
and including the libhashdb library.  This impacts configure.ac
by adding libraries and the Boost library.  This implementation currently is
sloppy and I need to clean it up.

--------------------
3)
<resolved>.
--------------------
4)
<resolved>.

--------------------
5)
I changed usage of "#include <config.h>" to be in .c files and to never be 
in .h files.  I did this because it is inappropriate to have
"#include <config.h>" inside library header files.  In some cases, header
files require definitions that "#include <config.h>" would have provided,
such as WIN32.  The result is that the program including the library is
responsible for providing any definitions that the library needs.

--------------------
6)
When these issues get settled, I will want to push bulk_extractor changes
to github.  Currently I am archiving relavent files in SVN under sectorid2.


------------------------------------------------------------
Change in hashdb interface usage

Here is a kind of pseudocode view of current hashdb interfaces:

For hash lookup request, a vector of {hash id, hash digest} pairs is provided:
  hashes_request {
    vector<{id, digest}>
  }

For hash lookup response, a structure containing hash block size and a vector of {hash id, hash digest, duplicates count, lookup index, hash block offset value} is returned:
  hashes_response {
    hash_block_size;
    vector<{id, digest, count, lookup_index, hash_block_offset_value}>
  }

I like the request and response structures except for the use of variable "hash_block_size".  I provide "hash_block_size" so the user can validate that the hash block size the user expects and the hash block size the database uses match.  The test should be hidden, but the library doesn't know the hash block size the user is using.  I don't want to pass in hash_block_size during instantiation because the reason is obscure.  Also, testing on hash block size alone does not cover potential future checks.

I resolved this by removing the check.  The use must check the xml returned by "get_hashdb_info" interface to determine the hash_block_size.

------------------------------------------------------------
Testing:
Create hashdb of /corp/nps/files/govdocs1/.
Create hashdb of malware/.
Work under /raid/bdallen.
Perform bulk_extractor scan of /corp.
There are two approaches for creating the dfxml from govdocs:
  1) create one massive dfxml from all of govdocs.
  2) create dfxml for each primary directory under govdocs.

Compile on ncr.nps.edu.
Perform testing on r2.ncr.nps.edu.
Note available file r0.ncr.nps.edu /corp/quist/malware/files-4096.xml

Write small test programs for code that breaks BTree.
Known causes:
1) create empty BTree, then check for presence of an entry.
2) Reading using bulk_extractor recursive.

------------------------------------------------------------
Done:

Boost:
Use Boost extraction tool BCP to assemble required headers for tarball and
have autotools choose native Boost over assembled headers.
(bda): No, bcp tool created 1.3MB zip, without libs, so I may not pursue 
       assembled headers unless further directed.
Put back filesystem v3 compatibility for Boost v1.48 onward.
(bda): Done.

Functionality:
Added new scan parameter, sector_size, which is different from hash block size,
so that hashes can be managed on sector boundaries.

Added -x parameter to copy command to remove duplicates.
The implementation may be optimised in the future.

Renamed client request offset to query_id because it is an ID that is
not necessarily bound to an offset.

Usage now shows default values.

Locked the socket map during access so that threads can't break it.

Allow multiple targets:
Compiles to c++98, not just C++11, in order to improve compiler portability.

Added BTree input parameter option for specifying number of hashes
instead of number of hash functions k and hash size M.
Joel and Avner will provide feedback on hashdb usage.
Add new hashdb command for regenerating the Bloom filter
using different Bloom filter parameters.
Write the scan_md5 bulk_extractor plugin.
Help James with client code.
Add runtime timer information.

Improved integration with bulk_extractor.

Allows multiple targets: Linux, Windows, Mac.

Databases are portable across Linux, Windows32/Windows64, Mac.

Added hashdb API query functionality: query_sources_md5 and
query_hashdb_info are fully implemented.

------------------------------------------------------------
We will update the query interface to provide "add hash" services.  The
motivating use-case is to be able to generate whitelist hash values using
bulk_extractor.  Specifically, we will import block hashes of disk image
sectors into a hash database for multiple images, and then merging these
hashes.  We then use hash values that are common to multiple images as
whitelist hash values to be excluded from blacklist data.

Query services will be adjusted and added to to support additional interfaces.
Changes include:

The hashdb API interfaces are incorrectly prefixed with "query_".  Term
query will be renamed to hashdb to properly reflect that it is a hashdb API.

File hashdb.h is already properly named as "hashdb".

Code that dispatches queries to ZMQ are improperly named query*.hpp.  These
files will be renamed to zmq*.hpp and will have new API interfaces added to
support all hashdb access functions, not just queries.  Most significantly,
the ability to add hashes will be supported: "hashdb_add_hash_elements".

Existing code that accesses the hashdb databases is duplicated, once for
server code that services ZMQ requests, and once for query code that
bypasses ZMQ and accesses hashdb databases directly.  This will be changed
so that accesses uniformly go through ZMQ, wheter the server side is local
or remote.  This change needs to be made before more hashdb access functons
are added.

A complication with ZMQ has been managing variable-sizd records because of
strings used for repository names and filenames.  Now that Boost 1.43
has introduced the string_ref class, I will change ZMQ to use this type
for transmitting arrays of records containing string data.

All hashdb interfaces will be changed to not throw events or call
exit.  Specifically, Boost exceptions will be caught and converted to error
codes and any invocations of exit will be replaced by error codes.
Currently, error codes are used, but they only return 0 or 1.

The hashdb_manager tool will be changed to use the hashdb API uniformly
for lookup and for add operations, rather than have code in the manager
that replicates what the hashdb API does.

The hashdb_checker tool will be merged into the hashdb_manager tool
because these tool services are no longer distinct; the hashdb API will
support both read and write servcies.  Note that the hashdb_manager tool
already uses the hashdb API.

------------------------------------------------------------
Here is a summary of design strategies and configurable B-Tree parameters
that impact B-Tree performance:

 * Key size.  The Key size used impacts performance the most because it
   impacts the depth of the tree.  When the key size is small, more
   child elements fit in a node, resulting in a shorter tree.

   An indexed map can be used to minimize Key size, but each indexed map
   lookup is paired with a flat-map lookup of the Key's value, so indexed
   maps cannot have better performance than direct maps.

 * Node size, default 4096.  Larger nodes hold more child nodes, shortening
   the tree, specifically # child nodes = ( node size - overhead ) / Key size.
   Larger node sizes permit shorter trees, but the extra data read is not used;
   less than 4096 bytes of
   information is needed from the node, and reading more than the
   operating system's 4096-byte block size takes time, too.  In fact,
   on a discontiguous block read, the cost of reading a second 4096-byte
   block is equivalent to the cost of reading a node in the next branch of
   the tree.

   In other words, the node size should be the same as the block size used
   by the OS, typically 4096.

   The node size may be selected by passing it in to the btree constructor.

 * Traits: Endian traits are configurable to big, little, or native endian.
   Beman has chosen big-endian traits because this is portable and readable
   and the choice makes no measurable effect on performance.

   The traits used is selected by passing the desired traits class as a
   template parameter to the desired btree type during class instantiation.

 * Preload: Preload to OS file cache.  Enabled by flag in Open call.  This
   does not load the btree cache, but btree can be changed so that it does.

 * Cache branches: enable permanent cache of all branch nodes touched.
   The btree buffer manager will never deallocate nodes that have been read.
   Enabled by flag in Open call.

 * Caching: Amount of caching is selectable based on ratio of file size.
   We will select "fastest".  We may select "least_memory" for testing
   performance.  Selected by flag in Open call.  May also be changed during
   runtime via max_cache_size.

   Unused buffers are deallocated based on an algorithm when buffers
   reach max_cache_size.

 * reserve_default: Bytes to reserve for the flat file that is created when
   using indexed sets and maps.  Not used for sets and maps that are not
   indexed.

Here are optimizing B-Tree design characteristics which are different from
the classical B+ tree:
 * New-node-on-full (lazy) rather than split on full for ordered inserts
   allow 100% packing

 * Free-at-empty (lazy) rather than rebalancing erases.

Implementation optimizations we may want to request:
 * Add function "apply(key, mapped_value)" to insert if key not present,
   or update mapped_value if present.

   We change mapped_value for keys in the hash store when we add or remove
   elements from the hash duplicates store.

What this means for hashdb:
 * May want preload option if opening as socket service well before use.
 * No other user options; btree will be hardcoded for maximum speed and will
   use lots of memory.
 * Always open with "fastest" flag to maximize caching.
 * Do not preload.
 * Use default traits.
 * Use default node size of 4096 bytes.
 * Design with minimal key+value size, which we do.
 * We may request new "apply" interface to enable more efficient performance
   when changing a mapped_value.

------------------------------------------------------------
Hash database Statistics:
The hash database is managed as a B-Tree map indexed by key = hash values.
By iterating over the database by key, we can determine the following:
 * Hash values with the highest repeat counts, for example:

       hash value                            hash duplicate count
       xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx1           40,000
       xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx2           500

 * The number of hases that reflect a given repeat count value, for example:

       hash duplicate count         # hashes with this hash duplicate count
              1                            1,000,000
              2                            50,000
              3                            20,000
              4                            500
              5                            25
              6                            8
              7                            20

A deeper statistical analysis of hash databases may be conducted by using
an indexed multiset B-Tree with an indexed lookup for each field.  Fields are:
 * Hash value
 * Source repository name
 * Source filename
 * Source file offset

Using this, we can generate statistical information about the prevalence
of source repository names and filenames.

An indexed multiset B-Tree would be created by copying a B-Tree map indexed
by key.  The copy can take some time.

It would be nice to use an indexed B-Tree set instead of a B-Tree map,
but this hampers read speed.  So this will be an offline post-processing
capability.

Since it will be offline, it will likely not be part of the hashdb API.
This implies that not all hashdb services will be wrapped into the hashdb API.

------------------------------------------------------------
Obscure boost::btree errors from programming errors:
It would be nice if btree had more checks during instantiation in order
to help with programming errors.  Currently, misuse results in segmentation
faults from deep in Boost.  These errors are from my mistakes:
1) Close after creating and modifying a new btree.  I failed to call delete
after new.  After next open, read crashed.
2) When opening an index file, pass index_by_key(file), not key_file.
The compiler allows this because the method prototype is available.  But it
is the wrong one.  I don't know if a check is possible for this error.

------------------------------------------------------------
Compiler warnings
 * bloom filter generates warnings so they are disabled with the -isystem
   compiler flag.
 * btree code generates warnings so warnings are disabled with the -isystem
   compiler flag.  A comparison warning still shows even with warnings
   disabled.

At some point, these warnings should be examined.

------------------------------------------------------------
Tagging a version and Uploading to digitalcorpora
 * $ git tag -a v0.9.1 -m "v0.9.1
   $ git push origin --tags

 * Upload to digitalcorpora.org/downloads/hashdb
   at ncr:/var/www/digitalcorpora/downloads/hashdb

