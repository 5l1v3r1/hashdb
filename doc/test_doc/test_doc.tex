
%\documentclass[10pt,twoside,twocolumn]{article}
\documentclass[12pt,twoside]{article}
\usepackage[bf,small]{caption}
\usepackage[letterpaper,hmargin=1in,vmargin=1in]{geometry}
\usepackage{paralist} % comapctitem, compactdesc, compactenum
\usepackage{titlesec}
\usepackage{titletoc}
\usepackage{times}
\usepackage{hyperref}
\usepackage{algorithmic}
\usepackage{graphicx}
\graphicspath{{./graphics/}}
\usepackage{xspace}
\usepackage{verbatim}
\usepackage{url}
\usepackage{float}
\hyphenation{Sub-Bytes Shift-Rows Mix-Col-umns Add-Round-Key}

\setlength{\parskip}{12pt}
\setlength{\parindent}{0pt}

\newcommand{\hdb}{\emph{hashdb}\xspace}
\newcommand{\bulk}{\emph{bulk\_extractor}\xspace}
\newcommand{\mdd}{\emph{md5deep}\xspace}
\newcommand{\bev}{\emph{Bulk Extractor Viewer}\xspace}

\begin{document}

\begin{center}
\Large \hdb System Requirements Specification
\large Working Draft
\end{center}


1 Introduction
1.1 Document Purpose
This document describes the HashDB software and the testing effort being requested to support it through its lifecycle.  The requirements specified are primarily for testing of the software in specific areas.  The scope of the effort may be reduced or extended based on need and when approved by STIL Director.
This document can be updated over the course of the effort as the requirements evolve form the initial set.
1.2 Background
1.2.1 Introduction
The HashDB software provides a different approach to File Type Identification and File Similarity/Content Similarity through the use of Sector Level Hashing.  An overview of the effort can be found in the presentation by Michael McCarrin given at STIL on June 26, 2014.   


The approach to sector hashing provides additional capabilities to detect and identify similar content even in scenarios where the digital objects being compared have been modified such that not all sectors are identical.  By looking at sectors and matching against a known signature, the presence of known target material can be extrapolated even when alterations to some sectors or sector overwrites cause the file-level hashes to differ.

At its core, HashDB is a C++ application that creates a database using simple data structures (primarily a B-Tree and a Bloom Filter).  The minimal design permits a small storage footprint and very fast lookups without the overhead of running a full relational database.
1.2.2 System Vision
The overall vision of this effort would be to eventually incorporate this technology for forensic exploitation at various levels from initial triage to downstream analytics either as a stand-alone project or integrated with existing frameworks.
1.3 References
The following documents were used and meetings attended to provide the material for these requirements:
See section 1.2.1 for imbedded presentation
1 Documents
Most of the prior documentation was reviewed as input to this phase.  The key documents used are:
Imbedded Presentation
1.3.1 Meetings
June 26, 2014 IPR for STIL - Briefing
Meeting between Kamal Husain and Michael McCarrin on July 9th, 2014
1.4 Assumptions
The following lists enumerate the assumptions on which the requirement set is based.
Only testing of specific Test Scenarios will be performed as documented in this or related documents (Test Plan) and upon approval
Testing of new releases will be scheduled in advance and no ad-hoc testing is expected
Software will be provided with install and testing instructions
All testing will be performed at the STIL and reported back to customer and developer
1.5 Constraints
The following list enumerates the constraints which the requirement set must consider.
No Constraints identified
2 Functional Requirements
Below are the requirements for the HashDB Testing and Evaluation effort.
1 UI Look and Feel
Req #
Requirement / Allowed Function

Not Applicable
2.1 Functionality to implement
Req #
Requirement/Allowed Function

Not Applicable









3 System Requirements
These will be similar to the Functional Requirement
1 User Interfaces
Functional only at this time
Req #
Requirement

Same As Functional – No Business Specific Requirements at this time.


3.1 Integration
Integration requirements address how the product/tool integrates with other technologies.
Req #
Requirement

NONE AT THIS TIME – MAY BE ADDED LATER


3.2 Audit Trail
All actions that modify a hash database are appended to the history.xml file in the hash database.  Audit information includes the computer the test is run on, how hashdb was compiled, the command line used, and timing information (see history.xml).
Req #
Requirement

NONE
3.3 Reliability
Reliability is the degree to which the system executes the application software correctly without terminating abnormally or corrupting information or other processes.  Poor reliability is evident when a system is off-line frequently or is unavailable for long periods of time. 
Req #
Requirement

NONE
3.4 Scalability
The ability of the application to continue to function effectively as the quantity of queried data increases. With respect to hashdb, queried data will consist of hash/metadata pairs, where the hash is the md5 hash of a sector and the metadata identifies the original context of the sector.
Req #
Requirement

NONE
3.5 Security

Req #
Requirement

NONE



4 Evaluation Requirements
1 Evaluation Methodology
Testing will evaluate performance, reliability, accuracy, precision and recall.

HashDB operates in two modes: import and scan. 

In import mode the hashes are inserted into a database. This operation assumes a preprocessing step in which the hashes are produced from the target media and either stored in an intermediate dfxml file or inserted on-the-fly. fiwalk and md5deep can generate appropriate dfxml; bulk extractor’s hashdb scanner can pass hashes to a database on-the-fly.
Scan is the process of querying a reference database for matches against a set up hashes. The query set can be stored in dfxml or generated on-the-fly by the bulk-extractor hashdb scanner.
Performance will be measured separately for each mode by monitoring both memory consumed and time elapsed. Generally, on-the-fly processing with bulk extractor’s hashdb scanner is expected to be faster and more space-efficient. 
Additional factors of note are the size of the database on disk and the available resources (memory, processing cores) on the test machine. 
It is intended that data ingestion will occur on high-performance machines with fewer resource constraints, whereas scanning may be performed using commodity hardware at point of collection.

Req #
Requirement/Allowed Function
E1
Testing speed and reliability of import mode as database size increases
E1.A
Test importing hashes into an Empty Hash Database from dfxml.  This may be a common use case since small databases can be built individually and then added to a master database using the merge operation.  Recommended methodology is to create a dfxml file containing the sector hashes from allocated files on disk images using fiwalk, then import the dfxml file.
E1.B
Test importing hashes into an Empty Hash Database with the bulk extractor hashdb scanner.  This may be a common use case since small databases can be built individually and then added to a master database using the merge operation.  Recommended methodology is to mount the disk image and run bulk extractor recursively on the root of the mounted file systems with the scanner enabled and set to import mode. The database produced by this method should be identical to the ones produced in E1.A, provided the scanner is run with the –E hashdb and –S hashdb_ignore_empty_blocks=NO option.
E1.C
Test importing hashes from dfxml into an Existing Hash Database.  This will test how well the system ingests when already populated. Recommended testing method is to create a single database containing all hashes from allocated files on all available test images using fiwalk.
How does database size affect ingestion speed?
How does database size affect memory consumed during ingestion?
How is the performance affected when the database size exceeds available memory?

We will not be testing for Deduplication or insertion failures, only how long the process takes to complete and if there is any way to use the results to extrapolate to average number of disk images that can be managed in a single database.

We will report on the following metrics:
Time to import hashes as database size increases.
Memory consumed during import process as database size increases.
Time to import hashes for databases that require more than the available memory.
Any run-time failures of the application.
Any space/size metrics which can be collected and stored on the test system
All error messages produced by the software or OS if they can be collected or are accessible in a single area.  If not, an image or the file can be provided for review to the development team

If the software provides a log of all transactions, it will be provided to the development team for review.
E1.D
Repeat above, importing from bulk extractor hashdb scanner instead of a pre-generated dfxml file.
E1.E
How is performance affected depending on Hardware
Test the import process on the Mercury Server 
This will provide the development team with metrics to compare against their own testing on their commodity hardware where the software is currently being tested. Expected use of the import mode is on high-end machines.
Test the import process on Windows using 
E1.F
Regression testing – Make sure the two import methods yield the same results. Check that the Windows and Linux versions yield the same results and produce compatible databases (i.e. you can create a database with one version and add to it with the other). Change the order of the imports and make sure the result is the same.

The hashdb merge and subtract functions may be useful in this context.  


E2
Scan mode Testing
E2.A
Test to see how well the scan mode performs depending on the size of the database, the frequency of hits against the database, and the available resources on the test machine.

Baseline sizes for the reference databases should be on the order of 10 million, 100 million and 1 billion hashes. Reference databases will be built from allocated files as described in E1.

The recommended testing process is to run the bulk extractor hashdb scanner in “scan” mode on disk images containing files from which the reference database was created. For example, if the reference database was created from hashes od allocated files stored on disk images A-Z, scan mode could be tested by using the E01 files from image A as input to bulk extractor and querying against the database.

By default, the bulk extractor scanner will hash a sliding window of 4096 bytes that advances in 512 byte increments. This behavior is designed to account for all possible alignments of the files; a consequence is that only 1 one out of 8 hashes is expected to match the reference database.

We will report on the following metrics:
Time to scan an image that is known to contain files that have been hashed and inserted in reference databases of the 3 sizes specified.
Time to scan an image that is known to contain no matching files against reference databases of the 3 sizes specified.
Any run time failures of the application
Any space/size metrics which can be collected and stored on the test system
All error messages produced by the software or OS if they can be collected or are accessible in a single area.  If not, an image or the file can be provided for review to the development team

If the software provides a log of all transactions, it will be provided to the development team for review. This will include all identified_blocks.txt files produced by the bulk extractor scanner.

E2.B
How is scan mode affected by hardware/OS – 
Testing can be on the Mercury Server; however testing scan mode on commodity systems is preferred.
This will provide the development team with metrics to compare against their own testing on their commodity hardware where the software is currently being tested.
The amount of memory on the machine, use of local ssds vs spinning disk drives, and cores on the machine are all relevant for testing speed.
Comparison of the speed of the Windows vs. the Linux versions of hashdb and the bulk extractor hashdb scanner is also of interest.

Regression testing – Make sure the Windows and Linux versions yield the same results (i.e. same bulk extractor “identified blocks” output files). 
E3
Additional Testing requested by M. McCarrin on White List Creation and Accuracy.  Will need further elaboration on this to formulate the appropriate test cases. This may be added later.




E4
Integration Testing – May be added later
E5
Accuracy/Precision/Recall Testing
E5.A
Test the accuracy, precision and recall of hashdb when used to query a disk for the presence of 4096 byte blocks that are stored in a reference database.

Recommended testing procedure is to build a reference database from the allocated files on various disk images and then scan the disk images against the database to ensure that all files are correctly identified. This can be performed simultaneously with the scan mode performance tests described in E2.

Precision, accuracy and recall scores will be provided, in addition to relevant identified_blocks.txt files produced during the bulk extractor scan operation.
E5.B
Repeat the above-described test for both Windows and Linux versions of the tool.

5 Appendix A –Acronyms

Acronym / Initials
Definition
























\end{document}

