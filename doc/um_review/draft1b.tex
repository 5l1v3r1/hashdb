% To build, please type: pdflatex draft1.tex

\documentclass[12pt,twoside]{article}
\usepackage[bf,small]{caption}
\usepackage[letterpaper,hmargin=1in,vmargin=1in]{geometry}
\usepackage{paralist} % comapctitem, compactdesc, compactenum
\usepackage{titlesec}
\usepackage{titletoc}
\usepackage{times}
\usepackage{hyperref}
\usepackage{algorithmic}
\usepackage{graphicx}
\graphicspath{{./graphics/}}
\usepackage{xspace}
\usepackage{verbatim}
\usepackage{url}
\usepackage{float}
\hyphenation{Sub-Bytes Shift-Rows Mix-Col-umns Add-Round-Key}

\setlength{\parskip}{12pt}
\setlength{\parindent}{0pt}

\newcommand{\hdb}{\emph{hashdb}\xspace}
\newcommand{\libhdb}{\emph{libhashdb}\xspace}
\newcommand{\bulk}{\emph{bulk\_extractor}\xspace}
\newcommand{\hashid}{\emph{hashid}\xspace}
\newcommand{\hid}{\emph{hashid}\xspace}
\newcommand{\mdd}{\emph{md5deep}\xspace}
\newcommand{\bev}{\emph{Bulk Extractor Viewer}\xspace}

\begin{document}
This is a continuation of draft1.

\subsubsection*{3.2 Run \hdb tools}
\subsubsection*{3.2.1 Creating a DFXML file}

There was confusion that \texttt{demo\_video.mp4} was an instructional
video rather than just user data in the form of a video file.
Also, I want to show the \mdd recursive directory option.
\begin{compactitem}
\item Please rename \texttt{demo\_video.mp4}
to \texttt{mock\_files/mock\_video.mp4},
\texttt{demo\_video\_db} to \texttt{mock\_files\_db},
and \texttt{demo\_video\_dfxml} to \texttt{mock\_files\_dfxml}.
\item Please use:
\begin{verbatim}
md5deep -p 4096 -d -r mock_files
\end{verbatim}
and describe: Enables recursive mode. All subdirectories are traversed.
\end{compactitem}
I redid the demos at
\url{http://digitalcorpora.org/downloads/hashdb/demo/demo\_video\_dfxml}
to match this.

Table 1, last option: I changed \texttt{-i} to \texttt{-b}.

For the example of creating a new database,
I like that you show the list of files; it shows what people can expect
to be there.
As they populate it, some file sizes will grow.
But for run-time heuristics, please direct them to feedback
such as \texttt{hashes inserted=2595} or the \texttt{get\_statistics}
command that I haven't written yet.

About \texttt{identified\_blocks.txt}:
The first field (not number) is the Forensic Path
(not just offset of the sector).
The first field of this and all feature files that \bulk generates
is the Forensic Path.
The Forensic Path is wonderful because in addition to pointing
to a simple byte offset into a media,
it can point to deeply nested content.
For example \texttt{500-ZIP-4096} points at the feature 4096 bytes into
un-zipped extracted content found 500 bytes into the media image.
It is a big deal that the \hid scanner
can recognize a matching block embedded in part of another file.
No other tool can do this.
Other tools can only find complete un-embedded files.

The third column is not the number of times the hash was found in the data.
It is the number of times the particular hash value has been added to the
hash database.
It is a hash duplicates count.
Hash duplicates happen when the hash value is the same
but any of the source information is unique.
Source information is the repository name, filename, and file offset.

Please add another section for finding the source information
associated with the hash values in \texttt{identified\_blocks.txt}
that were found.
Use command \texttt{expand\_identified\_blocks}
to reveal the repository name, filename, and file offsets of sources
containing these hash values, as shown in the second part of the demo at
\url{http://digitalcorpora.org/downloads/hashdb/demo/scanner\_demo.pdf}.

Users may be put off by the quantity of matches incurred
by low-entropy data in their databases
such as blocks of zeros or metadata header blocks from files
that are otherwise unique.
A lot of study will go into managing this.
\hdb provides commands for this:
\begin{compactitem}
\item Use the \texttt{subtract} command to remove known whitelist data
created from sources such as benign operating system images and the NSRL.
\item Alternatively, use the \texttt{deduplicate} command
to remove all hash values that have been imported more than once.
\end{compactitem}

\subsubsection*{4.1 Updating Hash Databases}
Copy, edit, append, and merge do not indicate unique actions.
How about just ``manipulate the contents of hash databases''.
Set operators are provided in the form of add, subtract, and intersect.

Table 2: Rename \texttt{copy} to \texttt{add}.
Also, please add missing entry \texttt{import}.

The tables do not cover all commands.
Should there be another table for the rest?

\subsubsection*{4.2 Querying for Source or Databased Information}
Please clarify: \texttt{scan} searches for hash blocks in a DFXML file
that match hash blocks in a database.
The DFXML file can come from something \mdd created or from a database
\hdb exported via \texttt{export}.
This may be an unpopular command used mostly for testing.
But please keep the example use case, I like it.

Table 3 typo: scan, not copy, in scan command, usage column.

\subsubsection*{4.2 Querying a Remote Hash Database}
The scope of \hdb library services changed dramatically from 0.9.0 to 1.0.0
and hence the capabilities provided by sockets, which use the \hdb library.
As a result, we do not provide lots of services via sockets.
Users are now expected to copy databases between computers and perform
database change operations locally.

The only socket service provided is for scanning.
The \hdb \texttt{scan} command accepts \texttt{<path or socket>},
the \hdb library constructor for scanning accepts \texttt{<path or socket>},
and the \bulk \hid scanner accepts \texttt{<path or socket>}
when in scan mode.

I see that the examples and \bulk \hid scanner text in \hdb usage
is terribly dated, sorry about that.
Please take updated usage from here:
\url{http://digitalcorpora.org/downloads/hashdb/hashdb\_usage.txt}.
Thanks.

\subsubsection*{4.3 Writing Software to Query a Hash Database}
\subsubsection*{4.3.1 Scanning or Importing to a Database using \bulk}
Please update \hid usage per corrected usage.

\subsubsection*{4.4 Optimizing a Hash Database}
Please recommend using the \texttt{btree} storage type
unless performance is slow,
then consider copying the database to one created
using the \texttt{unordered-hash}
storage type if the database is expected to not be written to again.

Bloom Filter:
For large databases, it takes a bit of time to look up a hash value
to determine whether it is in the database.
This time adds up when scanning for millions of hash values.
As an optimization, we provide Bloom filters.
\hdb scans first quickly consult a Bloom filter to know if a hash value
is not in the hash database.
If it is not, great, move on.
But Bloom filters that are too small fill up,
and then too often they give false positives
that indicate that the hash value might be in the database.
Then we have to check the database just to be sure.
No more performance gain.

We can't just indiscriminately make giant Bloom filters, though,
because they take up lots of memory and disk space.
We tune them to fit a balance between being helpful and getting in the way.

The optimal configuration for your Bloom filter
depends on the size of your dataset.
Although several tuning controls are available, we recommend just using
\texttt{bloom1\_n <n>}, where \texttt{n} is the expected number of hashes
in your dataset.

The default setting is good for about 45,000,000 hashes
and takes about 268MB of space.


\subsubsection*{Worked Examples}
All worked examples should run fine, so you should be able to finish them now.
Specifically: Use one process for \bulk.
The \texttt{intersect} command has been fixed.


\end{document}

