		Announcing hashdb 1.0.1
		        <DATE>

hashdb Version 1.0.1 has been released for Linux, MacOS and Windows.

Major improvements
==================

Bug Fixes
=========
* Improve error detection and reporting in the event of invalid input
    * Discontinued command options -t, -a, and -b are removed.  Use of these options now provide an error rather than quietly disregarding their use.
    * Improve wording for the change report written to stdout.
    * Detect when the database configuration is different from the hashdb configuration, and fail gracefully.
    * Detect when multiple databases being referenced are not compatible with each other, specifically, if their hash block size is different or if the databases is the same one.
    * Prevent creation of a database when the hash block size is invalid.
* Fix history log bug in subtract command where change was incorrectly logged to hashdb2 instead of hashdb3.
* Fix so test suite builds for Windows.
* Fix Windows bug where the history and setting xml file failed to back up.  The failure was that on Windows, std::rename fails to rename to target if a file is at the target path.
* Fix the command test so that it does not add a database to itself.  This is an error condition and it can corrupt the database.
* Add more validation checks to the commands test set.
* Add the Users Manual to the distribution.
* Reorganize and add content to the Users Manual.
* Organize the commands in the usage text into categories, and fix and clarify usage language.
* Set endian module to revision sha1=05ac935 since the newer endian module is not compatible with available Boost versions.
* Improve usability of the deduplicate command by creating the target database if it is not already there.
* Improve parsability of output from the expand_identified_blocks command by changing space to underscore.
* Improve parsability of output from the scan_expanded command by changing space to underscore.
* Remove compiler warning generated by the clang compiler.
* Improve the import command performance by importing hashes as they are read from the DFXML file rather than buffering them all before importing them.  This change also improves runtime progress feedback.
* Fix for compatibility with CentOS 6.4 which uses C++ v4.4.7.

Bug Fixes (in progress)
=============================
* Check Boost RO attach and find and remove the single-attach database limitation.

Functional Change: Tracking the File Size (in progress)
=======================================================
Add capability to track the file size so we can know the size of the source file
that a hash belongs to.

Changes:
* Add a file size field to the source database.  The source database lookup shall contain: repository name index, filename index, file size.
* Update the import command to set the file size from the filesize fileld that is inside the fileobject tag and adjacent to the filename tag.
* Update the export command to additionally export the filesize field.
Only export the filesize field for the zero-block entry since it is only needed once.
* To the hashdb API, file hashdb.hpp, add interface for querying the file size:
    get_file_size(repository_name, filename).
* Change the bulk_extractor hashdb scanner so that
when importing hashes via the hashdb scanner, also gather the file size iniformation from information available in the scanner params object.
* The source data will be versioned by either incrementing the hashdb database version number or by adding a separate versioning scheme specific to the source data.

Functional Change: Improve identified_blocks.txt output (in progress)
=====================================================================
Data in the identified_blocks.txt file generated by the bulk_extractor hashdb scanner is incomplete in that it lacks information about where identified blocks were sourced from.  Currently, this data is obtained by post-processing the identified_blocks.txt file.
The change is to include the source information during the scan.
For each hash match, have the hashdb API return every matched source entry, not just a count indicating how many matches were found.  For compatibility, continue to return the match count value.
The identified_blocks.txt file feature information will include the following:
offset, tab, hash, tab, repository name, filename, file offset.
Associated changes:
* Remove support for scanning for hashes through a network socket since this change would impact the data transfer design.  It is simpler to remove socket support than to work with it.
* Remove command
    expand_identified_blocks <hashdb> <identified blocks file> > destfile
since it will be obsoleted.

Functional Change: Allow hash storage for blocks of any size
================================================
A use case exists where we track hashes generated from blocks of any size.
This use case accepts hashes generated from blocks of any size.
This use case does not track a file offset.

Changes:
* Allow a block partition size of zero in order to enable blocks of any size.
Specifically, allow option -p 0 when creating a hash database.
* Use zero for the file offset value when the partition size is configured to be zero.

Functional Change: Improve statistical analysis capability (in progress)
======================================
The following functional changes add statistical analysis capability:

* Add command:
    scan_hash <hashdb> <hash value>
to print out the repository name, filename, and file offset for each source that matches the given hash.
* Replace statistics command:
    hash_table <hashdb>
which prints the entire hash table, which is too much information, with
    hash_table <hashdb> <filename>
which prints the entire hash table for the specified file.
Multiple files will be shown if there are multiple repositories.
* Add command:
    add_repository <source hashdb> <destination hashdb> <repository name>
to allow adding the hashes specific to a selected repository.
* Add command:
    subtract_exact <source hashdb 1> <source hashdb 2> <destination hashdb>
to create a destination database that does not contain hashes where the repository name and filename exactly match.
Note: I thought the subtract_exact capability was available, but it was not; just subtract was available.  subtract_exact is being added to allow undo capability.
Note: source filenames are not removed during the subtract operation.  To remove them, a copy is still required.
* Add command:
    intersect_exact <source hashdb 1> <source hashdb 2> <destination hashdb>
to find intersection of exact match.  The repository name and filename must exactly match.  This capability was missing.

Changes to bulk_extractor hashdb scanner (in progress)
======================================================
* Make BEViewer find Image at relative path on Windows systems.
* Force file list in BEViewer Reports tree to refresh to accomodate showing new identified_blocks_expanded.txt file that hashdb can crate.
* Change BEViewer GUI to permit easy copy of filename so it can be pasted elsewhere.
* Make BEViewer print whole feature line for identified_blocks_expanded.txt similar to how it is printed for identified_blocks.txt.
* To avoid incorrectly managing hash values, we may want to make the md5 of the image bulk_extractor is hashing visible to the scanner so that each feature in identified_blocks.txt includes this file hash value.
* Zero-byte extend file blocks when ingesting files recursively via bulk_extractor.
* Make BEViewer allow copy of the file field.  Currently, this is a text field that cannot be selected or copied.
* Remove the Byte Order Marking from the beginning of the identified_blocks.txt feature file.
* Remove code marker in feature file that shows up next to the filename of recursively scanned files.

Potential future changes to hashdb
==================================
* As a speed performance optimization, presort hashes before adding them.
* Replace Bloom filter with a trie data structure.  Quote from email 09/20, 07:05:
1. Use a 1-level trie.  For example, a 2^24-ary trie that takes the first 3 bytes of the MD5 as an index to the leaf.  This is essentially an array of pointers.
2. Leaves are sorted arrays of the next few bytes of the MD5 (let us use 2 bytes as an example)
3. Search the arrays using a secant search, which is like a binary search but with interpolation to reduce the number of cache line reads.
 
If storing a billion hashes in this filter, there will be 2 bytes of data stored per hash (2GB), 2^24 64-bit pointers in the trie node (128 MB), and some overhead in the allocation efficiency.  However, you will effectively get 5 bytes of hash matching, where the equivalent bloom filter would require 2^40 bits or 128GB of memory.

Potential future changes to bulk_extractor
==========================================
* Add code in the finalization state of the hashdb scanner for generating post-processing statistics:
    * Print files that the image fully contained.
    * For partial matches, print number of blocks out of total blocks matched for each file.

Availability
============
* Release source code: http://digitalcorpora.org/downloads/hashdb/
* Windows installer: http://digitalcorpora.org/downloads/hashdb/
* GIT repository: https://github.com/simsong/hashdb

Contacts
========
* Developer: mailto:bdallen@nps.edu
* Bulk Extractor Users Group: http://groups.google.com/group/bulk_extractor-users

