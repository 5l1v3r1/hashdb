                     Announcing hashdb 1.0.1
                              <DATE>

                          RELEASE NOTES

hashdb Version 1.0.1 has been released for Linux, MacOS and Windows.

Release source code and Windows installer: http://digitalcorpora.org/downloads/bulk_extractor/

GIT repository: https://github.com/simsong/bulk_extractor

#Major improvements

#Bug Fixes

* Improve error detection and reporting in the event of invalid input
    * Discontinued command options -t, -a, and -b are removed.  Use of these options now provide an error rather than quietly disregarding their use.
    * Improve wording for the change report written to `stdout`.
    * Detect when the database configuration is different from the hashdb configuration and fail gracefully.
    * Detect when multiple databases being referenced are not compatible with each other, specifically, detect if their hash block size is different or if the databases specified are the same database.
    * Prevent creation of a database when the hash block size requested is invalid.
* Fix history log bug in subtract command where change was incorrectly logged to hashdb2 instead of hashdb3.
* Fix so test suite builds for Windows.
* Fix Windows bug where the history and setting xml file failed to back up.  The failure was that on Windows, std::rename fails to rename to target if a file is at the target path.
* Fix the command test so that it does not add a database to itself.  This is an error condition and it can corrupt the database.
* Add more validation checks to the commands test set.
* Add the Users Manual to the distribution.
* Reorganize and add content to the Users Manual.
* Organize the commands in the usage text into categories, and fix and clarify usage language.
* Set endian module to revision sha1=05ac935 since the newer endian module is not compatible with available Boost versions.
* Improve usability of the deduplicate command by creating the target database if it is not already there.
* Improve parsability of output from the expand_identified_blocks command by changing space to underscore.
* Improve parsability of output from the scan_expanded command by changing space to underscore.
* Remove compiler warning generated by the clang compiler.
* Improve the import command performance by importing hashes as they are read from the DFXML file rather than buffering them all before importing them.  This change also improves runtime progress feedback.
* Fix for compatibility with CentOS 6.4 which uses C++ v4.4.7.

# Bug Fixes (in progress)

* Check Boost RO attach and find and remove the single-attach database limitation.

# Functional Change: Tracking the File Size (in progress)

Add capability to track the file size so we can know the size of the source file
that a hash belongs to.

Changes:

* Add a file size field to the source database.  The source database lookup shall contain: repository name index, filename index, and file size.
* Update the import command to capture the file size from the DFXML input.  The file size field captured is located inside the fileobject tag adjacent to the filename tag.
* Update the export command to additionally export the filesize field.
Only export the filesize field for the zero-block entry since it only needs to be specified once.
* Update output of scans to include file size information.
* Change the _bulk\_extractor_ hashdb scanner so that
when importing hashes via the _hashdb_ scanner, it will gather file size iniformation.  It will obtain the file size from information available in `sbuf.pagesize` and will forward it to _hashdb_.
* The hash database source data will be versioned to preserve backward compatibility.  Versioning will be managed by either incrementing the hashdb database version number or by adding a separate versioning scheme specific to the source data.

# Functional Change: Improve identified_blocks.txt output (in progress)

Data in the `identified_blocks.txt` file generated by the _bulk\_extractor_  _hashdb_ scanner is incomplete in that it lacks information about where the identified blocks were sourced from.  Currently, this data is obtained by post-processing the identified_blocks.txt file.  The change is for `identified_blocks.txt` to contain this information.

Change:

* Include the source information in the `identified_bloccks.txt` output during the scan.
For each hash match, have the _hashdb_ API return every matched source entry, not just a count indicating how many matches were found.  For compatibility, continue to return the match count value.
The `identified_blocks.txt` feature file information will include the following information:
offset, hash, repository name, filename, and file offset.
* Remove support for scanning for hashes through a network socket.  The change to return each source impacts data transfer design, and it is simpler to remove socket support than to change it.
* Remove the `server` command since socket support is removed.
* Remove the `expand_identified_blocks <hashdb> <identified blocks file> > destfile` command
since it will be obsoleted.

# Functional Change: Modify Socket Support (in progress)
* Change the internal response data structure so that instead of expecting match records with counts, it will expect complete source information for each match.

Functional Change: Allow hash storage for blocks of any size (in progress)
================================================
A use case exists where we want to track hashes generated from blocks of any size.
The change is to have a mode that accepts hashes generated from blocks of any size.
This mode does not track file offsets assoociated with hashes.

Changes:

* Allow a block partition size of zero in order to enable blocks of any size.
Specifically, allow option -p 0 when creating a hash database.
* have the _hashdb_ `hashdb_manager` code allow hash blocks of any size when the hash block size is configured to be zero.

# Add New Database Use Case (in progress)
* Change the `create` command to accept `-p 0` as a parameter value.  When 0 is used:
 * _hashdb_ accepts block sizes of any value, otherwise, values of mismatched block size are not used.

# Add Import/Export Information (in progress)
File sizes will be managed during import and export.  To support this:

* A file size field in the source data data structure is added to hold the file size.
* The DFXML import parser will parse the file size from the filesize tag under the fileobject tag and next to the filename tag.
* The DFXML export function will include the filesize tag and file size information.  This will be provided only for the first entry where the file offset value is zero.

# Fix Database Manipulation Deficiencies (in progress)
* Add command `add_repository <source hashdb> <destination hashdb> <repository name>`
to allow adding the hashes specific to a selected repository.
* Change command `subtract <A.hdb> <B.hdb> <C.hdb>` to add A but not B into C instead of adding A but not hashes in B into C, which was not symmetric with the add command.  It was a functional deficiency for this command to be missing.
* Add command `subtract_hash <A.hdb> <B.hdb> <C.hdb>` to add A but not hashes in B into C.
* Change command `intersect <source hashdb 1> <source hashdb 2> <destination hashdb>` to find the intersection of exact matches, not just the intersection of hash values.  This capability was previously missing.
* Add command `intersect_hash` to find the intersection of hash value matches.  The repository name and filename must exactly match.

# Fix Scan Service Interfaces (in progress)
* Change the `scan <path_or_socket> <DFXML file>` command to `scan <hashdb.hdb> <DFXML file>`.
 * Change `path_or_socket` to `hashdb.hdb` since socket is discontinued.
 * Change output to include the repository name, filename, and file offset for each source that matches the given hash since this provides a more complete report.
 * Add file size information to the output since hashdb is changed to additionally track the file size.
* Remove the `scan_expanded` command since it is replaced by the new `scan` command.
* Add command `scan_hash <hashdb.hdb> <hash value>` to allow capability to scan for a specific hash value.
* Remove command `expand_identified_blocks <hashdb.hdb>`.  This command is obsoleted since the _bulk\_extractor_ _hashdb_ scanner now produces this information.

# Improve Statistics interfaces (in progress)
Existing capabilities for analyzing hash databases are too limited.
The following functional changes enable additional support for statistical analysis:

* Replace statistics command `hash_table <hashdb>`
which prints the entire hash table, which is unworkably large, with `hash_table <hashdb> <filename>`,
which prints the entire hash table for the specified file.
Hashes from multiple files will be shown if hashes from the same filename have been added from multiple repositories.
* Add a `-q` option for quiet mode, to suppress status output for statistics commands that take a long time to run.  An example line of status output is: `Processing index 100000 of 5644399...`.

# Changes to _BEViewer_ (in progress)
* Make _BEViewer_ find Image at relative path on Windows systems.
* Force file list in _BEViewer_ Reports tree to refresh to accomodate showing new `identified_blocks_expanded.txt` file that hashdb can crate.
* Change _BEViewer_ GUI to permit easy copy of filename so it can be pasted elsewhere.
* Make _BEViewer_ print whole feature line for `identified_blocks_expanded.txt` similar to how it is printed for `identified_blocks.txt`.

# Changes to _bulk\_extractor_ _hashdb_ scanner (in progress)
* To avoid incorrectly managing hash values, we may want to make the md5 of the image bulk\_extractor is hashing visible to the scanner so that each feature in `identified_blocks.txt` includes this file hash value.
* Zero-byte extend file blocks when ingesting files recursively via bulk\_extractor.
* Remove the Byte Order Marking (BOM) from the beginning of the `identified_blocks.txt` feature file.
* Remove the code marker separator that incorrectly shows up next to the filename of recursively scanned files in feature file `identified_blocks.txt`.
* Limit the maximum number of lines allowed for the identified_blocks.txt file in order to bound its growth.
* Permit simultaneous import and scan operations.

#Potential future changes to hashdb
* As a speed performance optimization, presort hashes before adding them.
* Replace Bloom filter with a trie data structure.  Quote from email 09/20, 07:05:
 1. Use a 1-level trie.  For example, a 2^24-ary trie that takes the first 3 bytes of the MD5 as an index to the leaf.  This is essentially an array of pointers.
 2. Leaves are sorted arrays of the next few bytes of the MD5 (let us use 2 bytes as an example)
 3. Search the arrays using a secant search, which is like a binary search but with interpolation to reduce the number of cache line reads.
 
 If storing a billion hashes in this filter, there will be 2 bytes of data stored per hash (2GB), 2^24 64-bit pointers in the trie node (128 MB), and some overhead in the allocation efficiency.  However, you will effectively get 5 bytes of hash matching, where the equivalent bloom filter would require 2^40 bits or 128GB of memory.
* Allow filename globbing for the `hash table [-q] <hashdb.hdb> <filename>` command.
* Add a B-Tree tuning hint least\_memory or all hint possibilities settable when the database is created.
* Possibly add flush at intervals.  Peformance analysis for flush has not been tested.
* Document how 0.17 was calculated as the number for calculating M from n in the Bloom filter, and document the percent false positive range this provides.

#Potential future changes to bulk_extractor
* Add code in the finalization state of the hashdb scanner for generating post-processing statistics:
 * Print files that the image fully contained.
 * For partial matches, print number of blocks out of total blocks matched for each file.

# Alternative Design Considerations
* The ideal k for the False Positive Rate (FPR) is about 6-7, but it does not make that big a difference in the FPR and using k=3 cuts down on the number of random accesses (i.e. 3 cache line loads instead of 6).
* An ideal bloom filter uses 1.44 log2(1/FPR) bits per key whereas the theoretical minimum for any data structure is log2(1/FPR).  A 1-level trie structure has redundancy in each of the buckets for a total space of about log2(average leaf node size)+log2(1/FPR) per key.  The overhead could be removed if we were to implement a binary trie in each leaf node, but the structure has too high of an access time and too high of development complexity to be useful.

Availability
============
* Release source code: http://digitalcorpora.org/downloads/hashdb/
* Windows installer: http://digitalcorpora.org/downloads/hashdb/
* GIT repository: https://github.com/simsong/hashdb

Contacts
========
* Developer: mailto:bdallen@nps.edu
* Bulk Extractor Users Group: http://groups.google.com/group/bulk_extractor-users
 
