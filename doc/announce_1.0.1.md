                     Announcing hashdb 1.0.1
                              <DATE>

                          RELEASE NOTES

hashdb Version 1.0.1 has been released for Linux, MacOS and Windows.

Release source code and Windows installer: http://digitalcorpora.org/downloads/bulk_extractor/

GIT repository: https://github.com/simsong/bulk_extractor

#Major improvements

#Bug Fixes

* Improve error detection and reporting in the event of invalid input
    * Discontinued command options -t, -a, and -b are removed.  Use of these options now provide an error rather than quietly disregarding their use.
    * Improve wording for the change report written to `stdout`.
    * Detect when the database configuration is different from the hashdb configuration and fail gracefully.
    * Detect when multiple databases being referenced are not compatible with each other, specifically, detect if their hash block size is different or if the databases specified are the same database.
    * Prevent creation of a database when the hash block size requested is invalid.
* Fix history log bug in subtract command where change was incorrectly logged to hashdb2 instead of hashdb3.
* Fix so test suite builds for Windows.
* Fix Windows bug where the history and setting xml file failed to back up.  The failure was that on Windows, std::rename fails to rename to target if a file is at the target path.
* Fix the command test so that it does not add a database to itself.  This is an error condition and it can corrupt the database.
* Add more validation checks to the commands test set.
* Add the Users Manual to the distribution.
* Reorganize and add content to the Users Manual.
* Organize the commands in the usage text into categories, and fix and clarify usage language.
* Set endian module to revision sha1=05ac935 since the newer endian module is not compatible with available Boost versions.
* Improve usability of the deduplicate command by creating the target database if it is not already there.
* Improve parsability of output from the expand_identified_blocks command by changing space to underscore.
* Improve parsability of output from the scan_expanded command by changing space to underscore.
* Remove compiler warning generated by the clang compiler.
* Improve the import command performance by importing hashes as they are read from the DFXML file rather than buffering them all before importing them.  This change also improves runtime progress feedback.
* Fix for compatibility with CentOS 6.4 which uses C++ v4.4.7.
* Correct wording to indicate distinct hashes, not unique hashes.

# Bug Fixes (in progress)

* Check Boost RO attach and find and remove the single-attach database limitation.

# Functional Change: Tracking File Size and File Hash

Add capability to track source metadata, specifically, the file size and its cryptographic hash, so we can know the size and hash of source files that block hashes belong to.

Changes:

* New `source_metadata_store` data has been added to the hash database for tracking the file size and the file hash.

 In the future, this store may be replaced by a SQLite3 database.
The justification is that source metadata does not require the speed performance of a B-Tree and managing metadata via SQLite3 will improve adaptability.
* Update the import command to capture the file size and file hash from the DFXML input.  These fields are located inside the fileobject tag adjacent to the filename tag.
* Update the export command to additionally export this metadata.
This metadata is exported after the block hashes are exported.
* The new `explain_identified_files` command may access this metadata.
* Change the _bulk\_extractor_ hashdb scanner so that
when importing hashes via the _hashdb_ scanner, it will gather this metadata.  It will calculate the file MD5.  It will obtain the file size from information available in `sbuf.pagesize` and will forward it to _hashdb_.
* The hash database source data will be versioned to preserve backward compatibility.  Versioning will be managed by either incrementing the hashdb database version number or by adding a separate versioning scheme specific to the source data.

# Functional Change: Add post-processing
Add new command `explain_identified_blocks` to support post-processing of the `identified_blocks.txt` file generated by _bulk\_extractor_.
The intent is to show information about less frequently observed hashes by 1) printing matching hashes only once, and 2) by removing source information that is identified as common.

Flow is:

pass 1 - go through the `identifed_blocks.txt` file:
	- for every hash
		- If we have processed this hash before, skip 
		- If the hash matches more than N files (default 20), skip
	- add to the `INTERSTING_FILE_SET` the fileIDs of the files that the hash matches

pass 2 - go through the `identified_blocks.txt` file:
	- for every hash
		- If we have processed this hash before, skip 
	- Get a list of the files and offsets that the hash matches
	- Report each file and every offset at which it is observed if the file is in the `INTERESTING_FILE_SET`

Change:

* Add new `explain_identified_blocks <hashdb.hdb> <identified_blocks.txt> <number>` command useful for post-processing analysis.
* Remove the `expand_identified_blocks <hashdb> <identified blocks file> > destfile` command
since it is obsoleted by this command.

Functional Change: Allow Hash Storage for Blocks of Any Size
================================================
A mode is added to allow tracking hashes generated from blocks of any size.
This mode does not track file offsets assoociated with hashes.

Changes:

* When creating a database using a hash block size of zero, specifically, using the `-p 0` option, allow any partition size to be valid.
* The _hashdb_ `hashdb_manager` code allows hash blocks of any size when the hash block size is configured to be zero.

# Interface Changes
### Add New Database Use Case
* Change the `create` command to accept `-p 0` as a parameter value.  When 0 is used:
 * _hashdb_ accepts block sizes of any value, otherwise, values of mismatched block size are not used.

### Add Import/Export Information
File sizes are managed during import and export.  To support this:

* New database storage `source_metadata_store` is added to additionally track the file size and file hash.
* The DFXML import parser parses the file size from the filesize tag under the fileobject tag and next to the filename tag.
* The DFXML export function additionally exports metadata information after exporting block hash information.

### Fix Database Manipulation Deficiencies
* Add command `add_repository <source hashdb> <destination hashdb> <repository name>`
to allow adding the hashes specific to a selected repository.
* Change command `subtract <A.hdb> <B.hdb> <C.hdb>` to add A but not B into C instead of adding A but not hashes in B into C, which was not symmetric with the add command.  It was a functional deficiency for this command to be missing.
* Add command `subtract_hash <A.hdb> <B.hdb> <C.hdb>` to add A but not hashes in B into C.
* Change command `intersect <source hashdb 1> <source hashdb 2> <destination hashdb>` to find the intersection of exact matches, not just the intersection of hash values.  This capability was previously missing.
* Add command `intersect_hash` to find the intersection of hash value matches.  The repository name and filename must exactly match.

### Fix Scan Service Interfaces
* Add command `scan_hash <hashdb.hdb> <hash value>` to allow capability to scan for a specific hash value.
* Remove command `expand_identified_blocks <hashdb.hdb>`.  This command produced too much information as designed.
Alternate functionality is provided by the new `explain_identified_blocks` statistical command.

### Improve Statistics interfaces
Existing capabilities for analyzing hash databases are too limited.
The following functional changes enable additional support for statistical analysis:

* Replace statistics command `hash_table <hashdb>`
which prints the entire hash table, which is unworkably large, with `hash_table <hashdb> <filename>`,
which prints the entire hash table for the specified file.
Hashes from multiple files will be shown if hashes from the same filename have been added from multiple repositories.
* Add a `-q` option for quiet mode, to suppress status output for statistics commands that take a long time to run.  An example line of status output is: `Processing index 100000 of 5644399...`.
* Add post-processing command `explain_identified_blocks` as described above.

# Changes to _BEViewer_ (in progress)
* Make _BEViewer_ find Image at relative path on Windows systems.
* Force file list in _BEViewer_ Reports tree to refresh to accomodate showing new `identified_blocks_expanded.txt` file that hashdb can crate.
* Change _BEViewer_ GUI to permit easy copy of filename so it can be pasted elsewhere.
* Make _BEViewer_ print whole feature line for `identified_blocks_expanded.txt` similar to how it is printed for `identified_blocks.txt`.

# Changes to _bulk\_extractor_ _hashdb_ scanner (in progress)
* When importing, also import source metadata.  Specifically:
 * Calculate and import the file hash.
 * Import the file size available as `sbuf` size.
* Zero-byte extend file blocks when ingesting files recursively via bulk\_extractor.
* Remove the Byte Order Marking (BOM) from the beginning of the `identified_blocks.txt` feature file.
* Remove the code marker separator that incorrectly shows up next to the filename of recursively scanned files in feature file `identified_blocks.txt`.
* Limit the maximum number of lines allowed for the identified_blocks.txt file in order to bound its growth.
* Permit simultaneous _import_ and _scan_ operations.

#Potential future changes to hashdb
* As a speed performance optimization, presort hashes before adding them.
* Replace Bloom filter with a trie data structure.  Quote from email 09/20, 07:05:
 1. Use a 1-level trie.  For example, a 2^24-ary trie that takes the first 3 bytes of the MD5 as an index to the leaf.  This is essentially an array of pointers.
 2. Leaves are sorted arrays of the next few bytes of the MD5 (let us use 2 bytes as an example)
 3. Search the arrays using a secant search, which is like a binary search but with interpolation to reduce the number of cache line reads.
 
 If storing a billion hashes in this filter, there will be 2 bytes of data stored per hash (2GB), 2^24 64-bit pointers in the trie node (128 MB), and some overhead in the allocation efficiency.  However, you will effectively get 5 bytes of hash matching, where the equivalent bloom filter would require 2^40 bits or 128GB of memory.
* Allow filename globbing for the `hash table [-q] <hashdb.hdb> <filename>` command.
* Add a B-Tree tuning hint least\_memory or all hint possibilities settable when the database is created.
* Possibly add flush at intervals.  Peformance analysis for flush has not been tested.
* Document how 0.17 was calculated as the number for calculating M from n in the Bloom filter, and document the percent false positive range this provides.

#Potential future changes to bulk_extractor
* Fix makefile dependency rules in src_win so make does not first require make clean.
* Add code in the finalization state of the hashdb scanner for generating post-processing statistics:
 * Print files that the image fully contained.
 * For partial matches, print number of blocks out of total blocks matched for each file.

# Alternative Design Considerations
* The ideal k for the False Positive Rate (FPR) is about 6-7, but it does not make that big a difference in the FPR and using k=3 cuts down on the number of random accesses (i.e. 3 cache line loads instead of 6).
* An ideal bloom filter uses 1.44 log2(1/FPR) bits per key whereas the theoretical minimum for any data structure is log2(1/FPR).  A 1-level trie structure has redundancy in each of the buckets for a total space of about log2(average leaf node size)+log2(1/FPR) per key.  The overhead could be removed if we were to implement a binary trie in each leaf node, but the structure has too high of an access time and too high of development complexity to be useful.

Availability
============
* Release source code: http://digitalcorpora.org/downloads/hashdb/
* Windows installer: http://digitalcorpora.org/downloads/hashdb/
* GIT repository: https://github.com/simsong/hashdb

Contacts
========
* Developer: mailto:bdallen@nps.edu
* Bulk Extractor Users Group: http://groups.google.com/group/bulk_extractor-users
 
