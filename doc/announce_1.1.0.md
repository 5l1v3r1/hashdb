                     Announcing hashdb 1.1.0
                        October 23, 2014

                          RELEASE NOTES

hashdb Version 1.1.0 has been released for Linux, MacOS and Windows.

Release source code and Windows installer: http://digitalcorpora.org/downloads/bulk_extractor/

GIT repository: https://github.com/simsong/bulk_extractor

#Major improvements

#Bug Fixes

* Error detection and reporting in the event of invalid input is improved:
    * Discontinued command options -t, -a, and -b are removed.  Use of these options now provide an error rather than quietly disregarding their use.
    * Wording for the change report written to `stdout` is clarified.
    * _hashdb_ Detects when the database configuration is different from the hashdb configuration and fails gracefully.
    * _hashdb_ detects when multiple databases being referenced are not compatible with each other, specifically, if their hash block size is different or if the databases specified are the same database.
    * _hashdb_ prevents creation of a database when the hash block size requested is invalid.
*A history log bug in subtract command where change was incorrectly logged to hashdb2 instead of hashdb3 was fixed.
* The test suite now builds for Windows.
* The Windows bug where the history and setting xml file failed to back up has been fixed.  The failure was that on Windows, std::rename fails to rename to target if a file is at the target path.
* The command test is fixed so that it does not add a database to itself.  This action is not allowed because it can corrupt the database.
* Validation checks are added to the commands test set.
* The Users Manual is added to the distribution.
* The Users Manual is reorganized and contains more content.
* The usage text is organized into categories and usage language is clarified.
* The endian module is set to revision sha1=05ac935 since the newer endian module is not compatible with available Boost versions.
* Commands such as the deduplicate command create a target database if one is not already there, instead of failing and requiring that the target exists.  The settings for the new target are taken from the first source specified.
* Change output of expand_identified_blocks to use JSON formatting.
* In the bulk_extractor hashdb scanner: Improve parsability of output from the scan_expanded command by using JSON formatting.
* Remove compiler warning generated by the clang compiler.
* Improve the import command performance by importing hashes as they are read from the DFXML file rather than buffering them all before importing them.  This change also improves runtime progress feedback.
* Fix for compatibility with CentOS 6.4 which uses C++ v4.4.7.
* Correct wording to indicate distinct hashes, not unique hashes.
* Fix `scan_random` command to correctly scan duplicates from fresh copy rather than from the already open database.
* Flush every progress status line.  Before, progress did not appear until the stdout buffer was full.

# Known Deficiencies
* On Windows systems, hashdb cannot open the same database multiple times in read-only mode.

# Functional Changes
### Tracking File Size and File Hash
_hashdb_ now tracks the file size and file hash of source files.
This information is useful for understanding the nature of the source files from which hashes match.
The following specific changes support this capability:

* New `source_metadata_store` data is added to the hash database for tracking the file size and the file hash.

 In the future, this store may be replaced by a SQLite3 database.
The justification is that source metadata does not require the speed performance of a B-Tree and managing metadata via SQLite3 will improve adaptability.
* The import command command captures the file size and file hash metadata from the DFXML input.  These fields are located inside the fileobject tag adjacent to the filename tag.
* The export command additionally exports this metadata.
This metadata is exported after the block hashes are exported.
* The new `explain_identified_files` command accesses this metadata.
* The _bulk\_extractor_ hashdb scanner is changed so that
when importing hashes via the _hashdb_ scanner, it gathers this metadata.  It calculates the file MD5.  It obtains the file size from `sbuf.pagesize`.
* The hash database source data preserves backward compatibility by providing the ability to create a blank `source_metadata_store`.  

### Post-processing
New command `explain_identified_blocks <hashdb.hdb> <identified_blocks.txt> <number>` is added to support post-processing of the `identified_blocks.txt` file generated by _bulk\_extractor_.
The output consists of:

 * A table of hashes that are observed no more times than a threshold value number, default 20, along with source information.
 * A table of source information relavent to the hashes identified.

###Store Hash Blocks of Any Size
Previously, only hash blocks of a preconfigured size were accepted.
This prevented cluttering the database with hashes calculated from data from end blocks of files, in particular, hashes generated from _md5deep_.

This mode permits hashes from blocks of any size.
This mode does not track file offsets assoociated with hashes.

Changes:

* When creating a database using a hash block size of zero, specifically, using the `-p 0` option, allow any partition size to be valid.
* The _hashdb_ `hashdb_manager` code allows hash blocks of any size when the hash block size is configured to be zero.
* The file offset field is not used and is set to zero.

# Interface Changes
The following user interface changes provide missing usability or enable further analysis.
### New Database
* The `create` command now accepts `-p 0` as a parameter value.  When 0 is used _hashdb_ accepts block sizes of any value, otherwise, values of mismatched block size are noted but not added.

### Import/Export
New storage _source\_metadata\_store_ is added for storing the source file size and the source file hash digest.

* The parameter ordering of the `import` command is changed to be more intuitive: the database parameter is provided before the dfxml filename parameter.
* The source file size and file hash are now imported from DFXML.
* The source file size and hash are now exported to DFXML.

### Database Manipulation
* New command `add_repository <source hashdb> <destination hashdb> <repository name>`
allows adding the hashes specific to a selected repository.
* Command `subtract` is renamed to `subtract_hash` to reflect that hash values are subtracted out, and not just hashes where exact sources match.
* Command `subtract <A.hdb> <B.hdb> <C.hdb>` is added to allow subtracting hashes where the exact sources match.  Adding this command filled a functional deficiency.
* Command `intersect` is renamed to `intersect_hash` to reflect that the intersection of hashes are taken, not just the intersection where the exact sources match.
* Command `intersect` is added to allow intersection of hashes where the exact sources match.  Adding this command filled a functional deficiency.

###Scan Services
* New command `scan_hash <hashdb.hdb> <hash value>` allows the ability to scan for a specific hash value.
* New command `scan_expanded_hash <hashdb.hdb> <hash value>` allows the ability to scan for a specific hash value, displaying all sources.
* Command `scan_expanded <hashdb.hdb> <hash value>` is modified to identify fileobject tags within the DFXML and group found objects by these filenames.

### Statistics
Previous analysis interfaces did not scale well when working with large datasets.  The following interface changes enable more scalable analysis of large data:

* Replace statistics command `hash_table <hashdb>` with `hash_table <hashdb> <repository name> <filename>`,
which prints the entire hash table for the specified file.

* New command `explain_identified_blocks` produces condensed tables of sources and hashes associated with a `identified_blocks.txt` file generated by the _bulk\_extractor_ _hashdb_ scanner.

### Tuning
* New command `upgrade` is introduced to upgrade _hashdb_ v1.0.0 databases to be compatible with v1.1.0.

###Miscellaneous
* A `-q` option for quiet mode is now available to suppress status output for statistics commands that take a long time to run.  An example line of status output is: `Processing index 100000 of 5644399...`
* A `-f flags` option is provided for controlling B-Tree settings.

# Changes to the _bulk\_extractor_ _hashdb_ scanner
* When importing, the _hashdb_ scanner additionally imports source file size and file hash metadata.
* When importing, the _hashdb_ scanner zero-byte extends short file blocks and stores these hashes as well.
* When scanning, flags about the block hash are included in the `identified_blocks.txt` feature file, in addition to the hash count.
* The code marker separator was incorrectly being included with the source filename.  This separator is no longer included.
* Option _hashdb\_scan\_max\_features_ is added to enable the ability to limit the maximum size of the generated `identified_blocks.txt` feature file.

# Proposed Future Changes to _BEViewer_
* Make _BEViewer_ find Image at relative path on Windows systems.
* Force file list in _BEViewer_ Reports tree to refresh to accommodate showing new `identified_blocks_expanded.txt` file that hashdb can crate.
* Change _BEViewer_ GUI to permit easy copy of filename so it can be pasted elsewhere.
* Make _BEViewer_ print the whole feature line for `identified_blocks_expanded.txt` similar to how it is printed for `identified_blocks.txt`.

#Potential future changes to hashdb
* As a speed performance optimization, presort hashes before adding them.
* Replace Bloom filter with a trie data structure.  Quote from email 09/20, 07:05:
 1. Use a 1-level trie.  For example, a 2^24-ary trie that takes the first 3 bytes of the MD5 as an index to the leaf.  This is essentially an array of pointers.
 2. Leaves are sorted arrays of the next few bytes of the MD5 (let us use 2 bytes as an example)
 3. Search the arrays using a secant search, which is like a binary search but with interpolation to reduce the number of cache line reads.
 
 If storing a billion hashes in this filter, there will be 2 bytes of data stored per hash (2GB), 2^24 64-bit pointers in the trie node (128 MB), and some overhead in the allocation efficiency.  However, you will effectively get 5 bytes of hash matching, where the equivalent bloom filter would require 2^40 bits or 128GB of memory.
* Allow filename globbing for the `hash table [-q] <hashdb.hdb> <filename>` command.
* Add a B-Tree tuning hint least\_memory or all hint possibilities settable when the database is created.
* Possibly add flush at intervals.  Peformance analysis for flush has not been tested.
* Document how 0.17 was calculated as the number for calculating M from n in the Bloom filter, and document the percent false positive range this provides.

#Potential future changes to bulk_extractor
* Fix makefile dependency rules in src_win so make does not first require make clean.
* Add code in the finalization state of the hashdb scanner for generating post-processing statistics:
 * Print files that the image fully contained.
 * For partial matches, print number of blocks out of total blocks matched for each file.
* Permit simultaneous _import_ and _scan_ operations.

# Alternative Design Considerations
* The ideal k for the False Positive Rate (FPR) is about 6-7, but it does not make that big a difference in the FPR and using k=3 cuts down on the number of random accesses (i.e. 3 cache line loads instead of 6).
* An ideal bloom filter uses 1.44 log2(1/FPR) bits per key whereas the theoretical minimum for any data structure is log2(1/FPR).  A 1-level trie structure has redundancy in each of the buckets for a total space of about log2(average leaf node size)+log2(1/FPR) per key.  The overhead could be removed if we were to implement a binary trie in each leaf node, but the structure has too high of an access time and too high of development complexity to be useful.

Availability
============
* Release source code: http://digitalcorpora.org/downloads/hashdb/
* Windows installer: http://digitalcorpora.org/downloads/hashdb/
* GIT repository: https://github.com/simsong/hashdb

Contacts
========
* Developer: mailto:bdallen@nps.edu
* Bulk Extractor Users Group: http://groups.google.com/group/bulk_extractor-users

