hashdb Version 1.2.0
Usage: hashdb [-h|-H|--help|--Help] [-v|-V|--version] [-q|--quiet]
              [-f|--flags=<flags>] <command> [<args>]

  -h, --help         print this message
  -H, --Help         print this message plus usage notes and examples
  -v, -V, --version, --Version    print version number
  -q, --quiet        quiet mode
  -f, --flags=flags  set B-Tree flags, any of: preload:cache_branches:
                     least_memory:low_memoy:balanced:fast:fastest

hashdb supports the following commands:

New database:
  create [options] <hashdb>
    Create a new <hashdb> hash database.

    Options:
    -p, --hash_block_size=<hash block size>
      <hash block size>, in bytes, of hashes(default 4096)
      expected <hash block size>, in bytes, or 0 for no restriction
      (default 4096)
    -m, --max=<maximum>
      <maximum> number of hash duplicates allowed, or 0 for no limit
      (default 0)
    --bloom <state>
      sets bloom filter <state> to enabled | disabled (default enabled)
    --bloom_n <n>
      expected total number <n> of distinct hashes (default 45634027)
    --bloom_kM <k:M>
      number of hash functions <k> and bits per hash <M> (default <k>=3
      and <M>=28 or <M>=value calculated from value in --bloom_n)

    Parameters:
    <hashdb>   the file path to the new hash database to create

Import/Export:
  import [-r <repository name>] <hashdb> <DFXML file>
    Import hashes from file <DFXML file> into hash database <hashdb>.

    Options:
    -r, --repository=<repository name>
      The repository name to use for the set of hashes being imported.
      (default is "repository_" followed by the <DFXML file> path).

    Parameters:
    <hashdb>       the hash database to insert the imported hashes into
    <DFXML file>   the DFXML file to import hashes from

  import_tab [-r <repository name>] [-s <sector size>] <hashdb> <tab file>
    Import hashes from file <tab file> into hash database <hashdb>.

    Options:
    -r, --repository=<repository name>
      The repository name to use for the set of hashes being imported.
      (default is "repository_" followed by the <NIST file> path).
    -s, --sector_size=<sector size>
      The sector size associated with the hashes being imported.
      (default 512)

    Parameters:
    <hashdb>       the hash database to insert the imported hashes into
    <NIST file>   the NIST file to import hashes from

  export <hashdb> <DFXML file>
    Export hashes from the <hashdb> to a <DFXML file>.

    Parameters:
    <hashdb>       the hash database containing hash values to be exported
    <DFXML file>   the new DFXML file to export hash values into

Database manipulation:
  add <source hashdb> <destination hashdb>
    Copy hashes from the <source hashdb> to the <destination hashdb>.

    Parameters:
    <source hashdb>       the source hash database to copy hashes from
    <destination hashdb>  the destination hash database to copy hashes into

  add_multiple <source hashdb 1> <source hashdb 2> <destination hashdb>
    Perform a union add of <source hashdb 1> and <source hashdb 2>
    into the <destination hashdb>.

    Parameters:
    <source hashdb 1>     a hash database to copy hashes from
    <source hashdb 2>     a second hash database to copy hashes from
    <destination hashdb>  the destination hash database to copy hashes into

  add_repository <source hashdb> <destination hashdb> <repository name>
    Copy hashes from the <source hashdb> to the <destination hashdb>
    when the <repository name> matches.

    Parameters:
    <source hashdb>       the source hash database to copy hashes from
    <destination hashdb>  the destination hash database to copy hashes into
    <repository name>     the repository name to match when adding hashes

  intersect <source hashdb 1> <source hashdb 2> <destination hashdb>
    Copy hashes that are common to both <source hashdb 1> and
    <source hashdb 2> into <destination hashdb>.  Hashes and their sources
    must match.

    Parameters:
    <source hashdb 1>     a hash databases to copy the intersection of
    <source hashdb 2>     a second hash databases to copy the intersection of
    <destination hashdb>  the destination hash database to copy the
                          intersection of exact matches into

  intersect_hash <source hashdb 1> <source hashdb 2> <destination hashdb>
    Copy hashes that are common to both <source hashdb 1> and
    <source hashdb 2> into <destination hashdb>.  Hashes match when hash
    values match, even if their associated source repository name and
    filename do not match.

    Parameters:
    <source hashdb 1>     a hash databases to copy the intersection of
    <source hashdb 2>     a second hash databases to copy the intersection of
    <destination hashdb>  the destination hash database to copy the
                          intersection of hashes into

  subtract <source hashdb 1> <source hashdb 2> <destination hashdb>
    Copy hashes that are in <souce hashdb 1> and not in <source hashdb 2>
    into <destination hashdb>.  Hashes and their sources must match.

    Parameters:
    <source hashdb 1>     the hash database containing hash values to be
                          added if they are not also in the other database
    <source hashdb 2>     the hash database containing the hash values that
                          will not be added
    <destination hashdb>  the hash database to add the difference of the
                          exact matches into

  subtract_hash <source hashdb 1> <source hashdb 2> <destination hashdb>
    Copy hashes that are in <souce hashdb 1> and not in <source hashdb 2>
    into <destination hashdb>.  Hashes match when hash values match, even if
    their associated source repository name and filename do not match.

    Parameters:
    <source hashdb 1>     the hash database containing hash values to be
                          added if they are not also in the other database
    <source hashdb 2>     the hash database containing the hash values that
                          will not be added
    <destination hashdb>  the hash database to add the difference of the
                          hashes into

  deduplicate <source hashdb> <destination hashdb>
    Copy the hashes that appear only once 
    from  <source hashdb> to <destination hashdb>.

    Parameters:
    <source hashdb>       the hash database to copy hashes from when source
                          hashes appear only once
    <destination hashdb>  the hash database to copy hashes to when source
                          hashes appear only once

Scan services:
  scan <path_or_socket> <DFXML file>
    Scan the <path_or_socket> for hashes that match hashes in the
    <DFXML file> and print out matches.

    Parameters:
    <hashdb>          the file path to the hash database to use as the
                      lookup source
    <DFXML file>      the DFXML file containing hashes to scan for

  scan_hash <path_or_socket> <hash value>
    Scan the <path_or_socket> for the specified <hash value> and print
    out matches.

    Parameters:
    <hashdb>          the file path to the hash database to use as the
                      lookup source
    <hash value>      the hash value to scan for

  scan_expanded [-m <number>] <hashdb> <DFXML file>
    Scan the <hashdb> for hashes that match hashes in the <DFXML file>
    and print out matches showing all sources.  Source information is
    suppressed if the number of sources exceeds the requested maximum.

    Options:
    -m <number>       <maximum> number of sources a hash can have before
                      suppressing printing them (default 200).

    Parameters:
    <hashdb>          the file path to the hash database to use as the
                      lookup source
    <DFXML file>      the DFXML file containing hashes to scan for

  scan_expanded_hash [-m <number>] <hashdb> <hash value>
    Scan the <hashdb> for the specified <hash value> and print out matches
    showing all sources.  Source information is suppressed if the number
    of sources exceeds the requested maximum.

    Options:
    -m <number>       <maximum> number of sources a hash can have before
                      suppressing printing them (default 200).

    Parameters:
    <hashdb>          the file path to the hash database to use as the
                      lookup source
    <hash value>      the hash value to scan for

  server <hashdb> <port number>
    Start a query server service for <hashdb> at <port number> for servicing
    hashdb queries.

    Parameters:
    <hashdb>       the hash database that the server service will use
    <port number>  the TCP port to make available for clients, for
                   example '14500'

Statistics:
  size <hashdb>
    Print out size information for the given <hashdb> database.

    Parameters:
    <hashdb>       the hash database to print size information for

  sources <hashdb>
    Print source information indicating where the hashes in the <hashdb>
    came from.

    Parameters:
    <hashdb>       the hash database to print all the repository name,
                   filename source information for

  histogram <hashdb>
    Print the histogram of hashes for the given <hashdb> database.

    Parameters:
    <hashdb>       the hash database to print the histogram of hashes for

  duplicates <hashdb> <number>
    Print the hashes in the given <hashdb> database that are sourced the
    given <number> of times.

    Parameters:
    <hashdb>       the hash database to print duplicate hashes about
    <number>       the requested number of duplicate hashes

  hash_table <hashdb> <source_id>
    Print hash information from the given <hashdb> database for hashes
    associated with the <source_id> source index.

    Parameters:
    <hashdb>              the hash database to print hashes from
    <source_id>           the source index of the hashes to print

  expand_identified_blocks [-m <number> <hashdb> <identified blocks file>
    Print source information for each hash in <identified blocks file> by
    referencing source information in <hashdb>.  Source information is
    suppressed if the number of sources exceeds the requested maximum.

    Options:
    -m <number>       <maximum> number of sources a hash can have before
                      suppressing printing them (default 200).

    Parameters:
    <hashdb>                  the hash database to use as the lookup source
                              associated with the identified blocks file
    <identified_blocks.txt>   the identified blocks feature file generated
                              by bulk_extractor

  explain_identified_blocks [-m <number>] <hashdb> <identified_blocks.txt>
    Print source information from the <hashdb> database for hashes in the
    <identified_blocks.txt> file for sources containing hashes that are not
    repeated more than a maximum number of times.

    Options:
    -m <number>               <maximum> number of repeats allowed before
                              a hash is dropped (default 20).

    Parameters:
    <hashdb>                  the hash database to use as the lookup source
                              associated with the identified blocks file
    <identified_blocks.txt>   the identified blocks feature file generated
                              by bulk_extractor

Tuning:
  rebuild_bloom [options] <hashdb>
    Rebuild the bloom filter in the <hashdb> hash database.

    Options:
    --bloom <state>
      sets bloom filter <state> to enabled | disabled (default enabled)
    --bloom_n <n>
      expected total number <n> of distinct hashes (default 45634027)
    --bloom_kM <k:M>
      number of hash functions <k> and bits per hash <M> (default <k>=3
      and <M>=28 or <M>=value calculated from value in --bloom_n)

    Parameters:
    <hashdb>       the  hash database for which the bloom filters will be
                   rebuilt

  upgrade <hashdb>
    Make <hashdb> created by v1.0.0 compatible with hashdb v1.1.0+.

    Parameters:
    <hashdb>       the hash database to upgrade

Performance analysis:
  add_random [-r <repository name>] <hashdb> <count>
    Add <count> randomly generated hashes into hash database <hashdb>.
    Write performance data in the database's log.xml file.

    Options:
    -r, --repository=<repository name>
      The repository name to use for the set of hashes being added.
      (default is "repository_add_random").

    Parameters:
    <hashdb>       the hash database to add randomly generated hashes into
    <count>        the number of randomly generated hashes to add

  scan_random <hashdb>
    Scan for random hashes in the <hashdb> database.  Writes performance
    in the database's log.xml file.

    Parameters:
    <hashdb>       the hash database to scan

bulk_extractor hashdb scanner:
  bulk_extractor -e hashdb -S hashdb_mode=import -o outdir1 my_image1
    Imports hashes from my_image1 to outdir1/hashdb.hdb

  bulk_extractor -e hashdb -S hashdb_mode=scan
                 -S hashdb_scan_path_or_socket=outdir1/hashdb.hdb
                 -o outdir2 my_image2
    Scans hashes from my_image2 against hashes in outdir1/hashdb.hdb

Examples:
This example uses the md5deep tool to generate cryptographic hashes from
hash blocks in a file, and is suitable for importing into a hash database
using the hashdb "import" command.  Specifically:
"-p 4096" sets the hash block partition size to 4096 bytes.
"-d" instructs the md5deep tool to produce output in DFXML format.
"my_file" specifies the file that cryptographic hashes will be
generated for.
The output of md5deep is directed to file "my_dfxml_file.xml".
    md5deep -p 4096 -d my_file > my_dfxml_file.xml

This example uses the md5deep tool to generate hashes recursively under
subdirectories, and is suitable for importing into a hash database using
the hashdb "import" command.  Specifically:
"-p 4096" sets the hash block partition size to 4096 bytes.
"-d" instructs the md5deep tool to produce output in DFXML format.
"-r mydir" specifies that hashes will be generated recursively under
directory mydir.
The output of md5deep is directed to file "my_dfxml_file.xml".
    md5deep -p 4096 -d -r my_dir > my_dfxml_file.xml

This example creates a new hash database named my_hashdb.hdb with default
settings:
    hashdb create my_hashdb.hdb

This example imports hashes into hash database my_hashdb.hdb from DFXML input
file my_dfxml_file.xml, categorizing the hashes as sourced from repository
"my repository":
    hashdb import -r "my repository" my_hashdb.hdb my_dfxml_file.xml

This example exports hashes in my_hashdb.hdb to output DFXML file my_dfxml.xml:
    hashdb export my_hashdb my_dfxml.xml

This example adds hashes from hash database my_hashdb1.hdb to hash database
my_hashdb2.hdb:
    hashdb add my_hashdb1.hdb my_hashdb2.hdb

This example performs a database merge by adding my_hashdb1.hdb and my_hashdb2.hdb
into new hash database my_hashdb3.hdb:
    hashdb create my_hashdb3.hdb
    hashdb add_multiple my_hashdb1.hdb my_hashdb2.hdb my_hashdb3.hdb

This example removes hashes in my_hashdb1.hdb from my_hashdb2.hdb:
    hashdb subtract my_hashdb1.hdb my_hashdb2.hdb

This example creates a database without duplicates by copying all hashes
that appear only once in my_hashdb1.hdb into new database my_hashdb2.hdb:
    hashdb create my_hashdb2.hdb
    hashdb deduplicate my_hashdb1.hdb my_hashdb2.hdb

This example rebuilds the Bloom filters for hash database my_hashdb.hdb to
optimize it to work well with 50,000,000 different hash values:
    hashdb rebuild_bloom --bloom_n 50000000 my_hashdb.hdb

This example starts hashdb as a server service for the hash database at
path my_hashdb.hdb at port number "14500":
    hashdb server my_hashdb.hdb 14500

This example searches the hashdb server service available at socket
tcp://localhost:14500 for hashes that match those in DFXML file my_dfxml.xml
and directs output to stdout:
    hashdb scan tcp://localhost:14500 my_dfxml.xml

This example searches my_hashdb.hdb for hashes that match those in DFXML file
my_dfxml.xml and directs output to stdout:
    hashdb scan my_hashdb.hdb my_dfxml.xml

This example searches my_hashdb.hdb for hashes that match MD5 hash value
d2d95... and directs output to stdout:
    hashdb scan_hash my_hashdb.hdb d2d958b44c481cc41b0121b3b4afae85

This example prints out source metadata of where all hashes in my_hashdb.hdb
came from:
    hashdb sources my_hashdb.hdb

This example prints out size information about the hash database at file
path my_hashdb.hdb:
    hashdb size my_hashdb.hdb

This example prints out statistics about the hash database at file path
my_hashdb.hdb:
    hashdb statistics my_hashdb.hdb

This example prints out duplicate hashes in my_hashdb.hdb that have been
sourced 20 times:
    hashdb duplicates my_hashdb.hdb 20

This example prints out the table of hashes along with source information
for hashes associated with source index 1 in my_hashdb.hdb:
    hashdb hash_table my_hashdb.hdb 1

This example uses bulk_extractor to scan for hash values in media image
my_image that match hashes in hash database my_hashdb.hdb, creating output in
feature file my_scan/identified_blocks.txt:
    bulk_extractor -e hashdb -S hashdb_mode=scan
    -S hashdb_scan_path_or_socket=my_hashdb.hdb -o my_scan my_image

This example uses bulk_extractor to scan for hash values in the media image
available at socket tcp://localhost:14500, creating output in feature
file my_scan/identified_blocks.txt:
    bulk_extractor -e hashdb -S hashdb_mode=scan
    -S hashdb_scan_path_or_socket=tcp://localhost:14500 -o my_scan my_image

This example uses bulk_extractor to import hash values from media image
my_image into hash database my_scan/hashdb.hdb:
    bulk_extractor -e hashdb -S hashdb_mode=import -o my_scan my_image

This example creates new hash database my_hashdb.hdb using various tuning
parameters.  Specifically:
"-p 512" specifies that the hash database will contain hashes for data
hashed with a hash block size of 512 bytes.
"-m 2" specifies that when there are duplicate hashes, only the first
two hashes of a duplicate hash value will be copied.
"--bloom enabled" specifies that the Bloom filter is enabled.
"--bloom_n 50000000" specifies that the Bloom filter should be sized to expect
50,000,000 different hash values.
    hashdb create -p 512 -m 2 --bloom enabled --bloom_n 50000000
    my_hashdb.hdb

Using the md5deep tool to generate hash data:
hashdb imports hashes from DFXML files that contain cryptographic
hashes of hash blocks.  These files can be generated using the md5deep tool
or by exporting a hash database using the hashdb "export" command.
When using the md5deep tool to generate hash data, the "-p <partition size>"
option must be set to the desired hash block size.  This value must match
the hash block size that hashdb expects or else no hashes will be
copied in.  The md5deep tool also requires the "-d" option in order to
instruct md5deep to generate output in DFXML format.  Please see the md5deep
man page.

Using the bulk_extractor hashdb scanner:
The bulk_extractor hashdb scanner provides two capabilities: 1) scanning
a hash database for previously encountered hash values, and 2) importing
block hashes into a new hash database.  Options that control the hashdb
scanner are provided to bulk_extractor using "-S name=value" parameters
when bulk_extractor is invoked.  Please type "bulk_extractor -h" for
information on usage of the hashdb scanner.  Note that the hashdb scanner
is not available unless bulk_extractor has been compiled with hashdb support.

Please see the hashdb Users Manual for further information.
