\documentclass[11pt,fleqn]{article} % Default font size and left-justified equations

%\usepackage{standalone}

\usepackage{todonotes}
\usepackage{color}
% use \todo{note} OR \missingfigure{Add my picture here}

\include{structure}

\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}
 
%\usepackage{arabtex}

\usepackage{verbatim}

\raggedbottom

\begin{document}

%define macros for commonly used terms that require special formatting
\newcommand \hdb {\textit{hashdb}\xspace}
\newcommand \sscope {\textit{SectorScope}\xspace}
\newcommand \aut {\textbf{Autopsy}\xspace}
\newcommand \bulk {\textbf{bulk\_extractor}\xspace}

\hypersetup{%
    pdfborder = {0 0 0}
}

\lstdefinestyle{customfile}{
basicstyle=\footnotesize\ttfamily, frame=single, float=htpb}

\input{./title.tex}

\pagenumbering{roman}
\setlength{\parindent}{0pt} %remove indenting from whole document
\newpage
\thispagestyle{empty}
\mbox{}
\newpage


\tableofcontents
\newpage
\pagenumbering{arabic}





\newpage

\section{Introduction}
\subsection {Overview of \hdb}
\hdb is a tool that can be used to find data in raw media using cryptographic hashes calculated from blocks of data. It is a useful forensic investigation tool for tasks such as malware detection, child exploitation detection or corporate espionage investigations. The tool provides several capabilities that include:
\begin{itemize}
\item Creating hash databases of MD5 block hashes, as opposed to file hashes.
\item Importing block hash values.
\item Scanning the hash database for matching hash values.
\item Providing the source information for hash values. 
\end{itemize}

Using \hdb, a forensic investigator can take a known set of blacklisted media and generate a hash database. The investigator can then use the hash database to search against raw media for blacklisted information. For example, given a known set of malware, an investigator can generate a sector hash database representing that malware. The investigator can then search a given corpus for fragments of that malware and identify the specific malware content in the corpus.\\

\hdb relies on block hashing rather than full file hashing. Block hashing provides an alternative methodology to file hashing with a different capability set. With file hashing, the file must be complete to generate a file hash, although a file carver can be used to pull together a file and generate a valid hash.  File hashing also requires the ability to extract files, which requires being able to understand the file system used on a particular storage device. Block hashing, as an alternative, does not need a file system or files. Artifacts are identified at the block scale (usually 512 bytes) rather than at the file scale. While block hashing does not rely on the file system, artifacts do need to be sector-aligned for \hdb to find hashes \cite{hashEncoding}.\\

\hdb provides an advantage when working with hard disks and operating systems that fragment data into discontiguous blocks yet still sector-align media. This is because scans are performed along sector boundaries. Because \hdb works at the block resolution, it can find part of a file when the rest of the file is missing, such as with a large video file where only part of the video is on disk. \hdb can also be used to analyze network traffic (such as that captured by \textbf{tcpflow}).  Finally, \hdb can identify artifacts that are sub-file, such as embedded content in a \texttt{.pdf} document.\\

\hdb stores cryptographic hashes (along with their source information) that have been calculated from hash blocks. It also provides the capability to scan other media for hash matches.
This manual describes uses cases for the \hdb tools, including usage with \aut, \sscope, \bulk, and the \hdb Python and C++ libraries, and demonstrates how users can take full advantage of all of its capabilities.

\subsection{Intended Audience}
This Users Manual is intended to be useful to new, intermediate and experienced users of \hdb. It provides an in-depth review of the functionality included in \hdb and shows how to access and utilize features through command line operation of the tool. This manual includes working examples with links to the input data used, giving users the opportunity to work through the examples and utilize all aspects of the system.  This manual also introduces Forensic tools that use \hdb.\\

For developers, this manual provides in-depth coverage of the data syntax used by \hdb and for interfacing with \hdb using the \hdb \textbf{c++} and \textbf{Python} interfaces.

\subsection{\hdb Resources}
Users are encouraged to visit the \hdb Wiki page at \url{https://github.com/NPS-DEEP/hashdb/wiki} for quick links to downloads, documentation, and examples.\\

All \hdb users should join the \bulk users Google group for more information and help with any issues encountered. To join, send an email to \textbf{bulk\_extractor-users+subscribe@} \textbf{googlegroups.com}.  \\

Several articles are available related to block hashing, and its practical and research applications. Some of those articles are specifically cited throughout this manual. Here are some additional references we recommend:
\begin{itemize}
\item Michael McCarrin, Bruce Allen. Rapid Recognition of Blacklisted Files and Fragments. Naval Postgraduate School. \url{http://www.osdfcon.org/presentations/2015/McCarrin-Allen_osdfcon.pdf}.
\item Jim Jones, Tahir Khan, Kathryn Laskey, Alex Nelson, Mary Laamanen, Doug White.  Inferring Past Activity from Partial Digital Artifacts. George Mason University, National Institute of Standards and Technology. \url{http://www.osdfcon.org/presentations/2015/Jim-Jones_EtAl-Release.pdf}.
\item Simson Garfinkel, Michael McCarrin. Hash-based Carving: Searching media for complete files and file fragments with sector hashing and hashdb. DFRWS 2015 USA. \url{http://www.sciencedirect.com/science/article/pii/S1742287615000468}
\item Joel Young, Kristina Foster, Simson Garfinkel, Kevin Fairbanks. Distinct Sector Hashes for Target File Detection. \url{http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&arnumber=6311397}.
\item Garfinkel, Simson, Alex Nelson, Douglas White and Vassil Rousseve. Using purpose-built functions and block hashes to enable small block and sub-file forensics. Digital Investigation. Volume 7. 2010. Page S13--S23. \url{http://www.dfrws.org/2010/proceedings/2010-302.pdf}.
\item Foster, Kristina. Using Distinct Sectors in Media Sampling and Full Media Analysis to Detect Presence of Documents From a Corpus. Naval Postgraduate School Masters Thesis, September 2012. \url{http://calhoun.nps.edu/public/handle/10945/17365}.
\end{itemize}

\subsection{Conventions Used in this Manual}
This manual uses standard formatting conventions to highlight file names, directory names and example commands. The conventions for those specific types are described in this section. \\

Names of programs including the post-processing tools native to \hdb and third-party tools are shown in \textbf{bold}, as in \textbf{bulk\_extractor}.\\

File names are displayed in a fixed width font. They will appear as \texttt{filename.txt} within the text throughout the manual.\\

Directory names are displayed in italics. They appear as \textit{directoryname/} within the text. The only exception is for directory names that are part of an example command. Directory names referenced in example commands appear in the example command format.\\

Database names are denoted with bold, italicized text. They are always specified in lower-case, because that is how they are referred in the options and usage information for \hdb. Names will appear as \textbf{\textit{databasename}}.\\

This manual contains example commands that should be typed in by the user. A command entered at the terminal is shown like this: \begin{Verbatim}[commandchars=\\\{\}]
\verbbf{command}
\end{Verbatim}

The first character on the line is the terminal prompt, and should not be typed. The black square is used as the standard prompt in this manual, although the prompt shown on a users screen will vary according to the system they are using.\\

\subsection{Changes Over the \hdb v2.0.1 Release}
\hdb Version 3 provides significant functional and performance improvements over v2.0.1:

\begin{itemize}
\item False positive block matches may be evaluated because metadata about hashes and sources are now being stored:
  \begin{itemize}
  \item Block labels and block entropy values indicate characteristics about data blocks.
  \item Source type and the nonprobative count of a source indicate the density of useful blocks within a source.
\end{itemize}
\item Sources are now tracked by source hash rather than by name.  This fixes two problems:
  \begin{itemize}
  \item By not storing duplicates, source relevance and similarity between sources may be weighed.
  \item Groups of identical sources are readily identified.
  \end{itemize}
\item Bulky output from scans has been significantly reduced:
  \begin{itemize}
  \item Information about matched sources and hashes are returned only once and are not reprinted if a source or hash is matched again.
  \item Information is returned in the more condensed JSON format rather than in XML.
  \end{itemize}
\item A complete \hdb API is now available for C++ and Python.
  \begin{itemize}
  \item A scan interface supports scan functions and functions for reading all hash and source information.
  \item An import interface supports functions for importing hash and source information.
  \item Free functions support access to settings and higher-layer capabilities.
  \end{itemize}
\item The database has been retuned to improve scan and import speed:
  \begin{itemize}
  \item Several tuned scan interfaces are available:
    \begin{itemize}
    \item \verb+find_expanded_hash+ scans for matches and returns complete match information in JSON format.  Matched sources and hashes are cached so that information is not reprinted in other matches.
    \item \verb+find_hash+ returns match information in a data structure.
    \item \verb+find_hash_count+ only returns a match count and does not take time to parse match information into a data structure.
    \item \verb+find_approximate_hash_count+ is fast because it does not read the hash information store when there is a match, but it can have false positives in its matching and in its count.
    \end{itemize}
  \item The Bloom filter has been removed in favor of the dense hash store.
  \end{itemize}
\item The build process has been restructured to support parallel build trees (VPATH builds).  The goal is to support compiling to additional targets such as the ARM processor.
\end{itemize}

\subsection{Licensing}
\hdb code is provided with the following notice:\\
\fbox{\begin{minipage}{37em}
The software provided here is released by the Naval Postgraduate School, an agency of the U.S. Department of Navy. The software bears no warranty, either expressed or implied. NPS does not assume legal liability nor responsibility for a User's use of the software or the results of such use.\\

Please note that within the United States, copyright protection, under Section 105 of the United States Code, Title 17, is not available for any work of the United States Government and/or for any works created by United States Government employees.  User acknowledges that this software contains work which was created by NPS government employees and is therefore in the public domain and not subject to copyright.\\

However, because hashdb includes source modules, the compiled hashdb
executable may be covered under a different copyright.\\

\texttt{rapidjson} is Copyright (C) 2015 THL A29 Limited, a Tencent company, and Milo Yip.  All rights reserved.\\

\texttt{liblmdb} is Copyright 2011-2016 Howard Chu, Symas Corp.  All rights reserved.\\

\texttt{libewf} is Copyright 2007 Free Software Foundation, Inc.\\

\texttt{crc32.h} is COPYRIGHT (C) 1986 Gary S. Brown.\\
\end{minipage}}

\subsection{Obtaining \hdb}
The \hdb tool and API interface library are readily available for Windows systems, Linux flavors, and MacOS.  A Windows installer is available for Windows users.  A source code distribution is available for Linux and Mac users.  Developers may download \hdb directly from source available on GitHub.\\

Steps for installing \hdb on Windows and one flavor of Linux are described here. For more installation options, please refer to the installation page on the \hdb Wiki at \url{https://github.com/NPS-DEEP/hashdb/wiki/Installing-hashdb}.\\

For information on installing \sscope and \bulk tools which use \hdb, Please see \textbf{\Autoref{OtherTools}}.\\

\subsubsection{Installing on Windows}
\label{InstallingOnWindows}
Windows users should download the Windows Installer for \hdb. The file to download is located at \url{http://digitalcorpora.org/downloads/hashdb} and is called \texttt{hashdb-x.y.} \texttt{z-windowsinstaller.exe} where x.y.z is the latest version number (1.1.1 as of publication of this manual).\\

You should close all Command windows before running the installation executable. Windows will not be able to find the \hdb tools in a Command window if any are open during the installation process. If you do not do this before installation, simply close all Command windows after installation. When you re-open, Windows should be able to find \hdb.\\

 Next run the \texttt{hashdb-x.y.z-windowsinstaller.exe} file. This will automatically install \hdb on your machine. Some Windows safeguards may try to prevent you from running it. Figure \ref{fig:windowsWarning} shows the message Windows 8 displays when trying to run the installer. To run anyway, click on ``More info'' and then select ``Run Anyway.'' \\
\begin{figure}
	\center
	\includegraphics[scale=.5]{windowsWarning.png}
	\caption{Windows 8 warning when trying to run the installer. Select ``More Info'' and then ``Run Anyway.''}
	\label{fig:windowsWarning}
\end{figure}

When the installer file is executed, the installation will begin and show a dialog like the one shown in Figure \ref{fig:windowsInstaller}.  Users should select the 64-bit configuration to install \hdb and select \verb+Add to path+ to add hashdb to \verb+PATH+. \hdb is now installed on your system can be run from the command line.\\

\begin{figure}
	\center
	\includegraphics[scale=.8]{WindowsInstaller.png}
	\caption{Dialog appears when the user executes the Windows Installer. Select the default configuration.}
	\label{fig:windowsInstaller}
\end{figure}

\subsubsection{Installing on Linux or Mac}
This section describes steps for installing \hdb on a Fedora 23 system and is intended to illustrate the installation process.  For steps on installing \hdb to other flavors of Linux or for MacOS, and for installing specific configurations, please refer to the installation page on the \hdb Wiki at \url{https://github.com/NPS-DEEP/hashdb/wiki/Installing-hashdb}.\\

Before compiling \hdb for your platform, you may need to install other packages on your system which \hdb requires to compile cleanly and with a full set of capabilities.\\

\textbf{Dependencies}\\
The following commands should add the requisite packages:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{sudo dnf update}
\verbbf{sudo dnf groupinstall development-tools}
\verbbf{sudo dnf install gcc-c++}
\verbbf{sudo dnf install openssl-devel}
\verbbf{sudo dnf install libewf-devel}
\verbbf{sudo dnf install swig}
\verbbf{sudo dnf install python-devel}
\end{Verbatim}

\textbf{Download and Install \hdb}\\
Next, download the latest version of \hdb. The software can be downloaded from \url{http://digitalcorpora.org/downloads/hashdb/}. The file to download is \texttt{hashdb-x.y.z.tar.gz} where x.y.z is the latest version.\\

After downloading the file, un-tar it by either right-clicking on the file and choosing ``extract to...' or typing the following at the command line:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{tar -xvf hashdb-x.y.z.tar.gz}
\end{Verbatim}

Then, in the newly created \textit{hashdb-x.y.z} directory, run the following commands to install \hdb in \textit{/usr/local/bin} (by default):

\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{./configure}
\verbbf{make}
\verbbf{sudo make install}
\end{Verbatim}
\hdb is now installed on your system and can be run from the command line. \\

Note: sudo is not required. If you do not wish to use sudo,  build and install \hdb and \bulk in your own space at ``\$HOME/local'' using the following commands:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{./configure --prefix=$HOME/local/ --exec-prefix=$HOME/local CPPFLAGS=-}
\textbf{                                 I$HOME/local/include/ LDFLAGS=-L$HOME/local/lib/}
\verbbf{make}
\verbbf{make install}
\end{Verbatim}

\subsubsection{Quickstart Guide}
The following steps provide a very brief introduction to running your new installation of \hdb. 
\begin{enumerate}
\item Navigate to the directory where you would like to create a hash database. Then, to run \hdb from the command line, type the following instructions: 
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb create sample.hdb}
\end{Verbatim} 

In the above instructions, \texttt{\textbf{sample.hdb}} is the empty database that will be created with default database settings.

\item Next, import data into the database. In this example, lets import hashes from the Kitty Material demo dataset available at \url{http://digitalcorpora.org/corpora/scenarios/2009-m57-patents/KittyMaterial/import}. But rather than downloading these files and importing them, lets just import the pre-made \verb+KittyMaterial.json+ data available at \url{http://digitalcorpora.org/downloads/hashdb/demo/KittyMaterial.json}:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb import sample.hdb blacklist_files}
\end{Verbatim} 
This command, if executed successfully, will print \verb+import completed+, along with statistics indicating changes to the database.

\item Next, scan a media image for matching hashes. In this example, lets scan the demo media image available at \url{ http://digitalcorpora.org/corpora/scenarios/2009-m57-patents/drives-redacted/jo-favorites-usb-2009-12-11.E01} which contains blacklisted Kitty block hashes. This can be done using the Python interface, \sscope, or \bulk, but lets just use the \hdb tool:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb scan_image sample.hdb jo-favorites-usb-2009-12-11.E01}
\end{Verbatim} 
With this image and dataset, the first block hash matched is at media image offset \verb+2543104+ for hash \verb+1d7379fd4d5cf676a9d4de1e48337e71+:

\begingroup
\footnotesize
\begin{Verbatim}[fontfamily=courier]
2543104	1d7379fd4d5cf676a9d4de1e48337e71	{"entropy":0,
"block_label":"","source_list_id":1193146442,"sources":
[{"file_hash":"1dd00f2e51aeebe7541cea4ade2e20b5","filesize":1549288,
"file_type":"","nonprobative_count":10,"name_pairs":
["default_repository","KittyMaterial/HighQuality/DSC00003.JPG"]}],
"source_offset_pairs":["1dd00f2e51aeebe7541cea4ade2e20b5",0]}
\end{Verbatim}
\endgroup

\end{enumerate}

\section{How \hdb Works}
The \hdb tool provides capabilities to create, edit, access and search databases of cryptographic hashes created from hash blocks. The cryptographic hashes are imported into a database from a directory, another database, \textbf{bulk\_extractor} or JSON data, or trough the \hdb API.
Once a databases is created, \hdb provides users with the capability to scan the database for matching hash values and identify matching content. Hash databases can be exported, added to, subtracted from and shared.\\


Figure \ref{fig:overviewFigure} provides an overview of the capabilities included with the \hdb tool. \hdb populates databases from whitelist source files
or other media provided in JSON format or through the API.
Users can add or remove data from the database after it is created.
Once the database is populated, \hdb can export content from the database in JSON format. It also provides an API that can be used by third party tools (as it is used in the \bulk program) to create, populate and access hash databases.\\

\begin{figure}
	\center
	\includegraphics[scale=.45]{drawings/hashdb_system_overview}
	\caption{Overview of the \hdb system}
	\label{fig:overviewFigure}
\end{figure}

\subsection{Block Hash}
\label{BlockHash}
\hdb works by matching hashes calculated from blocks of data.  \hdb is different from tools that match files because it can find matches even when part of a file is missing or changed.  \hdb stores and scans for hashes created from contiguous blocks of data.  We call the size of the block hashed the \textit{block size}.  \hdb stores and scans for hashes along a hash interval. Blocks along a hash interval are illustrated in Figure \ref{fig:hashInterval}.\\

\begin{figure}
	\center
	\includegraphics[scale=.45]{drawings/hash_interval}
	\caption{Data blocks are hashed along an interval of bytes}
	\label{fig:hashInterval}
\end{figure}

In the default configuration, the size of the data window being hashed and the interval between data blocks being hashed is 512 bytes.  The default interval and size is shown in Figure \ref{fig:defaultHashInterval}.\\

\begin{figure}
	\center
	\includegraphics[scale=.45]{drawings/default_hash_interval}
	\caption{By default, the block size and hash interval are 512 bytes}
	\label{fig:defaultHashInterval}
\end{figure}

As an optimization, \hdb provides a byte alignment setting. Allow \hdb to select the default unless you would like the step size to be smaller than the block size. Byte alignment is described in \textbf{\autoref{Settings}}.

\subsection{Blacklist Data}
Blacklist data is the data we scan against to determine whether forensic data
contains probative artifact.
We build a hash database of blacklist data by importing block hashes
from blacklist files, copying from other hash databases,
or importing from other sources using data prepared in JSON format.\\

Each block hash in the database contains one or more source, file offset pairs indicating a source and location within the source where the hash is located.  If a block is duplicated within a source, then several source, file offset pairs will be stored for that hash.  If a block hash is also found in another source, then source, file offset pair(s) from that source will also be stored for that hash. Block hashes with many source, file offset pairs tend to contain non-probative data.

\subsection{Forensic Data}
Forensic data is the data we scan to see if it contains artifact matching
that in our hash database.
Note that just having matches is not sufficiently probative.
Some matches are common to many files.
\hdb tracks entropy and data information to automate the process
of eliminating many false positives.
Direct analysis such as that provided by the \sscope tool
may be used to see the exact content at that location.
\sscope is available at \url{https://github.com/NPS-DEEP/NPS-SectorScope/wiki}.

\subsection{File Hash}
\hdb tracks sources by their file hash rather than by their filename
or repository name.  This approach provides several benefits:

\begin{itemize}
\item The database does not store block hashes from multiple sources
when the sources are actually the same file.
\item Source filenames and repository names for the same file are grouped
together and may be looked up by their file hash value.
\end{itemize}

\subsection{Managing False Positives}
A significant problem when scanning for probative blocks is dealing with false positives \cite{hashBasedCarving}. False positives arise from data that is easily generated or commonly duplicated such as sparse data or lookup tables. \hdb records and uses information about blocks and sources in order to identify blocks as nonprobative. Then, post-processing tools such as \sscope can readily evaluate matched blocks with these false positives removed.\\

Here we describe the data that \hdb stores with hashes and sources. How this data is used to classify blocks as nonprobative is a complex issue. \hdb stores this data. It is up to post-processing tools such as \sscope to evaluate it.\\

\begin{itemize}
\item \textbf{Entropy}\\
We calculate the entropy of data blocks and use this value to help estimate that the block may be nonprobative. Blocks with a low entropy value are often nonprobative. \hdb calculates the Shannon entropy of blocks using an alphabet of $2^{16}$ values. Unfortunately, data in blocks can be a member of many types of alphabets. \hdb uses one algorithm.  For improved results, we recommend considering the type of data along with the calculated entropy when estimating that a block may be nonprobative. The entropy field is new to \hdb v3.\\
\item \textbf{Block Label}\\
Block labels may be used to hold information about the nature of the block.  For example it might be used to indicate that the byte values increment, indicating a homogeneous data structure \cite{hashBasedCarving}.\\
\item \textbf{File Hash}\\
Matched source files are indexed by file hash (new to \hdb v3). The \sscope tool uses this value to visualize how specific source files are distributed across a media image.\\
\item \textbf{Repository Name and Filename}\\
The list of repository names and filenames associated with a source file are provided, enabling a comprehensive indication of what a hash match is a member of.\\
\item \textbf{Source File Size}\\
The source file size indicates how big a source file is. The \sscope tool uses this value to know what percentage of a source file is matched in a scan.\\
\item \textbf{Source File Type}\\
This field stores information about the type of the source file. This field is not used by the \hdb Tool but is available through the \hdb API interfaces.\\
\item \textbf{Source File Nonprobative Count}\\
The nonprobative count indicates how many blocks of the source file are deemed nonprobative. In the \bulk \hdb scanner, this value is set to the number of blocks that have been given a block label.  In \hdb, zzzzzzz.\\
\end{itemize}

\subsection{Building a \hdb Database}
There are several ways to populate a database:

\begin{itemize}
\item Using the \hdb \verb+import+ command.
\item Importing from correctly formatted JSON data.
\item Importing from another database.
\item Using the \bulk \hdb scanner.
\item Using the \hdb library through the Python or C++ interface.
\end{itemize}

A database may contain blacklist hashes from multiple source domains,
where a domain is called a \textit{repository}.
The repository name indicates the provenance of the dataset.
It is its description information, such as ``Company X's intellectual property files''.\\

\subsection{Scanning}
There are multiple ways users can scan for matches in a block hash database:

\begin{itemize}
\item Using one of the \hdb tool scan commands to scan from a media image,
list, stream, or specific hash.
\item Using one of the \hdb API scan interfaces to scan at a particular
level of detail.
\item Using the \bulk \hdb scanner Scan function.
\end{itemize}

\subsection{Contents of a Hash Database}
\label{ContentsOfDB}
Each \hdb database is contained in a directory called \textit{$<$databasename$>$.hdb} and contains a number of files. These files are:

\begingroup
\footnotesize
\begin{Verbatim}[fontfamily=courier]
lmdb_hash_data_store/data.mdb
lmdb_hash_data_store/lock.mdb
lmdb_hash_store/data.mdb
lmdb_hash_store/lock.mdb
lmdb_source_data_store/data.mdb
lmdb_source_data_store/lock.mdb
lmdb_source_id_store/data.mdb
lmdb_source_id_store/lock.mdb
lmdb_source_name_store/data.mdb
lmdb_source_name_store/lock.mdb
log.txt
settings.json
\end{Verbatim}
\endgroup

These files include several data store directories and files, a settings file, and a log file:

\begin{itemize}
\item Data store files \\
Data store files encode all the block hashes, source files, and related information that are in the database. Data store filenames start with the prefix \verb+lmdb+.

\item \texttt{settings.json} \\
This file contains the settings requested by the user when the block hash database was created, see \hdb settings and Bloom filter settings options.  This file also contains the internal \hdb settings version used to help \hdb identify whether a database is compatible with this version of \hdb. The \texttt{settings.json} file with the default settings looks like this:\\

\begingroup
\footnotesize
\begin{Verbatim}[fontfamily=courier]
{"settings_version":3, "sector_size":512, "block_size":512,
"max_id_offset_pairs":100000, "hash_prefix_bits":28,
"hash_suffix_bytes":3}
\end{Verbatim}
\endgroup

Settings are described in \textbf{\autoref{Settings}}.


\item \texttt{log.txt} \\
Every time a command is run that changes the content of the database, information about the change is appended to this log.  Each entry includes the command name, information about \hdb including the command typed and how \hdb was compiled, information about the operating system \hdb was just run on, timestamps indicating how much time the command took, and the specific \hdb changes applied.\\

Listing \ref{logfile} shows an example log file containing two entries, one for when the hash database was created, and one for when data was imported into the database.

\lstset{style=customfile}
\begin{lstlisting}[float, caption=An example \texttt{log.xml} log file showing two database activities, label=logfile]
# libhashdb version: 3.0.0-alpha-1
# command: "hashdb create KittyMaterial.hdb"
# username: bdallen
# start time 2016-04-07T19:20:06Z
{"name":"begin","delta":"0.000386","total":"0.000386"}
{"name":"end","delta":"0.000013","total":"0.000403"}
# libhashdb version: 3.0.0-alpha-1
# command: "hashdb import KittyMaterial.hdb KittyMaterial.json"
# username: bdallen
# start time 2016-04-07T19:20:23Z
{"name":"begin","delta":"0.000387","total":"0.000387"}
# hashdb changes:
#     hash_data_data_inserted: 401935
#     hash_data_data_same: 195
#     hash_data_source_inserted: 402130
#     hash_prefix_inserted: 401653
#     hash_suffix_inserted: 401935
#     hash_count_changed: 149
#     hash_not_changed: 46
#     source_data_inserted: 101
#     source_id_inserted: 101
#     source_id_already_present: 402231
#     source_name_inserted: 101
{"name":"end","delta":"2.608494","total":"2.608885"}
\end{lstlisting}


\item Timestamp\\
\verb+timestamp.json+ is not formally part of the \hdb database.  It is created by \hdb tool performance analysis commands shown in \textbf{\autoref{PerformanceAnalysis}}. This file is replaced rather than appended to. Please see \textbf{\Autoref{InputOutputSyntax}} for information on timestamp syntax.

\end{itemize}



\subsection{Settings}
\label{Settings}
Function and performance of a \hdb database is set using configurable settings:\\

\begin{itemize}
\item \textbf{Block size}\\
The size of data blocks the database expects to store. Block hashes are calculated from data of this size. The default is 512.\\

\item \textbf{Sector size}\\
The resolution of sectors that may reside on media. The operating system and/or storage device will align data on sector boundaries. Block hashes are calculated on aligned sector boundaries. The default is 512. Although 1 is valid, try to use the full sector size since \hdb makes a more compact database with larger values.\\

\item \textbf{Max source, offset pairs}\\
The maximum number of source, offset pairs to store for a given hash, default 100,000. Each pair identifies a location in a source where this hash is located. If source information is not interesting when there are many sources, offset pairs associated with a hash, use a low value to prevent storing extra entries.\\

\item \textbf{Hash prefix bits}\\
The number of bits of the hash prefix to use as the key in the store. The idea is to select a value given the size of the database so that the average number of hashes with this prefix is slightly greater than 0, for example 20. For example if you expect 5 billion hashes, you might select 28 because $5/2^{28}=18.6$, which is near 20.\\

\item \textbf{Hash suffix bytes}\\
The number of bytes of the hash suffix to store in the set of values of hash suffixes for this hash key. The idea is to store as few bytes as possible while minimizing false positives. For example if you expect 5 billion hashes, you might select 28 prefix bits and 3 suffix bytes because $5 / 2^{((28) + 8*(3))} = 5 / 2^{52} = 0.00011\%$ false positive rate, about 1 in 1 million.\\
\end{itemize}

\section {Running the \hdb Tool}
\label{Running}
The core capabilities provided by \hdb involve creating and maintaining a database of hash values and scanning media for those hash values. To perform those tasks, \hdb users need to start by building a database (if an existing database is not available for use).
Users then import hashes using \hdb commands, the \hdb \bulk scanner, or the \hdb API, and then possibly merge or subtract hashes to obtain the desired set of hashes to scan against.
Users then scan for hashes that match.
Additional commands are provided to support statistical analysis, performance tuning and performance analysis.\\

This section describes \hdb commands, along with examples, for performing these tasks.
For more examples of command usage, please see \textbf{\autoref{UseCases}}.
For a \hdb quick reference summary, please see \textbf{\autoref{QuickReference}}
or \url{http://digitalcorpora.org/downloads/hashdb/hashdb_quick_reference.pdf}.

\subsection{Creating a Hash Database}
\label{Creating}
A hash database must be created before hashes can be added to it.
The command to create a hash database is shown in Table \ref{tab:createDatabase}.
Configurable settings associated with the database is shown in Table \ref{tab:hashDBSettings} and described in \textbf{\Autoref{Settings}}.\\
\begin{table}[!ht]
\centering
\caption{Command for Creating Hash Databases}
\label{tab:createDatabase}
\begin{tabular}{|p{2.5 cm}|p{7 cm}|p{4 cm}|}
\hline \hline
\textbf{Command} & \textbf{Usage} & \textbf{Description} \\
\hline
\textbf{create} & \verb+create [-b <block size>]+ \verb+[-b <sector size>]+ \verb+[-m <max source offset pairs>]+ \verb+[-t <hash prefix bits:hash+ \verb+suffix bytes>]+ \verb+<hashdb.hdb>+ & Creates a new hash database with the given configuration settings.\\
\hline
\end{tabular}
\end{table}

\begin{table}[!ht]
\centering
\caption{Settings for New Databases}
\label{tab:hashDBSettings}
\begin{tabular}{|p{1.5 cm}|p{8 cm}|p{4 cm}|}
\hline \hline
\textbf{Option} & \textbf{Verbose Option} & \textbf{Specification} \\
\hline
\textbf{\texttt{-b}} & \verb+--block_size=+\textit{block\_size} & Specifies the block size in bytes used to generate the hashes that will be stored in the database. Default is 512 bytes.  \\
\hline
\textbf{\texttt{-s}} & \verb+--sector_size=+\textit{sector\_size} & Specifies the sector size in bytes used to calculate hashes along. Default is 512 bytes.  \\
\hline
\textbf{\texttt{-m}} & \verb+--max_source_offset_pairs=+\textit{max source offset pairs} & Specifies the maximum number of source, source offset pairs allowed. 0 value indicates there is no limit. Default is 0.\\
\hline
\textbf{\texttt{-t}} & \verb+--tuning=+\textit{prefix bits:suffix bytes} & Specifies the number of prefix bits and suffix bytes to use for compacting the database.\\
\hline
\end{tabular}
\end{table}

\textbf{Example}\\
To create an (empty) hash database named \textbf{\texttt{mock\_video.hdb}}, type the following command:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb create mock_video.hdb}
\end{Verbatim}
The above command will create a database with all of the default hash database settings. Most users will not need to change these settings.
Users can specify either the option and value or the verbose option value for each parameter along with the create command, as in:\\
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb create --max\_source\_offset\_pairs=20 mock_video.hdb}
\verbbf{hashdb create -m 20 mock_video.hdb}
\end{Verbatim}
The above two commands produce identical results, creating the database \texttt{mock\_video.hdb} that will accept a maximum of 20 source, offset pairs per hash.\\

\subsection{Importing and Exporting}
Hash databases may be imported to in several ways, and may be exported in JSON format.
Commands to import and export hashes are shown in Table \ref{tab:importExport}.\\
\begin{table}[!ht]
\centering
\caption{Commands for Importing into and Exporting Hash Databases}
\label{tab:importExport}
\begin{tabular}{|p{2.5 cm}|p{7 cm}|p{4 cm}|}
\hline \hline
\textbf{Command} & \textbf{Usage} & \textbf{Description} \\
\hline
\textbf{import\_dir} & \verb+import_dir [-r <repository name>]+ \verb+[-w <whitelist.hdb>]+ \verb+<hashdb.hdb>+ \verb+<import directory>+& Imports hashes from files under the import directory.\\
\hline
\textbf{import\_tab} & \verb+import_tab [-r <repository name>]+ \verb+<hashdb.hdb>+ \verb+<tab.txt>+& Imports values from the tab-delimited file into the hash database.\\
\hline
\textbf{import} & \verb+import <hashdb.hdb>+ \verb+<hashdb.json>+& Imports values from the JSON file into the hash database.\\
\hline
\textbf{export} & \verb+export <hashdb.hdb>+ \verb+<hashdb.json>+& Exports the hash database to the JSON file\\
\hline
\end{tabular}
\end{table}
Note that there are other ways to populate a database besides these listed here, including using other hash databases (discussed in \textbf{\autoref{updateSection}}),
by using the \bulk \hdb scanner (discussed in \textbf{\autoref{bulkextractorSection}}),
and through the use of the import capability provided by the \hdb library API (discussed in \textbf{\autoref{APISection}}).\\


To import block hashes from a directory of blacklist sources, type the following command:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb import_dir -r mock_video_repository mock_video.hdb mock_video_dir}
\end{Verbatim}
In the above command the option \textbf{-r} is used along with the repository name \texttt{mock\_video\_repository} to indicate the repository source of the block hashes being imported into the database. The repository name is used to keep track of the sources of hashes. Hash blocks contained in one database often originate from many different sources and the fileme may be the same. For example, if we add two separate but similar databases with partial overlap to a database, this will result in some duplicate hashes from multiple sources with the same filename. The repository name can be used with those duplicates to allow users to track all hashes back to their original sources. By default, the repository name used is the text \texttt{repository\_} with the filename of the file being imported from appended after it.\\

The \textbf{import} command in the above example imports block hashes from files in the \texttt{mock\_video\_dir} directory into the database \texttt{mock\_video.hdb}. \hdb prints the following to the command line to indicate that hashes have been inserted into the database: 

\begingroup
\footnotesize
\begin{Verbatim}[fontfamily=courier]
# hashdb changes:
#     hash_data_data_inserted: 401935
#     hash_data_data_same: 195
#     hash_data_source_inserted: 402130
#     hash_prefix_inserted: 401653
#     hash_suffix_inserted: 401935
#     hash_count_changed: 149
#     hash_not_changed: 46
#     source_data_inserted: 101
#     source_id_inserted: 101
#     source_id_already_present: 402231
#     source_name_inserted: 101
import completed.
\end{Verbatim}
\endgroup
These numbers don't correspond to the actual number of hashes inserted, but they do indicate that data was added.\\

The database \texttt{\textbf{mock\_video.hdb}} now contains these block hashes.\\

Also, database log file \texttt{log.txt} is updated to show that a set of hash blocks have just been inserted. The log in Figure \ref{logfile} was generated from similar \textbf{create} and \textbf{import} actions.  The contents of log files is described in \textbf{\autoref{ContentsOfDB}}.\\

Users may prefer to run statistical commands such as this to get information about the contents of the database (and confirm that values were inserted):
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb size mock_video.hdb}
\end{Verbatim}

\subsection{Database Manipulation}
\label{DatabaseManipulation}
Databases may need to be merged together or common hash values may need to be subtracted out in order to produce a specific set of blacklist data to scan against.
Commands that manipulate hash databases are outlined in Table \ref{tab:databaseManipulation}.
Destination databases are created if they do not exist yet.

\begin{table}[!ht]
\centering
\caption{Commands to Manipulate Hash Databases}
\label{tab:databaseManipulation}
\begin{tabular}{|p{3.5 cm}|p{6 cm}|p{4 cm}|}
\hline \hline
\textbf{Command} & \textbf{Usage} & \textbf{Description} \\
\hline
\textbf{add} & \verb+add <source db>+ \verb+<destination db>+ & Copies all of the hashes from \textit{source db} to \textit{destination db}\\
\hline
\textbf{add\_multiple} &  \verb+add_multiple <source db1>+ \verb+<source db2>+ \verb+...+ \verb+<destination db>+ & Adds databases \textit{source db1}, \textit{source db2}, etc.\ to \textit{destination db}\\
\hline
\textbf{add\_repository} & \verb+add_repository <source db1>+ \verb+<destination db2>+ \verb+<repository name>+ & Adds \textit{source db1} to \textit{destination db2} but only when the repository name matches\\
\hline
\textbf{intersect} & \verb+intersect <source db1>+ \verb+<source db2> <destination db>+ &   Copies hash values common to both \textit{source db1} and \textit{source db2} into \textit{destination db} where sources match\\
\hline
\textbf{intersect\_hash} & \verb+intersect_hash <source db1>+ \verb+<source db2> <destination db>+ &   Copies hash values common to both \textit{source db1} and \textit{source db2} into \textit{destination db} even if their sources are different.\\
\hline
\textbf{subtract} & \verb+subtract <source db1>+ \verb+<source db2> <destination db>+&   Copies hash values found in \textit{source db1} but not in \textit{source db2} into \textit{destination db} where sources match\\
\hline
\textbf{subtract\_hash} & \verb+subtract <source db1>+ \verb+<source db2> <destination db>+&   Copies hash values found in \textit{source db1} but not in \textit{source db2} into \textit{destination db} even if their sources are different.\\
\hline
\textbf{subtract \_repository} & \verb+subtract_repository+ \verb+<source db1>+ \verb+<destination db2>+ \verb+<repository namedb>+ & Adds \textit{source db1} to \textit{destination db2} unless the repository name matches\\
\hline
\textbf{deduplicate} & \verb+deduplicate <source db>+ \verb+<destination db>+ &   Copies hash values that have only one source, offset pair associated with them from \textit{source db} into \textit{destination db}\\
\hline
\end{tabular}
\end{table}

\subsubsection{Tracking Changes in Hash Databases}
Statistics about hash database changes are reported on the console and to the log file inside the hash database.
These statistics show specific changes made to stores within the hash database
and also changes not made because conditions were not met.
These statistics are shown in Table \ref{tab:changeStatistics}.
\begin{table}[!ht]

\centering
\caption{Database Changes Resulting from Commands that Manipulate Hash Databases}
\label{tab:changeStatistics}
\begin{tabular}{|p{5 cm}|p{8.8 cm}|}
\hline \hline
\textbf{Statistic} & \textbf{Meaning} \\
\hline

% hash_data
\verb+hash_data_data_inserted+ &  Number of hash data records inserted.\\
\hline
\verb+hash_data_data_changed+ &  Number of hash data records changed.\\
\hline
\verb+hash_data_data_same+ &  Number of hash data records provided but same.\\
\hline
\verb+hash_data_source_inserted+ &  Number of hash data source, offset pairs inserted.\\
\hline
\verb+hash_data_source_already_+ \verb+present+ &  Number of hash data source, offset pairs provided but same\\
\hline
\verb+hash_data_source_at_max+ &  Number of hash data source, offset pairs dropped because of max pair limit.\\
\hline

% hash
\verb+hash_prefix_inserted+ &  Number of hash prefix keys inserted.\\
\hline
\verb+hash_suffix_inserted+ &  Number of hash suffix values inserted.\\
\hline
\verb+hash_count_changed+ &  Number of hash count changes were applied.\\
\hline
\verb+hash_not_changed+ &  Number of hash and count values provided but same.\\
\hline

% source data
\verb+source_data_inserted+ &  Number of source data records inserted.\\
\hline
\verb+source_data_changed+ &  Number of source data records changed.\\
\hline
\verb+source_data_same+ &  Number of source data records provided but same.\\
\hline

% source_id
\verb+source_id_inserted+ &  Number of source ID records inserted.\\
\hline
\verb+source_id_already_present+ &  Number of source ID records provided but already present.\\
\hline

% source_name
\verb+source_name_inserted+ &  Number of source names inserted.\\
\hline
\verb+source_name_already_+ \verb+present+ &  Number of source names provided but already present.\\
\hline
\end{tabular}
\end{table}

\subsection{Scan Services}
\label{ScanServices}
\hdb can be used to determine if a file, directory or disk image has content that matches previously identified content. This capability can be used, for example, to determine if a set of files contains a specific file excerpt or if a media image contains a video fragment. Forensic investigators can use this feature to search for blacklisted content.
Scan services are shown in Table \ref{tab:scanServices}. \\

\begin{table}[!ht]
\centering
\caption{Commands that Provide Scan Services}
\label{tab:scanServices}
\begin{tabular}{|p{3.5 cm}|p{6 cm}|p{4 cm}|}
\hline \hline
\textbf{Command} & \textbf{Usage} & \textbf{Description} \\
\hline
\textbf{scan\_list} & \verb+scan <hashdb>+ \verb+<hash list file>+ & Scans the hashdb for hashes that match hashes in the hash list file and prints out matches\\
\hline
\textbf{scan\_hash} & \verb+scan_hash <hashdb>+ \verb+<hash value>+ & Scans the hashdb for the specified hash value and prints out whether it matches\\
\hline
\textbf{scan\_image} & \verb+scan_image+ \verb+<hashdb> <media image>+ & Scans the hashdb for hashes that match hashes in the media image and prints out matches\\
\hline
\end{tabular}
\end{table}

To scan, first identify the media that you would like to scan. For this example, we download and use video file \texttt{mock\_video\_redacted\_image} available at \url{http://digitalcorpora.org/downloads/hashdb/demo}.\\

Then identify the existing hash database that will be used to search for hash value matches. We'll use the database \texttt{\textbf{mock\_video.hdb}} that we created in the previous section. That database contains all of the block hash values from a media image. \\

Finally, run the \hdb scan command to scan for blocks in the media that match block hashes in the database:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb scan_image mock_video.hdb mock_video_redacted_image > matches.json}
\end{Verbatim}
This command tells \hdb to scan \textbf{mock\_video\_redacted\_image} and try to match the values found in the local database \texttt{\textbf{mock\_video.hdb}}, putting match data in file \verb+matches.json+.  An example match might look like this:

\begingroup
\footnotesize
\begin{Verbatim}[fontfamily=courier]
2543104	1d7379fd4d5cf676a9d4de1e48337e71	{"entropy":0,
"block_label":"","source_list_id":1193146442,"sources":
[{"file_hash":"1dd00f2e51aeebe7541cea4ade2e20b5","filesize":1549288,
"file_type":"","nonprobative_count":10,"name_pairs":
["default_repository","KittyMaterial/HighQuality/DSC00003.JPG"]}],
"source_offset_pairs":["1dd00f2e51aeebe7541cea4ade2e20b5",0]}
\end{Verbatim}
\endgroup


Users may be put off by the quantity of matches incurred by low-entropy data in their databases such as blocks of zeros or metadata header blocks from files that are otherwise unique. Database manipulation commands,
\textbf{\autoref{DatabaseManipulation}}, can mitigate this, for example:
\begin{itemize}
\item Use the ``subtract'' command to remove known whitelist data created from sources such as ``brand new'' operating system images and the NSRL.
\item Alternatively, use the ``deduplicate'' command to copy all hash values that have been imported exactly once.
\end{itemize}
These commands are provided to manage false positives.

\subsection{Statistics}
Various statistics are available about a given hash database including the size of a database, where its hashes were sourced from, a histogram of its hashes, and more.
Table \ref{tab:statistics} describes available statistics.\\

\begin{table}[!ht]
\centering
\caption{Commands that provide Statistics about Hash Databases}
\label{tab:statistics}
\begin{tabular}{|p{3.5 cm}|p{6 cm}|p{4 cm}|}
\hline \hline
\textbf{Command} & \textbf{Usage} & \textbf{Description} \\
\hline
\textbf{size} & \verb+size <hashdb>+ & Prints out size information relating to the database\\
\hline
\textbf{sources} & \verb+sources <hashdb>+ & Provides a top-level view of the repository names and filenames in the database. It prints out all repositories and files that have contributed to this database\\
\hline
\textbf{histogram} & \verb+histogram <hashdb>+ &  Prints a hash distribution for the hashes in the \textit{hashdb}\\
\hline
\textbf{duplicates} & \verb+duplicates <hashdb> <number>+ &  Prints out hashes in the database that are sourced the given number of times\\
\hline
\textbf{hash\_table} & \verb+hash_table <hashdb>+ \verb+<hex file hash>+ &  Prints hashes associated with the specified source identified by the source file hexdigest\\
\hline
\end{tabular}
\end{table}

To find the size of various data stores in hash database \texttt{example.hdb},
type the following:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb size examle.hdb}
\end{Verbatim}
The above command prints the size of various data stores within the database in JSON format.\\

To obtain a list of hashes in \texttt{example.hdb} associated with the source file identified by hexcode \texttt{16d75027533b0a5ab900089a244384a0}, type the following:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb hash_table example.hdb 16d75027533b0a5ab900089a244384a0}
\end{Verbatim}

\subsection{Performance Analysis}
\label{PerformanceAnalysis}
Performance analysis commands for analyzing \hdb performance are shown in Table \ref{tab:analysis}. Timing data is placed in file \verb+timestamp.json+, replacing any previous content.

\begin{table}[!ht]
\centering
\caption{Commands that Support \hdb Performance Analysis}
\label{tab:analysis}
\begin{tabular}{|p{3.5 cm}|p{6 cm}|p{4 cm}|}
\hline \hline
\textbf{Command} & \textbf{Usage} & \textbf{Description} \\
\hline
\textbf{add\_random} & \verb+add_random+ \verb+-r [<repository name>]+ \verb+<hashdb.hdb> <count>+ & Adds count random hashes to the given database, creating timing data in the \texttt{log.xml} file\\
\hline
\textbf{scan\_random} & \verb+scan_random <hashdb.hdb>+ & Scans random hashes in the given database, creating timing data in the \texttt{log.xml} file\\
\hline
\textbf{add\_same} & \verb+add_same+ \verb+-r [<repository name>]+ \verb+<hashdb.hdb> <count>+ & Adds count same hashes to the given database, creating timing data in the \texttt{log.xml} file\\
\hline
\textbf{scan\_same} & \verb+scan_same<hashdb.hdb>+ & Scans count same hashes in the given database, creating timing data in the \texttt{log.xml} file\\
\hline
\end{tabular}
\end{table}

\section{Tools that use \hdb}
\label{OtherTools}
\sscope, the \sscope Autopsy Plug-in, and the \bulk \hdb scanner use \hdb.

\subsection{\sscope}
The \sscope tool provides a GUI for analyzing data associated with block hash matches found on a media image. An example screenshot of the main window of \sscope showing a histogram of matches on a media image is shown in Figure \ref{fig:SectorScope_main}. \sscope also provides interfaces for building and scanning against \hdb databases. Please see \url{https://github.com/NPS-DEEP/NPS-SectorScope/wiki} for more information on \sscope.

\begin{figure}
	\center
	\includegraphics[scale=.45]{drawings/SectorScope_main}
	\caption{Example screenshot of the \sscope tool}
	\label{fig:SectorScope_main}
\end{figure}

\subsection{The \sscope \aut Plug-in}
\sscope provides an \aut plug-in for scanning for fragments of previously identified files. \aut is currently only available on Windows systems. This section describes how to set up the \sscope \aut plug-in.

\subsubsection{Installing the \sscope Plug-in}
The \sscope Windows installer installs the requisite \verb+.nbm+ Autopsy plug-in module onto the desktop. Please follow these steps to install this module:

\begin{enumerate}
\item Open \aut. From the Autopsy menu, select \verb+Tools | Plugins+.
\item Open the \verb+Downloaded+ tab and click the \verb+Add Plugins...+ button.
\item From the \verb+Add Plugins+ window, navigate to the \verb+.nbm+ module file that was installed onto the desktop, and open it.
\item Click \verb+Install+ and follow the wizard. Please note that it may be difficult to replace an old module of NPS-Autopsy-hashdb already installed in Autopsy. In the unlikely case that error \verb+Some plugins require plugin org.jdesktop.beansbinding+ \verb+to be installed+ appears, it may be necessary to uninstall and reinstall Autopsy.
\end{enumerate}

\subsubsection{Configuring the \sscope Plug-in}
The path to the \hdb database to scan against must be configured:

\begin{enumerate}
\item Start a new case, \verb+File | New Case...+, fill in the Case Information fields, and click \verb+Next+.
\item Fill in Case Information and click \verb+Finish+.
\item For Add Data Source (1 of 3), put in a media image for Autopsy to process and click \verb+Next+.
\item For Add Data Source (2 of 3), select checkboxes as desired, then click on \verb+NPS-SectorScope+ text to configure the path to your \hdb database to scan against. Currently a file chooser is not available, so please type in the full path, for example: \verb+C:\Users\me\my_hashdb.hdb+. Click Next.
\item For Add Data Source (3 of 3) click \verb+Finish+. When the NPS-SectorScope module begins processing, Autopsy will display "NPS-SectorScope ..." as \bulk runs, which may take up to several hours. Unfortunately, \bulk progress is not currently indicated. For diagnostics: please see if progress text is appearing in the generated \verb+bulk_extractor\report.xml+ file and in the generated log file or try running the scan manually.
\end{enumerate}

\subsection{\bulk}
\bulk is an open source digital forensics tool that extracts features such as email addresses, credit card numbers, URLs and other types of information from digital evidence files. It operates on disk images, files or a directory of files and extracts useful information without parsing the file system or file system structures.  For more information on how to use \bulk for a wide variety of applications, refer to the separate publication \textit{The \bulk Users Manual} available at \url{http://digitalcorpora.org/downloads/bulk_extractor/BEUsersManual.pdf} \cite{beusersguide}.\\

In particular, a \hdb \bulk scanner is available which may be used to import block hashes into a new hash database and to scan for hashes against an existing hash database.
Currently, \hdb requires a newer build of \bulk than is available on the \bulk site.
Please see the \hdb Wiki page at \url{https://github.com/NPS-DEEP/hashdb/wiki}
for information on obtaining a version of \bulk that is compatible with the current version of \hdb.\\

Options that control the hashdb
scanner are provided to \bulk using "\verb+-S name=value+" parameters.
Example syntax for the \bulk \hdb scanner is shown in Table \ref{tab:hashdbScanner}.
Scanner options are described in Table \ref{tab:hashdb_be_usage}.

\begin{table}[!ht]
\centering
\caption{\bulk \hdb Scanner Commands}
\label{tab:hashdbScanner}
\begin{tabular}{|p{3.5 cm}|p{6 cm}|p{4 cm}|}
\hline \hline
\textbf{Goal} & \textbf{Example} & \textbf{Description} \\
\hline
\textbf{import files} & \verb+bulk_extractor -E hashdb+ \verb+-S hashdb_mode=import+ \verb+-o outdir1 -R my_directory+ & Import hashes from directory into outdir1/hashdb.hdb\\
\hline
\textbf{import image} & \verb+bulk_extractor -E hashdb+ \verb+-S hashdb_mode=import+ \verb+-o outdir1 my_image1+ & Import hashes from image into outdir1/hashdb.hdb\\
\hline
\textbf{scan image} & \verb+bulk_extractor -E hashdb+ \verb+-S hashdb\_mode=scan+ \verb+-S hashdb_scan_path+ \verb+=outdir1/hashdb.hdb -o outdir2+ \verb+my_image+ & Scan image for hashes matching hashes in outdir1/hashdb.hdb\\
\hline
\end{tabular}
\end{table}

\begin{table}[!ht]

\centering
\caption{\bulk \hdb Scanner Options}
\label{tab:hashdb_be_usage}
%\begin{tabular}{|p{3.5 cm}|p{6 cm}|p{4 cm}|}
\begin{tabular}{|p{5 cm}|p{2.0 cm}|p{6.5 cm}|}
\hline \hline
\textbf{Option} & \textbf{Default} & \textbf{Specification} \\
\hline
\verb+hashdb_mode+ & \verb+none+ & The mode for the scanner, one of \verb+[none|import|scan]+. For ``none'', the scanner is active but performs no action. For ``import'', the scanner imports block hashes. For ``scan'', the scanner scans for matching block hashes.\\
\hline
\verb+hashdb_block_size+ &512 & Block size, in bytes, used to generate hashes.\\
\hline
\verb+hashdb_sector_size+ &512 & Sector size, in bytes.  Scans and imports along sector boundaries.\\
\hline
\verb+hashdb_scan_path+ & & The file path to a hash database to scan against.  Valid only in scan mode. No default provided. Value must be specified if in scan mode.\\
\hline
\verb+hashdb_import_repository_+ \verb+name+ & \verb+default_+ \verb+repository+ &Selects the repository name to attribute the import to.  Valid only in import mode.\\
\hline
\verb+hashdb_max_feature_file_+ \verb+lines+ & 0 &The maximum number of feature lines to record or 0 for no limit.  Valid only in scan mode.\\
\hline
\end{tabular}
\end{table}

\section{Use Cases for \hdb}
\label{UseCases}
There are many different ways to utilize the functionality provided by the \hdb tool. In this section, we highlight some of the most common uses of the system.

\subsection{Querying for Source or Database Information}
 Users can scan a hash database directly using various querying commands. Those commands are outlined in Table \ref{tab:scanServices}.  The ``scan'' command allows users to search for hash blocks.\\

\subsection{Writing Software that works with \hdb}
\label{APISection}
\hdb provides Python and C++ APIs that can manage all aspects of a hash database
including importing and scanning [see \textbf{\Autoref{APIs}} for information on using these APIs].
Other software programs can use these APIs to access database capabilities. The file \texttt{hashdb.hpp} found in the \textit{src} directory contains the complete specification of the API. That complete file is also contained in Appendix \ref{hashdbapi} of this document.  The two key features provided by the API include the ability to import values into a hash database and the ability to scan media for any values matching those in a given hash database.  The \bulk program uses the \hdb API to implement both of these capabilities.\\

\subsection{Scanning or Importing to a Database Using \bulk}
\label{bulkextractorSection}
The \bulk \textit{hashdb} scanner allows users to query for fragments of previously encountered hash values and populate a hash database with hash values. Options that control the \textit{hashdb} scanner are provided to \bulk using the ``-S name=value'' command line parameters. When \bulk executes, the parameters are sent directly to the scanner.\\

For example, the following command runs the \bulk \textit{hashdb} scanner in import mode and adds hash values calculated from the disk image \texttt{my\_image} to a hash database:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{bulk_extractor -e hashdb -o outputDir -S hashdb_mode=import my_image}
\end{Verbatim}
Note, \bulk will place feature file and other output not relevant to the \hdb application in the ``outputDir'' directory. When using the import command, the output directory will contain a newly created hash database called \texttt{hashdb.hdb}. That database can then be copied or added to a hash database in another location.


\subsection{Updating Hash Databases}
\label{updateSection}
\hdb provides users with the ability to manipulate the contents of hash databases. The specific command line options for performing these functions are described in Table \ref{tab:databaseManipulation}. \hdb databases are treated as sets with the add, subtract and intersect commands basically using add, subtract and intersect set operations. For example, the following command will  copy all non-duplicate values from \texttt{mock\_video.hdb} into \texttt{mock\_video\_dedup.hdb} :
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb deduplicate mock_video.hdb mock_video_dedup.hdb}
\end{Verbatim}
Whenever a database is created or updated, \hdb updates the file \texttt{log.xml}, found in the database's directory with information about the actions performed.\\

After each command to change a database, statistics about the changes are writen in the \texttt{log.xml} file and to \texttt{stdout}. Table \ref{tab:changeStatistics} shows all of the statistics tracked in the log file along with their meaning. The value of each statistic is the number of times the event happened during the command. For example, if 280 hashes are removed, the statistic ``hashes\_removed'' will be marked with a value of 280. \\

\subsubsection{Update Commands and ``Duplicate'' Hashes}
Commands that add or import hashes of the same value will result in hash duplicates if the source information is unique. If hash and source values are identical (including repository name), no hash values are added with the \textbf{add} or \textbf{import} commands. The \textbf{intersect} and \textbf{subtract} commands do not require source information to match. An intersection occurs when hatches match, regardless of whether the source information matches. Similarly, hash values are also subtracted from the database regardless of whether or not their source information matches.  The update statistics specified in the log file (shown in \ref{tab:changeStatistics}) will specify the results of each of these commands to help users track changes. \\

As discussed previously, users can only specify the repository name with the \textbf{import} command. As databases become large, the repository name for each hash value will help identify important source information. Users should plan on importing data with specific repository names whenever possible to avoid source confusion later.\\

Finally, we provide two philosophies for mitigating duplicate hash bloat:
\begin{itemize}
 \item If you know you have imported the same blacklist data twice,
  and you do not want to manage a 'whitelist' database,
  \texttt{deduplicate} is a quick and easy way to get rid of low-entropy noise.
  \item If your database has blacklist data from more than one source
  or you just want tighter control about what you want to remove
  and are willing to use a 'whitelist' database
  to remove hashes to improve lookup speed
  or to reduce noise about uninteresting hashes found,
  use \texttt{subtract}.
\end{itemize}







\subsection{Optimizing a Hash Database}
\label{optimizing}
 
For large databases, it takes a bit of time to look up a hash value to determine whether it is in the database. This time adds up when scanning for millions of hash values. As an optimization, \hdb provides tuning parameters for tuning the initial hash lookup when checking to see if a hash value is present in the database [see \textbf{\Autoref{Creating}} for information on using these tuning parameters].

\subsection{Exporting Hash Databases}
Users can export hashes from a hash database to a JSON export file using the ``export'' command [see \textbf{\Autoref{InputOutputSyntax}} for information on JSON syntax].  For example, the following command will export the \texttt{mock\_video.hdb} database to the file \texttt{demoVideoHashes.json}:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb export mock_video.hdb demoVideoHashes.xml}
\end{Verbatim}

\section{\hdb Input/Output Syntax}
\label{InputOutputSyntax}
Many of the \hdb commands and API interfaces require or emit data.
This section describes the syntax used and required by \hdb
commands and API interfaces.

\subsection{Tab-delimited Import File}
The \verb+import_tab+ command imports hashes from tab delimited files.
The tab-delimited import file consists of hash lines separated by carriage returns, where each line consists of a filename followed by a tab followed by the file hash followed by a sector index that starts at 1.  Comment lines are allowed by starting them with the \texttt{\#} character.
An example tab-delimited file is shown in Listing \ref{importTabFile}.\\

\lstset{style=customfile}
\begin{lstlisting}[caption={Example tab-delimited import file}, label=importTabFile]
# tab-delimited import file
# <file hexdigest> <tab> <block hash> <tab> <index>
fac7051447c781b69125994c5d125637    3b6b477d391f73f67c1c01e2141dbb17    1
fac7051447c781b69125994c5d125637    89a170b6b9a948d21d1d6ee1e7cdc467    2
fac7051447c781b69125994c5d125637    f58a09656658c6b41e244b4a6091592c    3
\end{lstlisting}

\subsection{Scan List File}
The \verb+scan_list+ command scans a list of hashes for matches.  Valid lines of input may be:
\begin{itemize}
\item Comment lines starting with \verb+#+.  Comment lines are forwarded to output.
\item Hash lines to scan against, where each line consists of a Forensic Path followed by a tab followed by the hash hexcode.
\end{itemize}
An example scan list file is shown in Listing \ref{ScanListFile}.\\

\lstset{style=customfile}
\begin{lstlisting}[caption={Example scan list file}, label=ScanListFile]
# Scan list file
# <Forensic Path> <tab> <block hash hexdigest>
0	3b6b477d391f73f67c1c01e2141dbb17
512	89a170b6b9a948d21d1d6ee1e7cdc467
1024	f58a09656658c6b41e244b4a6091592c
\end{lstlisting}

\subsection{Import/Export Syntax}
The import and export commands and API interfaces communicate source data and block hash data using JSON syntax.\\

\subsubsection{Source Data}
Source data defines information about a source. Source data is identified by the file hash of the source.  An example source data line is shown in Listing \ref{JSONSourceData}. Fields are described in Table \ref{tab:JSONSourceData}.\\

\lstset{style=customfile}
\begin{lstlisting}[caption={Example JSON source data used during import/export}, label=JSONSourceData]
{
  "file_hash":"fac7051447c781b69125994c5d125637",
  "filesize":8000,
  "file_type":"exe",
  "nonprobative_count":4,
  "name_pairs":["repository1","filename1","repo2","f2"]
}
\end{lstlisting}

\begin{table}[!ht]

\centering
\caption{Fields used in JSON source data}
\label{tab:JSONSourceData}
\begin{tabular}{|p{5 cm}|p{8.8 cm}|}
\hline \hline
\textbf{Field} & \textbf{Meaning} \\
\hline
\verb+file_hash+ & The hexdigest of the source file containing the block hash\\
\hline
\verb+filesize+ & The size, in bytes, of the source file\\
\hline
\verb+file_type+ & A classification of what type of file the source file is\\
\hline
\verb+nonprobative_count+ & The number of blocks in the source that are considered to be nonprobative\\
\hline
\verb+name_pairs+ & An array of source name, filename pairs associated with this source\\
\hline
\end{tabular}
\end{table}

\subsubsection{Block Hash Data}
Block hash data is identified by the file block hash.  An example block hash line is shown in Listing \ref{JSONBlockHashData}. Fields are described in Table \ref{tab:JSONBlockHashData}.\\

\lstset{style=customfile}
\begin{lstlisting}[caption={Example JSON block hash data used during import/export}, label=JSONBlockHashData]
{
  "block_hash":"3b6b477d391f73f67c1c01e2141dbb17",
  "entropy":8,
  "block_label":"W",
  "source_offset_pairs":["fac7051447c781b69125994c5d125637", 4096]
}
\end{lstlisting}

\begin{table}[!ht]

\centering
\caption{Fields used in JSON block hash data}
\label{tab:JSONBlockHashData}
\begin{tabular}{|p{5 cm}|p{8.8 cm}|}
\hline \hline
\textbf{Field} & \textbf{Meaning} \\
\hline
\verb+block_hash+ & A block hash hexdigest\\
\hline
\verb+entropy+ & The entropy value calculated for the block\\
\hline
\verb+block_label+ & A label describing the type of data within the block. The block label may include information that it matched a whitelist database during import. The entropy and block label fields may be used together to estimate that a block might be nonprobative\\
\hline
\verb+source_offset_pairs+ & An array of source file hash, source file offset pairs associated with this block hash\\
\hline
\end{tabular}
\end{table}

\subsection{Scan Data}
The \hdb scan commands and the \verb+find_expanded_hash+ scan API interface emit hash and source information relating to hashes matched while scanning. An example scan match is shown in Listing \ref{JSONScanData}. Fields are described in Table \ref{tab:JSONScanData}.\\

\hdb does not emit source or hash information multiple times.  If sources for a hash have already been shown, the sources field will read \verb+"sources":[]+. If a hash has already been shown, the entire JSON output will simply be \verb+{}+.\\

\lstset{style=customfile}
\begin{lstlisting}[float, caption={Example JSON block hash data output from a scan match}, label=JSONScanData]
{
  "entropy": 8,
  "block_label": "W",
  "source_list_id": 57,
  "sources": [{
    "file_hash": "f7035a...",
    "filesize": 800,
    "file_type": "exe",
    "nonprobative_count": 2,
    "name_pairs": ["repository1", "filename1", "repo2", "f2"]
  }],
  "source_offset_pairs": ["f7035a...", 0, "f7035a...", 512]
}
\end{lstlisting}

\begin{table}[!ht]

\centering
\caption{Fields used in JSON scan data}
\label{tab:JSONScanData}
\begin{tabular}{|p{5 cm}|p{8.8 cm}|}
\hline \hline
\textbf{Field} & \textbf{Meaning} \\
\hline
\verb+entropy+ & The entropy value calculated for the block\\
\hline
\verb+block_label+ & A label describing the type of data within the block\\
\hline
\verb+source_list_id+ & A source list ID which is a CRC of the associated source file hashes associated with the block hash\\
\hline
\verb+sources+ & An array of source data for each source matching the hash\\
\hline
\verb+file_hash+ & The hexdigest of the source file containing the block hash\\
\hline
\verb+filesize+ & The size, in bytes, of the source file\\
\hline
\verb+file_type+ & A classification of what type of file the source file is\\
\hline
\verb+nonprobative_count+ & The number of blocks in the source that are considered to be nonprobative\\
\hline
\verb+name_pairs+ & An array of repository name, filename pairs associated with this source\\
\hline
\verb+source_offset_pairs+ & An array of the source file hash, source file offset pairs associated with this block hash\\
\hline
\end{tabular}
\end{table}

\subsection{Size}
The \hdb \verb+size+ command and \verb+size+ API interface returns size information about internal data structures in JSON format. The size of the \verb+source_id_store+ indicates the number of sources. The size of the \verb+hash_store+ is greater than or equal to the number of hashes stored, and is not exact because of how data is stored. Although for internal use, these fields can give some sense of the size of a \hdb database. An example output is shown in Listing \ref{JSONSize}.\\

\lstset{style=customfile}
\begin{lstlisting}[float, caption={Example JSON output of database size values}, label=JSONSize]
{
{
  "hash_data_store":402221,
  "hash_store":401653,
  "source_data_store":101,
  "source_id_store":101,
  "source_name_store":101
}
\end{lstlisting}

\subsection{Sources}
The \verb+sources+ command prints JSON data as shown in Listing \ref{JSONSourceData} and described in Table \ref{tab:JSONSourceData}.\\

\subsection{Histogram}
The \verb+histogram+ command shows the density of hash duplicates across a hash database.  An example histogram output line is shown in Listing \ref{JSONHistogram}.  Fields are described in Table \ref{tab:JSONHistogram}.\\

\lstset{style=customfile}
\begin{lstlisting}[caption={Example JSON histogram format}, label=JSONHistogram]
{
  "duplicates":2,
  "distinct_hashes":3,
  "total":6
}
\end{lstlisting}

\begin{table}[!ht]

\centering
\caption{Fields used in JSON histogram output}
\label{tab:JSONHistogram}
\begin{tabular}{|p{5 cm}|p{8.8 cm}|}
\hline \hline
\textbf{Field} & \textbf{Meaning} \\
\hline
\verb+duplicates+ & The count of source, file offset pairs stored\\
\hline
\verb+distinct_hashes+ & The number of distinct hashes in the database with this duplicates count\\
\hline
\verb+total+ & The total number of hashes represented by this entry, specifically, \verb+duplicates+$*$\verb+distinct_hashes+\\
\hline
\end{tabular}
\end{table}

\subsection{Duplicates}
The \verb+duplicates+ command prints JSON data as shown in Listing \ref{JSONScanData} and described in Table \ref{tab:JSONScanData}.\\

\subsection{Hash Table}
The \verb+hash_table+ command prints JSON data as shown in Listing \ref{JSONScanData} and described in Table \ref{tab:JSONScanData}.\\

\subsection{Timing}
\hdb provides timing data in JSON format for use with timing analysis. Python scripts may use this output to produce performance plots. An example timestamp entry is shown in Listing \ref{JSONTimingData}. Fields are described in Table \ref{tab:JSONTimingData}.\\

\lstset{style=customfile}
\begin{lstlisting}[caption={Example JSON timestamp format}, label=JSONTimingData]
{
  "name":"begin",
  "delta":"0.000396",
  "total":"0.000396"
}
\end{lstlisting}

\begin{table}[!ht]

\centering
\caption{Fields used in JSON timing data}
\label{tab:JSONTimingData}
\begin{tabular}{|p{5 cm}|p{8.8 cm}|}
\hline \hline
\textbf{Field} & \textbf{Meaning} \\
\hline
\verb+name+ & The name of the timestamp\\
\hline
\verb+delta+ & The delta time since the previous timestamp. In this example, the delta is from the time the timestamping started\\
\hline
\verb+total+ & The total time since timestamping started\\
\hline
\end{tabular}
\end{table}



\section{Using the \hdb Library APIs}
\label{APIs}
\hdb provides C++ and Python interfaces for importing, scanning, and working with block hashes.

\subsection{Python}
Install Python module \verb+hashdb+ to use the \hdb Python interfaces.

\subsubsection{Import}
To import hash and source data, open an import manager, for example\\
\verb+{manager = import_manager_t("hashdb.hdb", "create my DB")+. Then use import functions to add data.

\begin{itemize}
\item \verb+manager = import_manager_t(hashdb_dir, cmd)+
\item \verb+manager.insert_source_name(file_binary_hash, repository_name, filename)+
\item \verb+manager.insert_source_data(file_binary_hash, filesize, file_type,+\\
\verb+nonprobative_count)+
\item \verb+manager.insert_hash(binary_hash, file_binary_hash, file_offset,+\\
\verb+entropy, block_label)+
\item \verb+error_message = manager.insert_json(json_string)+\\
Insert hash or source. Return "" on success.
\item \verb+ data_sizes = manager.size()+
\end{itemize}

\subsubsection{Scan}
To scan for hashes, open a scan manager, for example\\
\verb+manager = scan_manager_t("hashdb.hdb")+. Then use functions to find hash and source information. Functions that return less information run faster than functions that return more.

\begin{itemize}
\item \verb+scan_manager = scan_manager_t(hashdb_dir)+
\item \verb+expanded_text = scan_manager.find_expanded_hash(binary_hash)+\\
Find and return JSON text about the hash and its sources. Hash and source information is not returned twice. For example on a second hash match, the returned JSON will just be \verb+{}+.
\item \verb+json_hash_string = scan_manager.find_hash_json(binary_hash)+
\item \verb+approximate_count = scan_manager.find_approximate_hash_count(binary_hash)+\\
This is the fastest scan function. It returns an approximate source, offset pair count for the hash and can be wrong.
\item \verb+count = scan_manager.find_hash_count(binary_hash)+\\
Return the source, offset pair count for the hash.
\item \verb+json_source_string = scan_manager.find_source_json(file_binary_hash)+
\item \verb+first_binary_hash = scan_manager.first_hash()+
\item \verb+next_binary_hash = scan_manager.next_hash(binary_hash)+
\item \verb+first_file_binary_hash = scan_manager.first_source()+
\item \verb+file_binary_hash = scan_manager.next_source(file_binary_hash)+
\item \verb+db_sizes = scan_manager.size()+\\
Return sizes of internal data stores in JSON format.
\item \verb+size_hashes = scan_manager.size_hashes()+\\
Return the number of hash records in the database, which is typically more than the number of source, offset pairs in the database.
\item \verb+size_sources = scan_manager.size_sources()+\\
Return the number of sources in the database.
\end{itemize}

\subsubsection{Settings}
Holds \hdb settings.

\begin{itemize}
\item \verb+settings = settings_t()+\\
Obtain default settings. Configurable setting parameters are: \verb+settings_version,+\\
\verb+sector_size, block_size, max_source_offset_pairs, hash_prefix_bits,+\\
\verb+hash_suffix_bytes+.
\item \verb+settings_string = settings.settings_string()+\\
Return setting values in JSON format.
\end{itemize}

\subsubsection{Timestamp}
Provide timestamp support.

\begin{itemize}
\item \verb+timestamp = timestamp_t()+\\
Create a timestamp object.
\item \verb+timestamp_string = stamp(text)+\\
Create a named timestamp and provide time and delta from the last stamp time in JSON format.
\end{itemize}

\subsubsection{Support Functions}
Support functions provide miscellaneous support and are not part of a class.
\begin{itemize}
\item \verb+version = version()+\\
Return the \hdb version.
\item \verb+error_message = create_hashdb(hashdb_dir, settings)+\\
Create a hash database given settings. Return "" else reason for failure.
\item \verb+error_message = read_settings(hashdb_dir, &settings)+\\
Query settings else false and reason for failure.
\item \verb+binary_string = hex_to_bin(hex_string)+
\item \verb+hex_string = bin_to_hex(binary_string)+
\item \verb+import_recursive_dir(hashdb_dir, whitelist_dir, min_entropy,+\\
\verb+max_entropy, repository_name, import_path)+\\
Import from path to \hdb. zzz NOT AVAILABLE IN ALPHA.
\end{itemize}

\subsection{C++}
To use C++ interfaces, include interface file \verb+hashdb.hpp+ and link \hdb library \verb+libhashdb+. \hdb interfaces use the \hdb namespace. Interfaces can assert on unexpected error. For more detail, please see \hdb header file \texttt{hashdb.hpp} in \textbf{\Autoref{hashdbapi}}.\\

\subsubsection{Import}
Import hashes. Interfaces use lock for DB safety. The destructor appends changes to change log.

\begin{itemize}
\item \verb+import_manager_t(hashdb_dir, cmd)+
\item \verb+void insert_source_name(file_binary_hash, repository_name, filename)+
\item \verb+void insert_source_data(file_binary_hash, filesize, file_type, nonprobative_count)+\\
Change if different.
\item \verb+void insert_hash(binary_hash, file_binary_hash, file_offset, entropy, block_label)+\\
Change if different
\item \verb+std::string insert_json(json_string)+\\
Insert hash or source, return \verb+error_message+ or "" for no error.
\item \verb+std::string size()+\\
Return number of entries in LMDB databases.
\item \verb+size_t size_hashes()+\\
Return number of hash records in the database, which is typically more than the number of source, offset pairs in the database.
\item \verb+size_t size_sources()+\\
Return number of sources.
\item \verb+~import_manager_t()+\\
Append changes to change log at \verb+hashdb_dir/log.dat+.
\end{itemize}

\subsubsection{Scan}
Scan for hashes. Provide read-only access to hash and source data stores.

\begin{itemize}
\item \verb+scan_manager_t(hashdb_dir)+
\item \verb+std::string find_expanded_hash(binary_hash)+\\
Find and return JSON text else "" on no match.
\item \verb+bool find_hash(binary_hash, &entropy, &block_label, &source_offset_pairs_t)+\\
\item \verb+std::string find_hash_json(binary_hash)+\\
Return \verb+json_hash_string+ else "" on no match.
\item \verb+size_t find_approximate_hash_count(binary_hash)+\\
Return approximate source, offset pair count for the hash, and can be wrong.
\item \verb+size_t find_hash_count(binary_hash)+\\
Return the source, offset pair count for the hash.
\item \verb+bool find_source_data(file_binary_hash, &filesize, &file_type,+\\
\verb+&nonprobative_count)+
False on no source file binary hash.
\item \verb+bool find_source_names(file_binary_hash, &source_names_t)+\\
False on no source file binary hash.
\item \verb+std::string find_source_json(file_binary_hash)+\\
Return \verb+json_source_string+ else "" on no match.
\item \verb+std::string first_hash()+
\item \verb+std::string next_hash(binary_hash)+
\item \verb+std::string first_source()+
\item \verb+std::string next_source(file_binary_hash)+
\item \verb+string size()+\\
Return number of entries in LMDB databases.
\item \verb+size_t size_hashes()+\\
Return number of hash records in the database, which is typically more than the number of source, offset pairs in the database.
\item \verb+size_t size_sources()+\\
Return number of sources.
\end{itemize}

\subsubsection{Settings}
Hold \hdb settings.

\begin{itemize}
\item \verb+settings_t()+\\
Create a settings object with default values.
\item \verb+std::string settings_string()+\\
Return setting values in JSON format.
\end{itemize}

\subsubsection{Timestamp}
Provide timestamp support.

\begin{itemize}
\item \verb+timestamp_t()+
\item \verb+std::string stamp(text)+\\
Return stamp in JSON format.
\end{itemize}

\subsubsection{Support Functions}

\begin{itemize}
\item \verb+extern C char* version()+\\
Return the \hdb version.
\item \verb+bool create_hashdb(hashdb_dir, settings, &error_message)+\\
Create a hash database given settings. Return true and "" else false and reason for failure.
\item \verb+bool read_settings(hashdb_dir, &settings, &error_message)+\\
Query settings else false and reason for failure.
\item \verb+print_enviornment(command_line_string, &ostream)+\\
Print command and environment to output stream. - zzzz this interface may be removed.
\item \verb+import_recursive_dir(hashdb_dir, whitelist_dir, min_entropy,+\\
\verb+max_entropy, repository_name, import_path)+\\
Import from path to \hdb.
\end{itemize}

\subsubsection{Data Types}

The following data types are defined.

\begin{itemize}
\item \verb+source_offset_pair_t: typedef pair<file_binary_hash, file_offset> id_offset_pair_t+
\item \verb+source_offset_pairs_t: typedef set<source_offset_pair_t> source_offset_pairs_t+
\item \verb+source_name_t: typedef pair<repository_name, fillename> source_name_t+
\item \verb+source_names_t: typedef set<source_name_t> source_names_t+
\end{itemize}

\section{LMDB Data Stores}
This section provides details of how LMDB data stores are managed within a \hdb database. This technical information is provided to give context behind optimization settings and optimized programming interfaces available in \hdb, and is intended for advanced users.\\

\subsection{LMDB Hash Store}
The \textit{LMDB Hash Store} is a highly compressed optimized store of all the block hashes in the database.  When scanning for a hash, if it is not in this store, then it is not in the database.  Because of the degree of optimization, there can be false positives.  To compensate, when a hash is found in the \textit{LMDB Hash Store}, \hdb reads the \textit{LMDB Hash Information Store} to be sure the hash actually exists.\\

The \textit{LMDB Hash Store} is a hybrid B-Tree and List store:
\begin{itemize}
\item The \verb+key+ portion consists of the first few bits of the hash, with enough bits so that the \verb+value+ portion is not too large.
\item The \verb+value+ portion consists of a list of the last few bytes of the hash, with enough bytes so that the bits of the hash in the \verb+key+ and the bytes of the hash in the \verb+value+ collectively identify the hash as present with a sufficiently low false positive rate.
\end{itemize}

The \textit{LMDB Hash Store} uses two tuning parameters, hash prefix bits and hash suffix bytes, to minimize data store size and maximize lookup speed.  These parameters are described in \textbf{\Autoref{Settings}}.\\


\subsection{LMDB Hash Information Store}
The \textit{LMDB Hash Information Store} is a store of all hashes and their associated data and source information:

\begin{itemize}
\item The \verb+key+ portion consists of a block hash, in binary.
\item The \verb+value+ portion contains metadata fields about the hash plus a list of locations in sources where the block was identified.  Specifically, the \verb+value+ portion consists of:
  \begin{itemize}
  \item \verb+entropy+ The calculated entropy for the block.
  \item \verb+block_label+ A label identifying information about the block.  Users may wish to examine \verb+entropy+ and \verb+block_label+ together to estimate that a block might be nonprobative.
  \item A list of source IDs and file offset pairs indicating places where the block was identified.
  \end{itemize}
\end{itemize}

The \textit{LMDB Hash Data Store} uses two tuning parameters, \verb+max_id_offset_pairs+ and \verb+block_size+ to minimize data store size.  These parameters are described in \textbf{\Autoref{Settings}}.\\

\subsection{LMDB Source ID Store}
The \textit{LMDB Source ID Store} maps source file hash values to source IDs.  Although the user never sees source IDs, we use source IDs in the \textit{LMDB Source ID Store}, \textit{LMDB Source Data Store}, and the \textit{LMDB Hash Information Store} because they are significantly shorter than source file hashes.  We wouldn't need source IDs if we didn't make this optimization.

\begin{itemize}
\item The \verb+key+ is the \verb+file_binary_hash+.
\item The \verb+value+ is the \verb+source_id+.
\end{itemize}

\subsection{LMDB Source Data Store}
The \textit{LMDB Source Data Store} holds all the metadata about sources:
\begin{itemize}
\item The \verb+key+ is the \verb+source_id+.
\item The \verb+value+ consists of these fields:
  \begin{itemize}
  \item \verb+file_binary_hash+ The source file hash associated with this source ID, in binary.
  \item \verb+filesize+ The size of the source file, in bytes.
  \item \verb+file_type+ A label indicating the type of the file, user defined.
  \item \verb+nonprobative_count+ The number of block hashes stored for this source which are considered to be nonprobative.  Users may wish to set the \verb+nonprobative_count+ value based on the \verb+entropy+ and \verb+block_label+ values of each block in the source.
  \end{itemize}
\end{itemize}

\subsection{LMDB Source Name Store}
The \textit{LMDB Source Name Store} multimap maps source IDs to source names.  This store allows us to not re-import hashes from the same source and also allows us to see the list of source names that are of the same source.
\begin{itemize}
\item The \verb+key+ is the \verb+source_id+.
\item The \verb+value+ is a name pair of:
  \begin{itemize}
  \item \verb+repository_name+ A label indicating the source repository.
  \item \verb+filename+ The path to this source.
  \end{itemize}
\end{itemize}

\bibliographystyle{acm} 
\bibliography{references}

\newpage
\appendix
\appendixpage

\section{\hdb Quick Reference}
\label{QuickReference}
\input ../hashdb_quick_reference/hashdb_quick_reference_text.tex
\newpage

\section{Output of the \hdb Help Command}
\label{HelpOutput}
\begingroup
\footnotesize
{
\fontfamily{courier}\selectfont
\verbatiminput{hashdb_usage.txt}
}
\endgroup


\section{\hdb C++ API: \texttt{hashdb.hpp}}
\label{hashdbapi}
\lstset{language=C++}
\lstset{basicstyle=\footnotesize}
\lstset{breaklines=true}
\lstset{breakatwhitespace=true}
\lstinputlisting{../../src_libhashdb/hashdb.hpp}

\end{document}
