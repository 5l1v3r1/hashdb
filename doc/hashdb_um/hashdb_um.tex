\documentclass[11pt,fleqn]{article} % Default font size and left-justified equations

%\usepackage{standalone}

\usepackage{todonotes}
\usepackage{color}
% use \todo{note} OR \missingfigure{Add my picture here}

\include{structure}

\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}
 
%\usepackage{arabtex}

\usepackage{verbatim}

\raggedbottom

\begin{document}

%define macros for commonly used terms that require special formatting
\newcommand \hdb {\textit{hashdb}\xspace}
\newcommand \sscope {\textit{SectorScope}\xspace}
\newcommand \aut {\textbf{Autopsy}\xspace}
\newcommand \bulk {\textbf{bulk\_extractor}\xspace}

\hypersetup{%
    pdfborder = {0 0 0}
}

\lstdefinestyle{customfile}{
basicstyle=\footnotesize\ttfamily, frame=single, float=htpb}

\input{./title.tex}

\pagenumbering{roman}
\setlength{\parindent}{0pt} %remove indenting from whole document
\newpage
\thispagestyle{empty}
\mbox{}
\newpage


\tableofcontents
\newpage
\pagenumbering{arabic}





\newpage

\section{Introduction}
\subsection {Overview of \hdb}
\hdb is a tool that can be used to find data in raw media using cryptographic hashes calculated from blocks of data. It is a useful forensic investigation tool for tasks such as malware detection, child exploitation detection or corporate espionage investigations. The tool provides several capabilities that include:
\begin{itemize}
\item Creating hash databases of MD5 block hashes, as opposed to file hashes.
\item Importing block hash values.
\item Scanning the hash database for matching hash values.
\item Providing the source information for hash values. 
\end{itemize}

Using \hdb, a forensic investigator can take a known set of blacklisted media and generate a hash database. The investigator can then use the hash database to search against raw media for blacklisted information. For example, given a known set of malware, an investigator can generate a sector hash database representing that malware. The investigator can then search a given corpus for fragments of that malware and identify the specific malware content in the corpus.\\

\hdb relies on block hashing rather than full file hashing. Block hashing provides an alternative methodology to file hashing with a different capability set. With file hashing, the file must be complete to generate a file hash, although a file carver can be used to pull together a file and generate a valid hash.  File hashing also requires the ability to extract files, which requires being able to understand the file system used on a particular storage device. Block hashing, as an alternative, does not need a file system or files. Artifacts are identified at the block scale (usually 512 bytes) rather than at the file scale. While block hashing does not rely on the file system, artifacts do need to be sector-aligned for \hdb to find hashes \cite{hashEncoding}.\\

\hdb provides an advantage when working with hard disks and operating systems that fragment data into discontiguous blocks yet still sector-align media. This is because scans are performed along sector boundaries. Because \hdb works at the block resolution, it can find part of a file when the rest of the file is missing, such as with a large video file where only part of the video is on disk. \hdb can also be used to analyze network traffic (such as that captured by \textbf{tcpflow}).  Finally, \hdb can identify artifacts that are sub-file, such as embedded content in a \texttt{.pdf} document.\\

\hdb stores cryptographic hashes (along with their source information) that have been calculated from hash blocks. It also provides the capability to scan other media for hash matches.
This manual describes uses cases for the \hdb tools, including usage with \aut, \sscope, \bulk, and the \hdb Python and C++ libraries, and demonstrates how users can take full advantage of all of its capabilities.

\subsection{Intended Audience}
This Users Manual is intended to be useful to new, intermediate and experienced users of \hdb. It provides an in-depth review of the functionality included in \hdb and shows how to access and utilize features through command line operation of the tool. This manual includes working examples with links to the input data used, giving users the opportunity to work through the examples and utilize all aspects of the system.  This manual also introduces Forensic tools that use \hdb.\\

For developers, this manual provides in-depth coverage of the data syntax used by \hdb and for interfacing with \hdb using the \hdb \textbf{c++} and \textbf{Python} interfaces.

\subsection{\hdb Resources}
Users are encouraged to visit the \hdb Wiki page at \url{https://github.com/NPS-DEEP/hashdb/wiki} for quick links to downloads, documentation, and examples.\\

All \hdb users should join the \bulk users Google group for more information and help with any issues encountered. To join, send an email to \textbf{bulk\_extractor-users+subscribe@} \textbf{googlegroups.com}.  \\

Several articles are available related to block hashing, and its practical and research applications. Some of those articles are specifically cited throughout this manual. Here are some additional references we recommend:
\begin{itemize}
\item Michael McCarrin, Bruce Allen. Rapid Recognition of Blacklisted Files and Fragments. Naval Postgraduate School. \url{http://www.osdfcon.org/presentations/2015/McCarrin-Allen_osdfcon.pdf}.
\item Jim Jones, Tahir Khan, Kathryn Laskey, Alex Nelson, Mary Laamanen, Doug White.  Inferring Past Activity from Partial Digital Artifacts. George Mason University, National Institute of Standards and Technology. \url{http://www.osdfcon.org/presentations/2015/Jim-Jones_EtAl-Release.pdf}.
\item Simson Garfinkel, Michael McCarrin. Hash-based Carving: Searching media for complete files and file fragments with sector hashing and hashdb. DFRWS 2015 USA. \url{http://www.sciencedirect.com/science/article/pii/S1742287615000468}
\item Joel Young, Kristina Foster, Simson Garfinkel, Kevin Fairbanks. Distinct Sector Hashes for Target File Detection. \url{http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&arnumber=6311397}.
\item Garfinkel, Simson, Alex Nelson, Douglas White and Vassil Rousseve. Using purpose-built functions and block hashes to enable small block and sub-file forensics. Digital Investigation. Volume 7. 2010. Page S13--S23. \url{http://www.dfrws.org/2010/proceedings/2010-302.pdf}.
\item Foster, Kristina. Using Distinct Sectors in Media Sampling and Full Media Analysis to Detect Presence of Documents From a Corpus. Naval Postgraduate School Masters Thesis, September 2012. \url{http://calhoun.nps.edu/public/handle/10945/17365}.
\end{itemize}

\subsection{Conventions Used in this Manual}
This manual uses standard formatting conventions to highlight file names, directory names and example commands. The conventions for those specific types are described in this section. \\

Names of programs including the post-processing tools native to \hdb and third-party tools are shown in \textbf{bold}, as in \textbf{bulk\_extractor}.\\

File names are displayed in a fixed width font. They will appear as \texttt{filename.txt} within the text throughout the manual.\\

Directory names are displayed in italics. They appear as \textit{directoryname/} within the text. The only exception is for directory names that are part of an example command. Directory names referenced in example commands appear in the example command format.\\

Database names are denoted with bold, italicized text. They are always specified in lower-case, because that is how they are referred in the options and usage information for \hdb. Names will appear as \textbf{\textit{databasename}}.\\

This manual contains example commands that should be typed in by the user. A command entered at the terminal is shown like this: \begin{Verbatim}[commandchars=\\\{\}]
\verbbf{command}
\end{Verbatim}

The first character on the line is the terminal prompt, and should not be typed. The black square is used as the standard prompt in this manual, although the prompt shown on a users screen will vary according to the system they are using.\\

\subsection{Changes Over the \hdb v2.0.1 Release}
\hdb Version 3 provides significant functional and performance improvements over v2.0.1:

\begin{itemize}
\item False positive block matches may be evaluated because metadata about hashes and sources are now being stored:
  \begin{itemize}
  \item Block labels and block entropy values indicate characteristics about data blocks.
  \item Source type, zero count, and nonprobative count of a source indicate the density of useful blocks within a source.
\end{itemize}
\item Sources are now tracked by source hash rather than by name.  This fixes two problems:
  \begin{itemize}
  \item By not storing duplicates, source relevance and similarity between sources may be weighed.
  \item Groups of identical sources are readily identified.
  \end{itemize}
\item Bulky output from scans has been significantly reduced:
  \begin{itemize}
  \item Information about matched sources and hashes are returned only once and are not reprinted if a source or hash is matched again.
  \item Information is returned in the more condensed JSON format rather than in XML.
  \end{itemize}
\item A complete \hdb API is now available for C++ and Python.
  \begin{itemize}
  \item A scan interface supports scan functions and functions for reading all hash and source information.
  \item An import interface supports functions for importing hash and source information.
  \item Free functions support access to settings and higher-layer capabilities.
  \end{itemize}
\item The database has been retuned to improve scan and import speed:
  \begin{itemize}
  \item Several tuned scan interfaces are available:
    \begin{itemize}
    \item \verb+find_expanded_hash_json+ scans for matches and returns complete match information in JSON format.  Matched sources and hashes are cached so that information is not reprinted in other matches.
    \item \verb+find_hash+ returns match information in a data structure.
    \item \verb+find_hash_count+ only returns a match count and does not take time to parse match information into a data structure.
    \item \verb+find_approximate_hash_count+ is fast because it does not read the hash information store when there is a match, but it can have false positives in its matching and in its count.
    \end{itemize}
  \item The Bloom filter has been removed in favor of the dense hash store.
  \end{itemize}
\item Ingest, image scan, and image read interfaces are added to \hdb. \bulk is no longer required to perform these functions.
\item The build process has been restructured to support parallel build trees (VPATH builds).  The goal is to support compiling to additional targets such as the ARM processor.
\end{itemize}

\subsection{Licensing}
\hdb code is provided with the following notice:\\
\fbox{\begin{minipage}{37em}
The software provided here is released by the Naval Postgraduate School, an agency of the U.S. Department of Navy. The software bears no warranty, either expressed or implied. NPS does not assume legal liability nor responsibility for a User's use of the software or the results of such use.\\

Please note that within the United States, copyright protection, under Section 105 of the United States Code, Title 17, is not available for any work of the United States Government and/or for any works created by United States Government employees.  User acknowledges that this software contains work which was created by NPS government employees and is therefore in the public domain and not subject to copyright.\\

However, because hashdb includes source modules, the compiled hashdb
executable may be covered under a different copyright.\\

\texttt{rapidjson} is Copyright (C) 2015 THL A29 Limited, a Tencent company, and Milo Yip.  All rights reserved.\\

\texttt{liblmdb} is Copyright 2011-2016 Howard Chu, Symas Corp.  All rights reserved.\\

\texttt{libewf} is Copyright 2007 Free Software Foundation, Inc.\\

\texttt{crc32.h} is COPYRIGHT (C) 1986 Gary S. Brown.\\
\end{minipage}}

\subsection{Obtaining \hdb}
\label{Obtaining}
The \hdb tool and API interface library are readily available for Windows systems, Linux flavors, and MacOS.  A Windows installer is available for Windows users.  A source code distribution is available for Linux and Mac users.  Developers may download \hdb directly from source available on GitHub.\\

Steps for installing \hdb on Windows and one flavor of Linux are described here. For more installation options, please refer to the installation page on the \hdb Wiki at \url{https://github.com/NPS-DEEP/hashdb/wiki/Installing-hashdb}.\\

For information on installing \sscope and \bulk tools which use \hdb, Please see \textbf{\Autoref{OtherTools}}.\\

\subsubsection{Installing on Windows}
Windows users should download the Windows Installer for \hdb. The file to download is located at \url{http://digitalcorpora.org/downloads/hashdb} and is called \texttt{hashdb-x.y.} \texttt{z-windowsinstaller.exe} where x.y.z is the latest version number (1.1.1 as of publication of this manual).\\

You should close all Command windows before running the installation executable. Windows will not be able to find the \hdb tools in a Command window if any are open during the installation process. If you do not do this before installation, simply close all Command windows after installation. When you re-open, Windows should be able to find \hdb.\\

 Next run the \texttt{hashdb-x.y.z-windowsinstaller.exe} file. This will automatically install \hdb on your machine. Some Windows safeguards may try to prevent you from running it. Figure \ref{fig:windowsWarning} shows the message Windows 8 displays when trying to run the installer. To run anyway, click on ``More info'' and then select ``Run Anyway.'' \\
\begin{figure}
	\center
	\includegraphics[scale=.5]{windowsWarning.png}
	\caption{Windows 8 warning when trying to run the installer. Select ``More Info'' and then ``Run Anyway.''}
	\label{fig:windowsWarning}
\end{figure}

When the installer file is executed, the installation will begin and show a dialog like the one shown in Figure \ref{fig:windowsInstaller}.  Users should select all options needed:
\begin{itemize}
\item \textbf{hashdb tool}\\
Installs the \hdb tool into the \verb+Program Files+ directory and installs the Users Manual shortcut in the \verb+Start+ menu.
\item \textbf{Add to PATH}\\
Appends the path to the \hdb tool to the System \verb+PATH+ variable so that it can be found at the command prompt and by other tools.
\item \textbf{hashdb Python module}\\
Installs the following files onto the desktop at \verb+Users\Public\Desktop+:
  \begin{itemize}
  \item \verb+hashdb.py+\\
  The \hdb Python interface file.
  \item \verb+_hashdb.pyd+\\
  The \verb+.dll+ file needed by \verb+hashdb.py+.
  \item \verb+test_hashdb_module.py+\\
  A small test program for helping to validate and diagnose the installation of \verb+hashdb.py+ and \verb+hashdb.py+. This file may be delted.
  \end{itemize}

Suggestions for managing these files include:
  \begin{itemize}
  \item Move these files to your working directory so that they can be found by your Python program.
  \item Move these files to another directory and set \verb+PATH+ to include the path to \verb+_hashdb.pyd+ and set \verb+PYTHONPATH+ to include the path to \verb+hashdb.py+.
  \end{itemize}
\end{itemize}

\begin{figure}
	\center
	\includegraphics[scale=.8]{WindowsInstaller.png}
	\caption{Dialog appears when the user executes the Windows Installer. Select the default configuration to install all components.}
	\label{fig:windowsInstaller}
\end{figure}

\hdb is now installed on your system can be run from the command line.\\

\subsubsection{Installing on Linux or Mac}
This section describes steps for installing \hdb on a Fedora 23 system and is intended to illustrate the installation process.  For steps on installing \hdb to other flavors of Linux or for MacOS, and for installing specific configurations, please refer to the installation page on the \hdb Wiki at \url{https://github.com/NPS-DEEP/hashdb/wiki/Installing-hashdb}.\\

Before compiling \hdb for your platform, you may need to install other packages on your system which \hdb requires to compile cleanly and with a full set of capabilities.\\

\textbf{Dependencies}\\
The following commands should add the requisite packages:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{sudo dnf update}
\verbbf{sudo dnf groupinstall development-tools}
\verbbf{sudo dnf install gcc-c++}
\verbbf{sudo dnf install openssl-devel}
\verbbf{sudo dnf install libewf-devel}
\verbbf{sudo dnf install swig}
\verbbf{sudo dnf install python-devel}
\end{Verbatim}

\textbf{Download and Install \hdb}\\
Next, download the latest version of \hdb. The software can be downloaded from \url{http://digitalcorpora.org/downloads/hashdb/}. The file to download is \texttt{hashdb-x.y.z.tar.gz} where x.y.z is the latest version.\\

After downloading the file, un-tar it by either right-clicking on the file and choosing ``extract to...' or typing the following at the command line:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{tar -xvf hashdb-x.y.z.tar.gz}
\end{Verbatim}

Then, in the newly created \textit{hashdb-x.y.z} directory, run the following commands to install \hdb in \textit{/usr/local/bin} (by default):

\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{./configure}
\verbbf{make}
\verbbf{sudo make install}
\end{Verbatim}
\hdb is now installed on your system and can be run from the command line. \\

Note: sudo is not required. If you do not wish to use sudo,  build and install \hdb and \bulk in your own space at ``\$HOME/local'' using the following commands:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{./configure --prefix=$HOME/local/ --exec-prefix=$HOME/local CPPFLAGS=-}
\textbf{                         I$HOME/local/include/ LDFLAGS=-L$HOME/local/lib/}
\verbbf{make}
\verbbf{make install}
\end{Verbatim}

\textbf{Run \hdb}\\
When installed as administrator, the \hdb tool should automatically be accessible. The \hdb Python interface can be made available by typing:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{export PYTHONPATH=/usr/local/lib/python2.7/site-packages:/usr/local/}
\textbf{                         lib64/python2.7/site-packages}
\end{Verbatim}

which provides access the installed \verb+python.py+ and \verb+_python.so+ resources, respectively.\\

When installed as a user, the \hdb tool can be made available by typing:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{export PATH=$HOME/local/bin:$PATH}
\end{Verbatim}

The \hdb Python interface can be made available by typing:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{export PYTHONPATH=~/local/lib/python2.7/site-packages:~/local/lib64/}
\textbf{                         python2.7/site-packages}
\end{Verbatim}

which provides access the installed \verb+python.py+ and \verb+_python.so+ resources, respectively.\\

\subsubsection{Quickstart Guide}
The following steps provide a very brief introduction to running your new installation of \hdb. 
\begin{enumerate}
\item Navigate to the directory where you would like to create a hash database. Then, to run \hdb from the command line, type the following instructions: 
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb create demo.hdb}
\end{Verbatim} 

In the above instructions, \texttt{\textbf{demo.hdb}} is the empty database that will be created with default database settings.

\item Next, import data into the database. In this example, lets import hashes from the Kitty Material demo dataset available at \url{http://digitalcorpora.org/corpora/scenarios/2009-m57-patents/KittyMaterial/import}. But rather than downloading these files and importing them, lets just import the pre-made \verb+KittyMaterial.json+ data available at \url{http://digitalcorpora.org/downloads/hashdb/demo/KittyMaterial.json}:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb import demo.hdb blacklist_files}
\end{Verbatim} 
This command, if executed successfully, will print \verb+import completed+, along with statistics indicating changes to the database.

\item Next, scan a media image for matching hashes. In this example, lets scan the demo media image available at \url{ http://digitalcorpora.org/corpora/scenarios/2009-m57-patents/drives-redacted/jo-favorites-usb-2009-12-11.E01} which contains blacklisted Kitty block hashes. This can be done using the Python interface, \sscope, or \bulk, but lets just use the \hdb tool:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb scan_image demo.hdb jo-favorites-usb-2009-12-11.E01}
\end{Verbatim} 
With this image and dataset, the first block hash matched is at media image offset \verb+2543104+ for hash \verb+1d7379fd4d5cf676a9d4de1e48337e71+:

\begingroup
\footnotesize
\begin{Verbatim}[fontfamily=courier]
2543104	1d7379fd4d5cf676a9d4de1e48337e71	{"entropy":0.5,
"block_label":"","source_list_id":1193146442,"sources":
[{"file_hash":"1dd00f2e51aeebe7541cea4ade2e20b5","filesize":1549288,
"file_type":"","zero_count":1,"nonprobative_count":10,"name_pairs":
["default_repository","KittyMaterial/HighQuality/DSC00003.JPG"]}],
"source_offset_pairs":["1dd00f2e51aeebe7541cea4ade2e20b5",0]}
\end{Verbatim}
\endgroup

\end{enumerate}

\section{How \hdb Works}
The \hdb tool provides capabilities to create, edit, access and search databases of cryptographic hashes created from hash blocks. The cryptographic hashes are imported into a database from a directory, another database, \textbf{bulk\_extractor} or JSON data, or trough the \hdb API.
Once a databases is created, \hdb provides users with the capability to scan the database for matching hash values and identify matching content. Hash databases can be exported, added to, subtracted from and shared.\\


Figure \ref{fig:overviewFigure} provides an overview of the capabilities included with the \hdb tool. \hdb populates databases from whitelist source files
or other media provided in JSON format or through the API.
Users can add or remove data from the database after it is created.
Once the database is populated, \hdb can export content from the database in JSON format. It also provides an API that can be used by third party tools (as it is used in the \bulk program) to create, populate and access hash databases.\\

\begin{figure}
	\center
	\includegraphics[scale=.45]{drawings/hashdb_system_overview}
	\caption{Overview of the \hdb system}
	\label{fig:overviewFigure}
\end{figure}

\subsection{Block Hash}
\hdb works by matching hashes calculated from blocks of data.  \hdb is different from tools that match files because it can find matches even when part of a file is missing or changed.  \hdb stores and scans for hashes created from contiguous blocks of data.  We call the size of the block hashed the \textit{block size}.  \hdb stores and scans for hashes in step increments along a hash interval. Blocks hashed at step-sized intervals are illustrated in Figure \ref{fig:hashInterval}.\\

\begin{figure}
	\center
	\includegraphics[scale=.45]{drawings/hash_interval}
	\caption{Data blocks are hashed along an interval of bytes}
	\label{fig:hashInterval}
\end{figure}

\begin{figure}
	\center
	\includegraphics[scale=.45]{drawings/default_hash_interval}
	\caption{By default, hashes are calculated from 512 byte blocks of data along 512 byte intervals and the database uses a byte alignment of 512}
	\label{fig:defaultHashInterval}
\end{figure}

As an optimization, \hdb provides a byte alignment setting. The byte alignment value must be divisible by the step size. The default configuration with 512 for step size, block size, and byte alignment is shown in Figure \ref{fig:defaultHashInterval}. Byte alignment is described in \textbf{\autoref{Settings}}.\\

\subsection{Blacklist Data}
Blacklist data is the data we scan against to determine whether forensic data
contains probative artifact.
We build a hash database of blacklist data by importing block hashes
from blacklist files, copying from other hash databases,
or importing from other sources using data prepared in JSON format.\\

Each block hash in the database contains one or more source, file offset pairs indicating a source and location within the source where the hash is located.  If a block is duplicated within a source, then several source, file offset pairs will be stored for that hash.  If a block hash is also found in another source, then source, file offset pair(s) from that source will also be stored for that hash. Block hashes with many source, file offset pairs tend to contain non-probative data.\\

\subsection{Repositories}
Blacklist data may come from multiple sources called ``repositories''. \hdb tracks repository names in order to know what categories blacklist data belongs to. When importing into a database, users may provide repository names specific to balcklist categories or cases, or allow \hdb to select default values.  When scanning, hashes may match sources from several repositories.\\

\subsection{Forensic Data}
Forensic data is the data we scan to see if it contains artifact matching
that in our hash database.
Note that just having matches is not sufficient to be considered probative.
Some matches are common to many files.
\hdb tracks entropy and data information to automate the process
of eliminating many false positives.
Direct analysis such as that provided by the \sscope tool
may be used to see the exact content at that location.
\sscope is available at \url{https://github.com/NPS-DEEP/NPS-SectorScope/wiki}.

\subsection{Forensic Path}
A forensic path is a byte offset into a media image. However, when recursive mode is used to uncompress data, the forensic path includes information for pointing to uncompressed data and also the compression algorithm used to reach that data. An example forensic path of \texttt{1000-zip-2100} at filename \texttt{myfile-100-zip} extracted from filename \texttt{myfile} is shown in Figure \ref{fig:forensicPath}.\\

\begin{figure}
	\center
	\includegraphics[scale=.45]{drawings/forensic_path}
	\caption{Example forensic path to an extracted file}
	\label{fig:forensicPath}
\end{figure}

\subsection{File Hash}
\hdb tracks sources by their file hash rather than by their filename
or repository name.  This approach provides several benefits:

\begin{itemize}
\item The database does not store block hashes from multiple sources
when the sources are actually the same file.
\item Source filenames and repository names for the same file are grouped
together and may be looked up by their file hash value.
\end{itemize}

\subsection{Managing False Positives}
A significant problem when scanning for probative blocks is dealing with false positives \cite{hashBasedCarving}. False positives arise from data that is easily generated or commonly duplicated such as sparse data or lookup tables. \hdb records and uses information about blocks and sources in order to identify blocks as nonprobative. Then, post-processing tools such as \sscope can readily evaluate matched blocks with these false positives removed.\\

Here we describe the data that \hdb stores with hashes and sources. How this data is used to classify blocks as nonprobative is a complex issue. \hdb stores this data. It is up to post-processing tools such as \sscope to evaluate it.\\

\begin{itemize}
\item \textbf{Entropy}\\
We calculate the entropy of data blocks and use this value to help estimate that the block may be nonprobative. Blocks with a low entropy value are often nonprobative. \hdb calculates the Shannon entropy of blocks using an alphabet of $2^{16}$ values. Unfortunately, data in blocks can be a member of many types of alphabets. \hdb uses one algorithm.  For improved results, we recommend considering the type of data along with the calculated entropy when estimating that a block may be nonprobative. The entropy field is new to \hdb v3.
\item \textbf{Block Label}\\
Block labels may be used to hold information about the nature of the block.  For example it might be used to indicate that the byte values increment, indicating a homogeneous data structure \cite{hashBasedCarving}.
\item \textbf{File Hash}\\
Matched source files are indexed by file hash (new to \hdb v3). The \sscope tool uses this value to visualize how specific source files are distributed across a media image.
\item \textbf{Repository Name and Filename}\\
The list of repository names and filenames associated with a source file are provided, enabling a comprehensive indication of what a hash match is a member of.
\item \textbf{Source File Size}\\
The source file size indicates how big a source file is. The \sscope tool uses this value to know what percentage of a source file is matched in a scan.
\item \textbf{Source File Type}\\
This field stores information about the type of the source file. This field is not used by the \hdb Tool but is available through the \hdb API interfaces.
\item \textbf{Source File Nonprobative Count}\\
The nonprobative count indicates how many blocks of the source file are deemed nonprobative. In the \bulk \hdb scanner import function and in the \hdb tool \verb+ingest+ function, this value is set to the number of blocks that have been given a block label.
\end{itemize}

\subsection{Building a \hdb Database}
There are several ways to populate a database:

\begin{itemize}
\item Using the \hdb \verb+import+ command.
\item Importing from correctly formatted JSON data.
\item Importing from another database.
\item Using the \bulk \hdb scanner.
\item Using the \hdb library through the Python or C++ interface.
\end{itemize}

A database may contain blacklist hashes from multiple source domains,
where a domain is called a \textit{repository}.
The repository name indicates the provenance of the dataset.
It is its description information, such as ``Company X's intellectual property files''.\\

\subsection{Scanning}
There are multiple ways users can scan for matches in a block hash database:

\begin{itemize}
\item Using one of the \hdb tool scan commands to scan from a media image,
list, stream, or specific hash.
\item Using one of the \hdb library Python or C++ scan interfaces.
\item Using the \bulk \hdb scanner Scan function.
\end{itemize}

Additionally, there are several output modes for receiving scan matches. These modes provide varying levels of detail and speed.

\subsection{Contents of a Hash Database}
\label{ContentsOfDB}
Each \hdb database is contained in a directory called \textit{$<$databasename$>$.hdb} and contains a number of files. These files are:

\begingroup
\footnotesize
\begin{Verbatim}[fontfamily=courier]
lmdb_hash_data_store/data.mdb
lmdb_hash_data_store/lock.mdb
lmdb_hash_store/data.mdb
lmdb_hash_store/lock.mdb
lmdb_source_data_store/data.mdb
lmdb_source_data_store/lock.mdb
lmdb_source_id_store/data.mdb
lmdb_source_id_store/lock.mdb
lmdb_source_name_store/data.mdb
lmdb_source_name_store/lock.mdb
log.txt
settings.json
\end{Verbatim}
\endgroup

These files include several data store directories and files, a settings file, and a log file:

\begin{itemize}
\item \texttt{lmdb store} files \\
The \texttt{lmdb store} files encode all the block hashes, source files, and related information that are in the database. These filenames start with the prefix \verb+lmdb+.

\item \texttt{settings.json} \\
This file contains the settings requested by the user when the block hash database was created. Settings are described in \textbf{\autoref{Settings}}. This file also contains the internal \hdb settings version used to help \hdb identify whether a database is compatible with this version of \hdb. The \texttt{settings.json} file with the default settings looks like this:

\begingroup
\footnotesize
\begin{Verbatim}[fontfamily=courier]
{"settings_version":3, "byte_alignment":512, "block_size":512,
"max_id_offset_pairs":100000, "hash_prefix_bits":28,
"hash_suffix_bytes":3}
\end{Verbatim}
\endgroup

\item \texttt{log.txt} \\
Every time a command is run that changes the content of the database, information about the change is appended to this log.  Each entry includes the command name, information about \hdb including the command typed and how \hdb was compiled, information about the operating system \hdb was just run on, timestamps indicating how much time the command took, and the specific \hdb changes applied.\\

Listing \ref{logfile} shows an example log file containing two entries, one for when the hash database was created, and one for when data was imported into the database.

\lstset{style=customfile}
\begin{lstlisting}[float, caption=An example \texttt{log.xml} log file showing a database creation entry and a datase import entry, label=logfile]
# command: "hashdb create KittyMaterial.hdb"
# hashdb version: 3.0.0-alpha-1
# username: bdallen
# start time 2016-04-07T19:20:06Z
{"name":"begin","delta":"0.000386","total":"0.000386"}
{"name":"end","delta":"0.000013","total":"0.000403"}
# command: "hashdb import KittyMaterial.hdb KittyMaterial.json"
# hashdb version: 3.0.0-alpha-1
# username: bdallen
# start time 2016-04-07T19:20:23Z
{"name":"begin","delta":"0.000387","total":"0.000387"}
# hashdb changes:
#     hash_data_data_inserted: 401935
#     hash_data_data_same: 195
#     hash_data_source_inserted: 402130
#     hash_prefix_inserted: 401653
#     hash_suffix_inserted: 401935
#     hash_count_changed: 149
#     hash_not_changed: 46
#     source_data_inserted: 101
#     source_id_inserted: 101
#     source_id_already_present: 402231
#     source_name_inserted: 101
{"name":"end","delta":"2.608494","total":"2.608885"}
\end{lstlisting}


\item \texttt{timestamp.json}\\
\verb+timestamp.json+ is not formally part of the \hdb database.  It is created by the \hdb tool performance analysis commands described in \textbf{\autoref{PerformanceAnalysis}}. This file is replaced rather than appended to. Timestamp syntax is described in \textbf{\Autoref{InputOutputSyntax}}.
\end{itemize}

\subsection{Settings}
\label{Settings}
Function and performance of a \hdb database is set using configurable settings:

\begin{itemize}
\item \textbf{Block size}\\
The size of data blocks the database expects to store. Block hashes are calculated from data of this size. The default is 512.

\item \textbf{Byte alignment}\\
Byte alignment is an optimization parameter created to help reduce the size of the database. To be optimal, this value should be large, but it must be divisible by the step size used when importing hashes. If you only scan storage devices, use the sector size of the storage device since this is the smallest value that data in storage devices can align to, specifically, 512.

\item \textbf{Max source, offset pairs}\\
The maximum number of source, offset pairs to store for a given hash, default 100,000. Each pair identifies a location in a source where this hash is located. If source information is not interesting when there are many sources, offset pairs associated with a hash, use a low value to prevent storing extra entries.

\item \textbf{Hash prefix bits}\\
The number of bits of the hash prefix to use as the key in the store. The idea is to select a value given the size of the database so that the average number of hashes with this prefix is slightly greater than 0, for example 20. For example if you expect 5 billion hashes, you might select 28 because $5/2^{28}=18.6$, which is near 20.

\item \textbf{Hash suffix bytes}\\
The number of bytes of the hash suffix to store in the set of values of hash suffixes for this hash key. The idea is to store as few bytes as possible while minimizing false positives. For example if you expect 5 billion hashes, you might select 28 prefix bits and 3 suffix bytes because $5 / 2^{((28) + 8*(3))} = 5 / 2^{52} = 0.00011\%$ false positive rate, about 1 in 1 million.
\end{itemize}

\section {Running the \hdb Tool}
\label{Running}
The core capabilities provided by \hdb involve creating and maintaining a database of hash values and scanning media for those hash values. To perform those tasks, \hdb users need to start by building a database (if an existing database is not available for use).
Users then import hashes using \hdb tool commands, the \hdb \bulk scanner, or through the \hdb library API, and then possibly merge or subtract hashes to obtain the desired set of hashes to scan against.
Users then scan for hashes that match.
Additional commands are provided to support statistical analysis, performance tuning and performance analysis.\\

This section describes use of the \hdb tool commands, along with examples, for performing these tasks.
For more examples of command usage, please see \textbf{\autoref{UseCases}}.
For a \hdb quick reference summary, please see \textbf{\autoref{QuickReference}}, also available at \url{http://digitalcorpora.org/downloads/hashdb/hashdb_quick_reference.pdf}.

\subsection{Creating a New Hash Database}
\label{Creating}
A hash database must be created before hashes can be added to it.
Syntax for creating a hash database is shown in Table \ref{tab:createDatabase}.
Configurable settings associated with the database is shown in Table \ref{tab:hashDBSettings} and described in \textbf{\Autoref{Settings}}.\\
\begin{table}[!ht]
\centering
\caption{Command for Creating Hash Databases}
\label{tab:createDatabase}
\begin{tabular}{|p{2.5 cm}|p{7 cm}|p{4 cm}|}
\hline \hline
\textbf{Command} & \textbf{Usage} & \textbf{Description} \\
\hline
\textbf{create} & \verb+create [-b <block size>]+ \verb+[-a <byte alignment>]+ \verb+[-m <max source offset pairs>]+ \verb+[-t <hash prefix bits:hash+ \verb+suffix bytes>]+ \verb+<hashdb.hdb>+ & Creates a new hash database with the given configuration settings.\\
\hline
\end{tabular}
\end{table}

\begin{table}[!ht]
\centering
\caption{Settings for New Databases}
\label{tab:hashDBSettings}
\begin{tabular}{|p{1.5 cm}|p{8 cm}|p{4 cm}|}
\hline \hline
\textbf{Option} & \textbf{Verbose Option} & \textbf{Specification} \\
\hline
\textbf{\texttt{-b}} & \verb+--block_size=+\textit{block\_size} & Specifies the block size in bytes used to generate the hashes that will be stored and scanned against. Default is 512 bytes.  \\
\hline
\textbf{\texttt{-a}} & \verb+--byte_alignment=+\textit{byte\_alignment} & Specifies the byte alignment in bytes used to calculate hashes along. Default is 512 bytes.  \\
\hline
\textbf{\texttt{-m}} & \verb+--max_source_offset_pairs=+\textit{max source offset pairs} & Specifies the maximum number of source, source offset pairs allowed. 0 value indicates there is no limit. Default is 0.\\
\hline
\textbf{\texttt{-t}} & \verb+--tuning=+\textit{prefix bits:suffix bytes} & Specifies the number of prefix bits and suffix bytes to use for compacting the database.\\
\hline
\end{tabular}
\end{table}

\begin{itemize}
\item \textbf{create}\\
Create a new hash database configured with provided or default settings.\\

\textbf{Example}\\
To create an (empty) hash database named \textbf{\texttt{demo.hdb}}, type the following command:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb create demo.hdb}
\end{Verbatim}
The above command will create a database with all of the default hash database settings. Most users will not need to change these settings.
Users can specify either the option and value or the verbose option value for each parameter along with the create command, as in:\\
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb create --max\_source\_offset\_pairs=20 demo.hdb}
\verbbf{hashdb create -m 20 demo.hdb}
\end{Verbatim}
The above two commands produce identical results, creating the database \texttt{demo.hdb} that will accept a maximum of 20 source, offset pairs per hash.\\
\end{itemize}

\subsection{Importing and Exporting}
Hash databases may be imported to in several ways.  Syntax for commands that import and export hashes is shown in Table \ref{tab:importExport}. Import and export options are shown in Table \ref{tab:ImportExportOptions}.\\

Note that there are other ways to populate a database besides these listed here, including using other hash databases (discussed in \textbf{\autoref{updateSection}}),
by using the \bulk \hdb scanner (discussed in \textbf{\autoref{bulkextractorSection}}),
and through the use of the import capability provided by the \hdb library API (discussed in \textbf{\autoref{APISection}}).\\

\begin{table}[!ht]
\centering
\caption{Commands for Importing into and Exporting Hash Databases}
\label{tab:importExport}
\begin{tabular}{|p{2.5 cm}|p{7 cm}|p{4 cm}|}
\hline \hline
\textbf{Command} & \textbf{Usage} & \textbf{Description} \\
\hline
\textbf{ingest} & \verb+ingest [-r <repository name>]+ \verb+[-w <whitelist.hdb>]+ \verb+[-s <step size>] [-x <rel>]+ \verb+ <hashdb.hdb> <source directory>+& Computes and imports block hashes from files under the source directory into the hash database as directed by options.\\
\hline
\textbf{import\_tab} & \verb+import_tab [-r <repository name>]+ \verb+<hashdb.hdb>+ \verb+<tab.txt>+& Imports values from the tab-delimited file into the hash database. This command accepts a dash (\verb+-+) as a filename to allow terminal streaming from \verb+stdin+.\\
\hline
\textbf{import} & \verb+import <hashdb.hdb>+ \verb+<hashdb.json>+& Imports values from the JSON file into the hash database. This command accepts a dash (\verb+-+) as a filename to allow terminal streaming from \verb+stdin+.\\
\hline
\textbf{export} & \verb+export <hashdb.hdb>+ \verb+<hashdb.json>+& Exports the hash database to the JSON file. This command accepts a dash (\verb+-+) as a filename to allow terminal streaming to \verb+stdout+.\\
\hline
\end{tabular}
\end{table}

\begin{table}[!ht]
\centering
\caption{Options for Importing and Exporting Hash Databases}
\label{tab:ImportExportOptions}
\begin{tabular}{|p{1.5 cm}|p{8 cm}|p{4 cm}|}
\hline \hline
\textbf{Option} & \textbf{Verbose Option} & \textbf{Specification} \\
\hline
\textbf{\texttt{-r}} & \verb+--repository_name=+\textit{repository name} & Specifies the name to associate the imported hashes with. If not provided, the source filename entered is used as the repository name.\\
\hline
\textbf{\texttt{-w}} & \verb+--whitelist_dir=+\textit{whitelist directory} & If a whitelist database is provided, matching hashes are marked with \verb+w+ in their block label.\\
\hline
\textbf{\texttt{-s}} & \verb+--step_size=+\textit{step size} & The increment to step along for calculating block hashes. The step size must be compatible with the byte alignment defined in the database, specifically the byte alignment must be divisible by the byte alignment.\\
\hline
\textbf{\texttt{-x}} & \verb+--disable_processing=rel+ & Use this option to disable specific processing, specifically: \verb+r+ disables recursively processing embedded data, \verb+e+ disables calculating block entropy, and \verb+l+ disables calculating block labels.\\
\hline
\end{tabular}
\end{table}

\begin{itemize}
\item \textbf{ingest}\\
Computes and and imports hashes from files under the source directory, including files in subdirectories. Files with \verb+.E01+ extensions are treated as E01 files.\\

\textbf{Example}\\
To import block hashes from a directory of blacklist sources, type the following command:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb ingest -r demo_repository demo.hdb demo_blacklist_dir}
\end{Verbatim}
In the above command the option \textbf{-r} is used along with the repository name \texttt{demo\_repository} to indicate the repository source of the block hashes being imported into the database. The repository name is used to keep track of the sources of hashes. By default, the repository name used is the text \texttt{repository\_} with the filename of the file being imported from appended after it.\\

The \textbf{ingest} command in the above example imports block hashes from files in the \texttt{demo\_blacklist\_dir} directory into the database \texttt{demo.hdb}. When the Kitty Material demo dataset available at \url{http://digitalcorpora.org/corpora/scenarios/2009-m57-patents/KittyMaterial/import} is imported, \hdb prints output to the command line to indicate that hashes have been inserted into database \texttt{\textbf{demo.hdb}}. Listing \ref{DatabaseChanges} shows an example output of changes from running an ingest command.\\

Also, database log file \texttt{log.txt} is updated to show that a set of hash blocks have just been inserted. The log in Figure \ref{logfile} was generated from similar \textbf{create} and \textbf{import} actions.  The contents of log files is described in \textbf{\autoref{ContentsOfDB}}.\\

Users may prefer to run statistical commands such as this to get information about the contents of the database (and confirm that values were inserted):
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb size demo.hdb}
\end{Verbatim}

\item \textbf{import\_tab}\\
Imports values from the tab-delimited file into the hash database. Note that tab-delimited files are expected to contain block hashes calculated from 512-byte blocks along 512-byte boundaries. Tab-delimited files are described in \textbf{\autoref{TabFile}}.
\item \textbf{import}\\
Imports values from an exported database. Data is in JSON format as described in \textbf{\autoref{ImportExportSyntax}}.
\item \textbf{export}\\
Exports values from a \hdb block hash database. Data is in JSON format as described in \textbf{\autoref{ImportExportSyntax}}.
\end{itemize}

\subsection{Database Manipulation}
\label{DatabaseManipulation}
Databases may need to be merged together or common hash values may need to be subtracted out in order to produce a specific set of blacklist data to scan against.
Syntax for commands that manipulate hash databases is shown in Table \ref{tab:databaseManipulation}.
Destination databases are created if they do not exist yet.
\begin{table}[!ht]
\centering
\caption{Commands to Manipulate Hash Databases}
\label{tab:databaseManipulation}
\begin{tabular}{|p{3.5 cm}|p{6 cm}|p{4 cm}|}
\hline \hline
\textbf{Command} & \textbf{Usage} & \textbf{Description} \\
\hline
\textbf{add} & \verb+add <source db>+ \verb+<destination db>+ & Copies all of the hashes from \textit{source db} to \textit{destination db}\\
\hline
\textbf{add\_multiple} &  \verb+add_multiple <source db1>+ \verb+<source db2> ...+ \verb+<destination db>+ & Adds databases \textit{source db1}, \textit{source db2}, etc.\ to \textit{destination db}\\
\hline
\textbf{add\_repository} & \verb+add_repository <source db>+ \verb+<destination db>+ \verb+<repository name>+ & Adds \textit{source db} to \textit{destination db} but only when the repository name matches\\
\hline
\textbf{add\_range} & \verb+add_range<source db>+ \verb+<destination db>+ \verb+<m:n>+&   Copies hash values from \textit{source db} into \textit{destination db} that have source counts within range $m$ and $n$, inclusive\\
\hline
\textbf{intersect} & \verb+intersect <source db1>+ \verb+<source db2> <destination db>+ &   Copies hash values common to both \textit{source db1} and \textit{source db2} into \textit{destination db} where sources match\\
\hline
\textbf{intersect\_hash} & \verb+intersect_hash <source db1>+ \verb+<source db2> <destination db>+ &   Copies hash values common to both \textit{source db1} and \textit{source db2} into \textit{destination db} even if their sources are different.\\
\hline
\textbf{subtract} & \verb+subtract <source db1>+ \verb+<source db2> <destination db>+&   Copies hash values found in \textit{source db1} but not in \textit{source db2} into \textit{destination db} where sources match\\
\hline
\textbf{subtract\_hash} & \verb+subtract <source db1>+ \verb+<source db2> <destination db>+&   Copies hash values found in \textit{source db1} but not in \textit{source db2} into \textit{destination db} even if their sources are different.\\
\hline
\textbf{subtract \_repository} & \verb+subtract_repository+ \verb+<source db1>+ \verb+<destination db2>+ \verb+<repository namedb>+ & Adds \textit{source db1} to \textit{destination db2} unless the repository name matches\\
\hline
\end{tabular}
\end{table}

\begin{itemize}
\item \textbf{add}\\
Add a database to another database.
\item \textbf{add\_multiple}\\
Add multiple databases into a destination database.
\item \textbf{add\_repository}\\
Add a database to another database but only when the repository name matches.
\item \textbf{add\_range}\\
Add a database to another database but only when the hash source count falls within the given range.
\item \textbf{intersect}\\
Add hashes to a destination database when the hash and source are common.
\item \textbf{intersect\_hash}\\
Add hashes to a destination database when the hash is common, even if the referenced sources are different.
\item \textbf{subtract}\\
Add hashes to a destination database when the hash and source is in the first database but not in the second.
\item \textbf{subtract\_hash}\\
Add hashes to a destination database when the hash is in the first database but not in the second, even if the referenced sources are different.
\item \textbf{subtract\_repository}\\
Add a database to another database but only when the repository name does not match.
\end{itemize}

\subsection{Scan Services}
\label{ScanServices}
\hdb can be used to determine if a file, directory or disk image has content that matches previously identified content. This capability can be used, for example, to determine if a set of files contains a specific file excerpt or if a media image contains a video fragment. Forensic investigators can use this feature to search for blacklisted content.
Syntax for scan service commands is shown in Table \ref{tab:scanServices}. Scan service options are shown in Table \ref{tab:ScanOptions}.\\

\begin{table}[!ht]
\centering
\caption{Commands that Provide Scan Services}
\label{tab:scanServices}
\begin{tabular}{|p{3.5 cm}|p{6 cm}|p{4 cm}|}
\hline \hline
\textbf{Command} & \textbf{Usage} & \textbf{Description} \\
\hline
\textbf{scan\_list} & \verb+scan_list [-j e|o|c|a] <hashdb>+ \verb+<hash list file>+ & Scans the hashdb for hashes that match hashes in the hash list file and prints out matches\\
\hline
\textbf{scan\_hash} & \verb+scan_hash [-j e|o|c|a] <hashdb>+ \verb+<hash value>+ & Scans the hashdb for the specified hash value and prints out whether it matches\\
\hline
\textbf{scan\_image} & \verb+scan_image+ \verb+[-s <step size>] [-j e|o|c|a]+ \verb+[-x <r>] <hashdb> <media image>+ & Scans the hashdb for hashes that match hashes in the media image and prints out matches.\\
\hline
\end{tabular}
\end{table}

\begin{table}[!ht]
\centering
\caption{Options for Scanning from a Media Image}
\label{tab:ScanOptions}
\begin{tabular}{|p{1.5 cm}|p{8 cm}|p{4 cm}|}
\hline \hline
\textbf{Option} & \textbf{Verbose Option} & \textbf{Specification} \\
\hline
\textbf{\texttt{-s}} & \verb+--step_size=+\textit{step size} & The increment to step along for calculating block hashes. The step size must be compatible with the byte alignment defined in the database, specifically the byte alignment must be divisible by the byte alignment.\\
\hline
\textbf{\texttt{-j}} & \verb+--json_scan_mode=e|o|c|a+ & Select a mode, one of \textbf{e}xpanded, expanded \textbf{o}ptimized, \textbf{c}ount only, \textbf{a}pproximate count. Default is \textbf{o}.\\
\hline
\textbf{\texttt{-x}} & \verb+--disable_processing=r+ & Use this option to disable specific processing, specifically: \verb+r+ disables recursively processing embedded data.\\
\hline
\end{tabular}
\end{table}

\begin{itemize}
\item \textbf{scan\_list}\\
Scan for hashes in the list. List input syntax is described in \textbf{\autoref{ScanListInputFile}}. Scan output is described in \textbf{\autoref{ScanData}}. This command accepts a dash (\verb+-+) as a filename to allow terminal streaming from \verb+stdin+.\\
\item \textbf{scan\_hash}\\
Scan for the specified hash. The hash to scan for must be provided in hexadecimal format.
\item \textbf{scan\_image}\\
Scan the specified image for matching hashes.

\textbf{Example}\\
To scan, first identify the media that you would like to scan. For this example, we download and use the demo media image available at \url{ http://digitalcorpora.org/corpora/scenarios/2009-m57-patents/drives-redacted/jo-favorites-usb-2009-12-11.E01} which contains matching Kitty material.\\

Then identify the existing hash database that will be used to search for hash value matches. We'll use the database \texttt{\textbf{demo.hdb}} that we created from Kitty material in the previous section, containing block hash values calculated from pictures and videos of cats.\\

Finally, run the \hdb scan command to scan for blocks in the media that match block hashes in the database:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb scan_image demo.hdb jo-favorites-usb-2009-12-11.E01 > matches.json}
\end{Verbatim}
This command tells \hdb to scan media image \verb+jo-favorites-usb-2009-12-11.E01+ and try to match the values found in the local database \texttt{\textbf{demo.hdb}}, putting match data in file \verb+matches.json+.  An example match might look like this:

\begingroup
\footnotesize
\begin{Verbatim}[fontfamily=courier]
2543104	1d7379fd4d5cf676a9d4de1e48337e71	{"entropy":0.5,
"block_label":"","source_list_id":1193146442,"sources":
[{"file_hash":"1dd00f2e51aeebe7541cea4ade2e20b5","filesize":1549288,
"file_type":"","zero_count":1,"nonprobative_count":10,"name_pairs":
["default_repository","KittyMaterial/HighQuality/DSC00003.JPG"]}],
"source_offset_pairs":["1dd00f2e51aeebe7541cea4ade2e20b5",0]}
\end{Verbatim}
\endgroup

Users may be put off by the quantity of matches incurred by low-entropy data in their databases such as blocks of zeros or metadata header blocks from files that are otherwise unique. Database manipulation commands,
\textbf{\autoref{DatabaseManipulation}}, can mitigate this, for example:
\begin{itemize}
\item Use the ``subtract'' command to remove known whitelist data created from sources such as ``brand new'' operating system images and the NSRL.
\item Alternatively, use the ``add\_range'' command to copy all hash values that have been imported some number of times, for example, exactly once.
\end{itemize}

\end{itemize}

\subsection{Statistics}
Various statistics are available about a given hash database including the size of a database, where its hashes were sourced from, a histogram of its hashes, and more.
Table \ref{tab:statistics} shows syntax for the statistics commands.
Statistics options are shown in Table \ref{tab:StatisticsOptions}.
\begin{table}[!ht]
\centering
\caption{Commands that provide Statistics about Hash Databases}
\label{tab:statistics}
\begin{tabular}{|p{3.5 cm}|p{6 cm}|p{4 cm}|}
\hline \hline
\textbf{Command} & \textbf{Usage} & \textbf{Description} \\
\hline
\textbf{size} & \verb+size <hashdb>+ & Prints out size information relating to the database.\\
\hline
\textbf{sources} & \verb+sources <hashdb>+ & Prints source information for all sources in the database.\\
\hline
\textbf{histogram} & \verb+histogram <hashdb>+ & Prints a hash distribution for the hashes in the \textit{hashdb}.\\
\hline
\textbf{duplicates} & \verb+duplicates <hashdb> <number>+ &  Prints out hashes in the database that are sourced the given number of times.\\
\hline
\textbf{hash\_table} & \verb+hash_table <hashdb>+ \verb+<hex file hash>+ &  Prints hashes associated with the specified source.\\
\hline
\textbf{read\_bytes} & \verb+read_bytes <image file>+ \verb+<offset> <count>+ &  Prints count raw bytes from a media image file starting at the given forensic path.\\
\hline
\end{tabular}
\end{table}

\begin{table}[!ht]
\centering
\caption{Options for Commands that Provide Statistics}
\label{tab:StatisticsOptions}
\begin{tabular}{|p{1.5 cm}|p{8 cm}|p{4 cm}|}
\hline \hline
\textbf{Option} & \textbf{Verbose Option} & \textbf{Specification} \\
\hline
\textbf{\texttt{-j}} & \verb+--json_scan_mode=e|o|c|a+ & Select a mode, one of \textbf{e}xpanded, expanded \textbf{o}ptimized, \textbf{c}ount only, \textbf{a}pproximate count. Default is \textbf{o}.\\
\hline
\end{tabular}
\end{table}

\begin{itemize}
\item \textbf{size}\\
Prints size information about the given database.

To find the size of various data stores in hash database \texttt{example.hdb},
type the following:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb size examle.hdb}
\end{Verbatim}
The above command prints the size of various data stores within the database in JSON format.\\

\item \textbf{sources}\\
Prints out all source file references that have contributed to this database including repository names and filenames.
\item \textbf{histogram}\\
Prints a hash distribution of the hashes in the given database, see \textbf{\autoref{Histogram}} for syntax.
\item \textbf{duplicates}\\
Prints out hashes in the database that are sourced the given number of times.
\item \textbf{hash\_table}\\
Prints out hashes associated with the specified source identified by the source file hexdigest.

To obtain a list of hashes in \texttt{example.hdb} associated with the source file identified by hexcode \texttt{16d75027533b0a5ab900089a244384a0}, type the following:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb hash_table example.hdb 16d75027533b0a5ab900089a244384a0}
\end{Verbatim}

\item \textbf{read\_bytes}\\
Prints raw bytes from the given media image. Note that these bytes are often not printable.
\end{itemize}

\subsection{Performance Analysis}
\label{PerformanceAnalysis}
Performance analysis commands for analyzing \hdb performance are shown in Table \ref{tab:analysis}. Performance analysis options are shown in Table \ref{tab:AnalysisOptions}. Timing data is placed in file \verb+timestamp.json+, replacing any previous content.

\begin{table}[!ht]
\centering
\caption{Commands that Support \hdb Performance Analysis}
\label{tab:analysis}
\begin{tabular}{|p{3.5 cm}|p{6 cm}|p{4 cm}|}
\hline \hline
\textbf{Command} & \textbf{Usage} & \textbf{Description} \\
\hline
\textbf{add\_random} & \verb+add_random+ \verb+-r [<repository name>]+ \verb+<hashdb.hdb> <count>+ & Adds count random hashes to the given database, creating timing data in the \texttt{log.xml} file.\\
\hline
\textbf{scan\_random} & \verb+scan_random [-j e|o|c|a]+ \verb+<hashdb.hdb>+ & Scans random hashes in the given database, creating timing data in the \texttt{log.xml} file.\\
\hline
\textbf{add\_same} & \verb+add_same+ \verb+-r [<repository name>]+ \verb+<hashdb.hdb> <count>+ & Adds count same hashes to the given database, creating timing data in the \texttt{log.xml} file.\\
\hline
\textbf{scan\_same} & \verb+scan_same [-j e|o|c|a]+ \verb+<hashdb.hdb>+ & Scans count same hashes in the given database, creating timing data in the \texttt{log.xml} file.\\
\hline
\end{tabular}
\end{table}

\begin{table}[!ht]
\centering
\caption{Options for Commands that Support Performance Analysis}
\label{tab:AnalysisOptions}
\begin{tabular}{|p{1.5 cm}|p{8 cm}|p{4 cm}|}
\hline \hline
\textbf{Option} & \textbf{Verbose Option} & \textbf{Specification} \\
\hline
\textbf{\texttt{-j}} & \verb+--json_scan_mode=e|o|c|a+ & Select a mode, one of \textbf{e}xpanded, expanded \textbf{o}ptimized, \textbf{c}ount only, \textbf{a}pproximate count. Default is \textbf{o}.\\
\hline
\end{tabular}
\end{table}

\begin{itemize}
\item \textbf{add\_random}\\
Add random hashes, leaving timing data in \verb+log.xml+.
\item \textbf{scan\_random}\\
Scan random hashes, leaving timing data in \verb+log.xml+. Although this command does not produce output, the scan mode impacts timing.
\item \textbf{add\_same}\\
Add the same hash, leaving timing data in \verb+log.xml+.
\item \textbf{scan\_same}\\
Scan the same hash, leaving timing data in \verb+log.xml+. Although this command does not produce output, the scan mode impacts timing.
\end{itemize}

\section{Tools that use \hdb}
\label{OtherTools}
\sscope, the \sscope Autopsy Plug-in, and the \bulk \hdb scanner use \hdb.

\subsection{\sscope}
The \sscope tool provides a GUI for analyzing data associated with block hash matches found on a media image. An example screenshot of the main window of \sscope showing a histogram of matches on a media image is shown in Figure \ref{fig:SectorScope_main}. \sscope also provides interfaces for building and scanning against \hdb databases. Please see \url{https://github.com/NPS-DEEP/NPS-SectorScope/wiki} for more information on \sscope.

\begin{figure}
	\center
	\includegraphics[scale=.45]{drawings/SectorScope_main}
	\caption{Example screenshot of the \sscope tool}
	\label{fig:SectorScope_main}
\end{figure}

\subsection{The \sscope \aut Plug-in}
\sscope provides an \aut plug-in for scanning for fragments of previously identified files. \aut is currently only available on Windows systems. This section describes how to set up the \sscope \aut plug-in.

\subsubsection{Installing the \sscope Plug-in}
The \sscope Windows installer installs the requisite \verb+.nbm+ Autopsy plug-in module onto the desktop. Please follow these steps to install this module:

\begin{enumerate}
\item Open \aut. From the Autopsy menu, select \verb+Tools | Plugins+.
\item Open the \verb+Downloaded+ tab and click the \verb+Add Plugins...+ button.
\item From the \verb+Add Plugins+ window, navigate to the \verb+.nbm+ module file that was installed onto the desktop, and open it.
\item Click \verb+Install+ and follow the wizard. Please note that it may be difficult to replace an old module of NPS-Autopsy-hashdb already installed in Autopsy. In the unlikely case that error \verb+Some plugins require plugin org.jdesktop.beansbinding+ \verb+to be installed+ appears, it may be necessary to uninstall and reinstall Autopsy.
\end{enumerate}

\subsubsection{Configuring the \sscope Plug-in}
The path to the \hdb database to scan against must be configured:

\begin{enumerate}
\item Start a new case, \verb+File | New Case...+, fill in the Case Information fields, and click \verb+Next+.
\item Fill in Case Information and click \verb+Finish+.
\item For Add Data Source (1 of 3), put in a media image for Autopsy to process and click \verb+Next+.
\item For Add Data Source (2 of 3), select checkboxes as desired, then click on \verb+NPS-SectorScope+ text to configure the path to your \hdb database to scan against. Currently a file chooser is not available, so please type in the full path, for example: \verb+C:\Users\me\my_hashdb.hdb+. Click Next.
\item For Add Data Source (3 of 3) click \verb+Finish+. When the NPS-SectorScope module begins processing, Autopsy will display "NPS-SectorScope ..." as \bulk runs, which may take up to several hours. Unfortunately, \bulk progress is not currently indicated. For diagnostics: please see if progress text is appearing in the generated \verb+bulk_extractor\report.xml+ file and in the generated log file or try running the scan manually.
\end{enumerate}

\subsection{\bulk}
\bulk is an open source digital forensics tool that extracts features such as email addresses, credit card numbers, URLs and other types of information from digital evidence files. It operates on disk images, files or a directory of files and extracts useful information without parsing the file system or file system structures.  For more information on how to use \bulk for a wide variety of applications, refer to the separate publication \textit{The \bulk Users Manual} available at \url{http://digitalcorpora.org/downloads/bulk_extractor/BEUsersManual.pdf} \cite{beusersguide}.\\

In particular, a \hdb \bulk scanner is available which may be used to import block hashes into a new hash database and to scan for hashes against an existing hash database.
Currently, \hdb requires a newer build of \bulk than is available on the \bulk site.
Please see the \hdb Wiki page at \url{https://github.com/NPS-DEEP/hashdb/wiki}
for information on obtaining a version of \bulk that is compatible with the current version of \hdb.\\

Options that control the hashdb scanner are provided to \bulk using "\verb+-S name=value+" parameters.  Example syntax for the \bulk \hdb scanner is shown in Table \ref{tab:hashdbScanner}.  Scanner options are described in Table \ref{tab:hashdb_be_usage}.\\

When importing, the new database of imported hashes is created in the output directory at \verb+hashdb.hdb+.  When scanning, matches are written in the output directory at file \verb+identified_blocks.txt+ with one match per line, as described in \textbf{\autoref{InputOutputSyntax}}) Listing \ref{ScanDataWithPath}.\\

\begin{table}[!ht]
\centering
\caption{\bulk \hdb Scanner Commands}
\label{tab:hashdbScanner}
\begin{tabular}{|p{3.5 cm}|p{6 cm}|p{4 cm}|}
\hline \hline
\textbf{Goal} & \textbf{Example} & \textbf{Description} \\
\hline
\textbf{import files} & \verb+bulk_extractor -E hashdb+ \verb+-S hashdb_mode=import+ \verb+-o outdir1 -R my_directory+ & Import hashes from directory into outdir1/hashdb.hdb\\
\hline
\textbf{import image} & \verb+bulk_extractor -E hashdb+ \verb+-S hashdb_mode=import+ \verb+-o outdir1 my_image1+ & Import hashes from image into outdir1/hashdb.hdb\\
\hline
\textbf{scan image} & \verb+bulk_extractor -E hashdb+ \verb+-S hashdb_mode=scan+ \verb+-S hashdb_scan_path+ \verb+=outdir1/hashdb.hdb -o outdir2+ \verb+my_image+ & Scan image for hashes matching hashes in outdir1/hashdb.hdb\\
\hline
\end{tabular}
\end{table}

\begin{table}[!ht]

\centering
\caption{\bulk \hdb Scanner Options}
\label{tab:hashdb_be_usage}
%\begin{tabular}{|p{3.5 cm}|p{6 cm}|p{4 cm}|}
\begin{tabular}{|p{5 cm}|p{2.0 cm}|p{6.5 cm}|}
\hline \hline
\textbf{Option} & \textbf{Default} & \textbf{Specification} \\
\hline
\verb+hashdb_mode+ & \verb+none+ & The mode for the scanner, one of \verb+[none|import|scan]+. For ``none'', the scanner is active but performs no action. For ``import'', the scanner imports block hashes. For ``scan'', the scanner scans for matching block hashes.\\
\hline
\verb+hashdb_block_size+ &512 & Block size, in bytes, used to generate hashes.\\
\hline
\verb+hashdb_byte_alignment+ &512 & byte alignment, in bytes.  Scans and imports along sector boundaries.\\
\hline
\verb+hashdb_scan_path+ & & The file path to a hash database to scan against.  Valid only in scan mode. No default provided. Value must be specified if in scan mode.\\
\hline
\verb+hashdb_import_repository_+ \verb+name+ & \verb+default_+ \verb+repository+ &Selects the repository name to attribute the import to.  Valid only in import mode.\\
\hline
\verb+hashdb_max_feature_file_+ \verb+lines+ & 0 &The maximum number of feature lines to record or 0 for no limit.  Valid only in scan mode.\\
\hline
\end{tabular}
\end{table}

\section{Use Cases for \hdb}
\label{UseCases}
There are many different ways to utilize the functionality provided by the \hdb tool. In this section, we highlight some of the most common uses of the system.

\subsection{Querying for Source or Database Information}
 Users can scan a hash database directly using various querying commands. Those commands are outlined in Table \ref{tab:scanServices}.  The ``scan'' command allows users to search for hash blocks.\\

\subsection{Writing Software that works with \hdb}
\label{APISection}
\hdb provides Python and C++ APIs that can manage all aspects of a hash database
including importing and scanning [see \textbf{\Autoref{APIs}} for information on using these APIs].
Other software programs can use these APIs to access database capabilities. The file \texttt{hashdb.hpp} found in the \textit{src} directory contains the complete specification of the API. That complete file is also contained in Appendix \ref{hashdbapi} of this document.  The two key features provided by the API include the ability to import values into a hash database and the ability to scan media for any values matching those in a given hash database.  The \bulk program uses the \hdb API to implement both of these capabilities.\\

\subsection{Scanning or Importing to a Database Using \bulk}
\label{bulkextractorSection}
The \bulk \textit{hashdb} scanner allows users to query for fragments of previously encountered hash values and populate a hash database with hash values. Options that control the \textit{hashdb} scanner are provided to \bulk using the ``-S name=value'' command line parameters. When \bulk executes, the parameters are sent directly to the scanner.\\

For example, the following command runs the \bulk \textit{hashdb} scanner in import mode and adds hash values calculated from the disk image \texttt{my\_image} to a hash database:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{bulk_extractor -e hashdb -o outputDir -S hashdb_mode=import my_image}
\end{Verbatim}
Note, \bulk will place feature file and other output not relevant to the \hdb application in the ``outputDir'' directory. When using the import command, the output directory will contain a newly created hash database called \texttt{hashdb.hdb}. That database can then be copied or added to a hash database in another location.


\subsection{Updating Hash Databases}
\label{updateSection}
\hdb provides users with the ability to manipulate the contents of hash databases. The specific command line options for performing these functions are described in Table \ref{tab:databaseManipulation}. \hdb databases are treated as sets with the add, subtract and intersect commands basically using add, subtract and intersect set operations. For example, the following command will copy all non-duplicate values from \texttt{demo.hdb} into \texttt{demo\_dedup.hdb} by copying all values with a count less than or equal to one:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb add_range demo.hdb demo_dedup.hdb :1}
\end{Verbatim}
Whenever a database is created or updated, \hdb updates the file \texttt{log.xml}, found in the database's directory with information about the actions performed.\\

After each command to change a database, statistics about the changes are writen in the \texttt{log.xml} file and to \texttt{stdout}. Table \ref{tab:changeStatistics} shows all of the statistics tracked in the log file along with their meaning. The value of each statistic is the number of times the event happened during the command.\\


\subsection{Exporting Hash Databases}
Users can export hashes from a hash database to a JSON export file using the ``export'' command [see \textbf{\Autoref{InputOutputSyntax}} for information on JSON syntax].  For example, the following command will export the \texttt{demo.hdb} database to the file \texttt{demo.json}:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb export demo.hdb demo.json}
\end{Verbatim}

\section{\hdb Input/Output Syntax}
\label{InputOutputSyntax}
Many of the \hdb commands and API interfaces require or emit data.
This section describes the syntax used and required by \hdb
commands and API interfaces.

\subsection{Tab-delimited Import File}
\label{TabFile}
The \verb+import_tab+ command imports hashes from tab delimited files.
The tab-delimited import file consists of hash lines separated by carriage returns, where each line consists of a filename followed by a tab followed by the file hash followed by a 512-byte sector index that starts at 1.  Comment lines are allowed by starting them with the \texttt{\#} character.
An example tab-delimited file is shown in Listing \ref{importTabFile}.\\

\lstset{style=customfile}
\begin{lstlisting}[float, caption={Example tab-delimited import file}, label=importTabFile]
# tab-delimited import file
# <file hexdigest> <tab> <block hash> <tab> <index>
fac7051447c781b69125994c5d125637    3b6b477d391f73f67c1c01e2141dbb17    1
fac7051447c781b69125994c5d125637    89a170b6b9a948d21d1d6ee1e7cdc467    2
fac7051447c781b69125994c5d125637    f58a09656658c6b41e244b4a6091592c    3
\end{lstlisting}

\subsection{Import/Export Syntax}
\label{ImportExportSyntax}
The import and export commands and API interfaces communicate source data and block hash data using JSON syntax.\\

\subsubsection{Source Data}
Source data defines information about a source. Source data is identified by the file hash of the source.  An example source data line is shown in Listing \ref{JSONSourceData}. Fields are described in Table \ref{tab:JSONSourceData}.\\

\lstset{style=customfile}
\begin{lstlisting}[float, caption={Example JSON source data used during import/export}, label=JSONSourceData]
{
  "file_hash":"fac7051447c781b69125994c5d125637",
  "filesize":8000,
  "file_type":"exe",
  "zero_count":1,
  "nonprobative_count":4,
  "name_pairs":["repository1","filename1","repo2","f2"]
}
\end{lstlisting}

\begin{table}[!ht]

\centering
\caption{Fields used in JSON source data}
\label{tab:JSONSourceData}
\begin{tabular}{|p{5 cm}|p{8.8 cm}|}
\hline \hline
\textbf{Field} & \textbf{Meaning} \\
\hline
\verb+file_hash+ & The hexdigest of the source file containing the block hash\\
\hline
\verb+filesize+ & The size, in bytes, of the source file\\
\hline
\verb+file_type+ & A classification of what type of file the source file is\\
\hline
\verb+zero_count+ & The number of blocks in the source that have all bytes in the block equal to zero\\
\hline
\verb+nonprobative_count+ & The number of blocks in the source that are considered to be nonprobative\\
\hline
\verb+name_pairs+ & An array of source name, filename pairs associated with this source\\
\hline
\end{tabular}
\end{table}

\subsubsection{Block Hash Data}
Block hash data is identified by the file block hash.  An example block hash line is shown in Listing \ref{JSONBlockHashData}. Fields are described in Table \ref{tab:JSONBlockHashData}.\\

\lstset{style=customfile}
\begin{lstlisting}[float, caption={Example JSON block hash data used during import/export}, label=JSONBlockHashData]
{
  "block_hash":"3b6b477d391f73f67c1c01e2141dbb17",
  "entropy":2.0,
  "block_label":"W",
  "source_offset_pairs":["fac7051447c781b69125994c5d125637", 4096]
}
\end{lstlisting}

\begin{table}[!ht]

\centering
\caption{Fields used in JSON block hash data}
\label{tab:JSONBlockHashData}
\begin{tabular}{|p{5 cm}|p{8.8 cm}|}
\hline \hline
\textbf{Field} & \textbf{Meaning} \\
\hline
\verb+block_hash+ & A block hash hexdigest\\
\hline
\verb+entropy+ & The entropy value calculated for the block\\
\hline
\verb+block_label+ & A label describing the type of data within the block. The block label may include information that it matched a whitelist database during import. The entropy and block label fields may be used together to estimate that a block might be nonprobative\\
\hline
\verb+source_offset_pairs+ & An array of source file hash, source file offset pairs associated with this block hash\\
\hline
\end{tabular}
\end{table}

\subsection{Scan Data}
\label{ScanData}
When hash matches are found, \hdb returns data in JSON format. Due to varying requirements for speed and completeness, several options are available. This section describes the JSON output options available for hash matches.\\

\subsubsection{Expanded Hash}
The returned JSON data contains all the information about a matched hash and the sources containing the hash that matched, even if it has already been returned in a previous scan. An example of expanded JSON output formatted with line breaks added for readability is shown in Listing \ref{JSONScanDataExpanded}. Fields are described in Table \ref{tab:JSONScanDataExpanded}.\\

\lstset{style=customfile}
\begin{lstlisting}[float, caption={Example JSON block hash expanded data output from a scan match, with line breaks added for readability}, label=JSONScanDataExpanded]
{
  "block_hash": "3b6b477d391f73f67c101e2141dbb17",
  "entropy": 2.0,
  "block_label": "W",
  "source_list_id": 57,
  "sources": [{
    "file_hash": "f7035a...",
    "filesize": 800,
    "file_type": "exe",
    "zero_count":1,
    "nonprobative_count": 2,
    "name_pairs": ["repository1", "filename1", "repo2", "f2"]
  }],
  "source_offset_pairs": ["f7035a...", 0, "f7035a...", 512]
}
\end{lstlisting}

\begin{table}[!ht]

\centering
\caption{Fields used in JSON scan data}
\label{tab:JSONScanDataExpanded}
\begin{tabular}{|p{5 cm}|p{8.8 cm}|}
\hline \hline
\textbf{Field} & \textbf{Meaning} \\
\hline
\verb+entropy+ & The entropy value calculated for the block\\
\hline
\verb+block_label+ & A label describing the type of data within the block\\
\hline
\verb+source_list_id+ & A source list ID which is a CRC of the associated source file hashes associated with the block hash\\
\hline
\verb+sources+ & An array of source data for each source matching the hash\\
\hline
\verb+file_hash+ & The hexdigest of the source file containing the block hash\\
\hline
\verb+filesize+ & The size, in bytes, of the source file\\
\hline
\verb+file_type+ & A classification of what type of file the source file is\\
\hline
\verb+zero_count+ & The number of blocks in the source that have all bytes in the block equal to zero\\
\hline
\verb+nonprobative_count+ & The number of blocks in the source that are considered to be nonprobative\\
\hline
\verb+name_pairs+ & An array of repository name, filename pairs associated with this source\\
\hline
\verb+source_offset_pairs+ & An array of the source file hash, source file offset pairs associated with this block hash\\
\hline
\end{tabular}
\end{table}

\subsubsection{Expanded Hash, Optimized}
The returned JSON data contains all the information about a matched hash and the sources containing the hash that matched in the first match, but hash and source metadata is not returned more than once. This optimization reduces the amount of data returned during the scan, but the user must remember associated hash and source metadata as it is returned because it is not returned in subsequent matches. An example output of a subsequent match of the same hash might be: \verb+{"block_hash":+ \verb+"3b6b477d391f73f67c1c01e2141dbb17}+.\\

\subsubsection{Hash Count}
Only the count field is returned, indicating the number of sources cited in each match. JSON output contains the hash and the count as shown in example Listing \ref{JSONScanDataCount}. This capability is an optimization provided for users who do not need other hash information.

\lstset{style=customfile}
\begin{lstlisting}[float, caption={Example JSON block hash data count output from a scan match, with line breaks added for readability}, label=JSONScanDataCount]
{
  "block_hash": "3b6b477d391f73f67c101e2141dbb17",
  "count": 501
}
\end{lstlisting}

\subsubsection{Approximate Hash Count}
Only an approximate count field is returned, and it is possible for the count to be wrong. This capability is an optimization provided for users who do not need other hash information and can accept count values that are not exact. This is the fastest scan option since it only reads the hash store. For information on the hash store, see \textbf{\Autoref{LMDBDataStores}}. JSON output contains the hash and the approximate count as shown in example Listing \ref{JSONScanDataApproximateCount}. 

\lstset{style=customfile}
\begin{lstlisting}[float, caption={Example JSON block hash data approximate count output from a scan match, with line breaks added for readability}, label=JSONScanDataApproximateCount]
{
  "block_hash": "3b6b477d391f73f67c101e2141dbb17",
  "approximate_count": 500
}
\end{lstlisting}

\subsection{Scan Data Output from Tools}
The scan commands provided by the \hdb tool and the \bulk \hdb scanner print one line of output per match. This output consists of the Forensic Path (offset), a tab, the hash hexcode, a tab, the expanded hash JSON data, and a carriage return. An example of a scan match is shown in Listing \ref{ScanDataWithPath}.\\

\lstset{style=customfile}
\begin{lstlisting}[float, caption={Example output from a scan match}, label=ScanDataWithPath]
8000	3b6b477d391f73f67c1c01e2141dbb17    {"hash": "3b6b477d391f73f67c1
c01e2141dbb17", "entropy": 2.0, "block_label": "W", "source_list_id": 57, "
sources": [{ "file_hash": "f7035a...", "filesize": 800, "file_type": "exe
", "zero_count": 1, "nonprobative_count": 2, "name_pairs": ["repository1"
, "filename1", "repo2", "f2"] }], "source_offset_pairs": ["f7035a...", 0,
 "f7035a...", 512] }
\end{lstlisting}

\subsection{Scan Stream Interface Data}
Scan stream interface data consists of packed binary strings of unscanned input data and packed binary strings of scanned output data. Each contains an array of data as follows:\\
\begin{itemize}
\item \textbf{unscanned data}
  \begin{itemize}
  \item A binary hash to scan for, of length \verb+hash_size+ bytes.
  \item A 2-byte unsigned integer in native-Endian format indicating the length, in bytes, of the upcoming binary label associated with the scan record.
  \item A binary label associated with the scan record, of the length just indicated.
  \end{itemize}
\item \textbf{scanned data}
  \begin{itemize}
  \item A binary hash that matched, of length \verb+hash_size+ in bytes.
  \item A 2-byte unsigned integer in native-Endian format indicating the length, in bytes, of the upcoming binary label associated with the hash that matched.
  \item A binary label associated with the scan record, of the length just indicated.
  \item A 4-byte unsigned integer in native-Endian format indicating the length, in bytes, of the upcoming JSON text associated with the hash that matched.
  \item JSON text formatted based on the scan mode selected, of the
length just indicated.
  \end{itemize}
\end{itemize}

\subsection{Scan List Input File}
\label{ScanListInputFile}
The \verb+scan_list+ command scans a list of hashes for matches.  Valid lines of input may be:
\begin{itemize}
\item Comment lines starting with \verb+#+.  Comment lines are forwarded to output.
\item Hash lines to scan against, where each line consists of a Forensic Path followed by a tab followed by the hash hexcode.
\end{itemize}
An example scan list input file is shown in Listing \ref{ScanListFile}.\\

\lstset{style=customfile}
\begin{lstlisting}[float, caption={Example scan list input file}, label=ScanListFile]
# Scan list input file
# <Forensic Path> <tab> <block hash hexdigest>
0	3b6b477d391f73f67c1c01e2141dbb17
512	89a170b6b9a948d21d1d6ee1e7cdc467
1024	f58a09656658c6b41e244b4a6091592c
\end{lstlisting}

\subsection{Size}
The \hdb \verb+size+ command and \verb+size+ API interface returns size information about internal data structures in JSON format. The size of the \verb+source_id_store+ indicates the number of sources. The size of the \verb+hash_store+ is greater than or equal to the number of hashes stored, and is not exact because of how data is stored. Although for internal use, these fields can give some sense of the size of a \hdb database. An example output is shown in Listing \ref{JSONSize}.\\

\lstset{style=customfile}
\begin{lstlisting}[float, caption={Example JSON output of database size values}, label=JSONSize]
{
  "hash_data_store":402221,
  "hash_store":401653,
  "source_data_store":101,
  "source_id_store":101,
  "source_name_store":101
}
\end{lstlisting}

\subsection{Sources}
The \verb+sources+ command prints JSON data as shown in Listing \ref{JSONSourceData} and described in Table \ref{tab:JSONSourceData}.\\

\subsection{Histogram}
\label{Histogram}
The \verb+histogram+ command shows the density of hash duplicates across a hash database. Fields are described in Table \ref{tab:JSONHistogram}. An example histogram output line is shown in Listing \ref{JSONHistogram}.

\lstset{style=customfile}
\begin{lstlisting}[float, caption={Example JSON histogram format}, label=JSONHistogram]
{
  "duplicates":2,
  "distinct_hashes":3,
  "total":6
}
\end{lstlisting}

\begin{table}[!ht]

\centering
\caption{Fields used in JSON histogram output}
\label{tab:JSONHistogram}
\begin{tabular}{|p{5 cm}|p{8.8 cm}|}
\hline \hline
\textbf{Field} & \textbf{Meaning} \\
\hline
\verb+duplicates+ & The count of source, file offset pairs stored\\
\hline
\verb+distinct_hashes+ & The number of distinct hashes in the database with this duplicates count\\
\hline
\verb+total+ & The total number of hashes represented by this entry, specifically, \verb+duplicates+$*$\verb+distinct_hashes+\\
\hline
\end{tabular}
\end{table}

\subsection{Duplicates}
The \verb+duplicates+ command prints JSON data associated with hashes with a specified duplicates count as shown in Listing \ref{JSONScanDataExpanded} and described in Table \ref{tab:JSONScanDataExpanded}.\\

\subsection{Hash Table}
The \verb+hash_table+ command prints JSON data associated with a file hash as shown in Listing \ref{JSONScanDataExpanded} and described in Table \ref{tab:JSONScanDataExpanded}.\\

\subsection{Read Bytes}
The \verb+read_bytes+ command prints raw binary bytes from an image file. It is intended that this output be consumed by other tools since raw binary data is typically unreadable.\\

\subsection{Timing}
\hdb provides timing data in JSON format for use with timing analysis. Python scripts may use this output to produce performance plots. An example timestamp entry is shown in Listing \ref{JSONTimingData}. Fields are described in Table \ref{tab:JSONTimingData}.\\

\lstset{style=customfile}
\begin{lstlisting}[float, caption={Example JSON timestamp format}, label=JSONTimingData]
{
  "name":"begin",
  "delta":"0.000396",
  "total":"0.000396"
}
\end{lstlisting}

\begin{table}[!ht]

\centering
\caption{Fields used in JSON timing data}
\label{tab:JSONTimingData}
\begin{tabular}{|p{5 cm}|p{8.8 cm}|}
\hline \hline
\textbf{Field} & \textbf{Meaning} \\
\hline
\verb+name+ & The name of the timestamp\\
\hline
\verb+delta+ & The delta time since the previous timestamp. In this example, the delta is from the time the timestamping started\\
\hline
\verb+total+ & The total time since timestamping started\\
\hline
\end{tabular}
\end{table}

\subsection{Database Changes}
Statistics about hash database changes are reported on the console and to the log file inside the hash database. These statistics show specific changes made to stores within the hash database and also changes not made because conditions were not met. An example change report is shown in Listing \ref{DatabaseChanges}. Changes with a count of zero are not reported. The complete set of statistics tracked is shown in Table \ref{tab:changeStatistics}.

\lstset{style=customfile}
\begin{lstlisting}[float, caption={Example report of a database change from an import operation}, label=DatabaseChanges]
# hashdb changes:
#     hash_data_data_inserted: 401935
#     hash_data_data_same: 195
#     hash_data_source_inserted: 402130
#     hash_prefix_inserted: 401653
#     hash_suffix_inserted: 401935
#     hash_count_changed: 149
#     hash_not_changed: 46
#     source_data_inserted: 101
#     source_id_inserted: 101
#     source_id_already_present: 402231
#     source_name_inserted: 101
import completed.
\end{lstlisting}

\begin{table}[!ht]
\centering
\caption{Database Changes Resulting from Commands that Manipulate Hash Databases}
\label{tab:changeStatistics}
\begin{tabular}{|p{5 cm}|p{8.8 cm}|}
\hline \hline
\textbf{Statistic} & \textbf{Meaning} \\
\hline

% hash_data
\verb+hash_data_data_inserted+ &  Number of hash data records inserted.\\
\hline
\verb+hash_data_data_changed+ &  Number of hash data records changed.\\
\hline
\verb+hash_data_data_same+ &  Number of hash data records provided but same.\\
\hline
\verb+hash_data_source_inserted+ &  Number of hash data source, offset pairs inserted.\\
\hline
\verb+hash_data_source_already_+ \verb+present+ &  Number of hash data source, offset pairs provided but same\\
\hline
\verb+hash_data_source_at_max+ &  Number of hash data source, offset pairs dropped because of max pair limit.\\
\hline

% hash
\verb+hash_prefix_inserted+ &  Number of hash prefix keys inserted.\\
\hline
\verb+hash_suffix_inserted+ &  Number of hash suffix values inserted.\\
\hline
\verb+hash_count_changed+ &  Number of hash count changes were applied.\\
\hline
\verb+hash_not_changed+ &  Number of hash and count values provided but same.\\
\hline

% source data
\verb+source_data_inserted+ &  Number of source data records inserted.\\
\hline
\verb+source_data_changed+ &  Number of source data records changed.\\
\hline
\verb+source_data_same+ &  Number of source data records provided but same.\\
\hline

% source_id
\verb+source_id_inserted+ &  Number of source ID records inserted.\\
\hline
\verb+source_id_already_present+ &  Number of source ID records provided but already present.\\
\hline

% source_name
\verb+source_name_inserted+ &  Number of source names inserted.\\
\hline
\verb+source_name_already_+ \verb+present+ &  Number of source names provided but already present.\\
\hline
\end{tabular}
\end{table}

\section{Using the \hdb Library APIs}
\label{APIs}
\hdb provides C++ and Python interfaces for importing, scanning, and working with block hashes:

\begin{itemize}
\item \textbf{C++ Interfaces}\\
To use C++ interfaces, include interface file \verb+hashdb.hpp+ and link \hdb library \verb+libhashdb+. \hdb interfaces use the \hdb namespace. Interfaces can assert on unexpected error.
\item \textbf{Python Interfaces}\\
To use the Python interfaces, load the \verb+hashdb+ module.
\end{itemize}

For information on installing the \hdb interfaces, please see \textbf{\Autoref{Obtaining}}.  For further details on syntax and usage, please see \hdb header file \texttt{hashdb.hpp} in \textbf{\Autoref{hashdbapi}}. Python users may also want to reference the Python interface test module in the source code at \verb+hashdb/python_bindings/test_hashdb.py+.\\

\subsection{Import}
To import hash and source data, open an import manager, for example\\
\verb+{manager = import_manager_t("hashdb.hdb", "create my DB")+. Then use import functions to add data. Information in the log file will be added when the import manager closes. The contents of log files is described in \textbf{\autoref{ContentsOfDB}}.\\
\begin{itemize}
\item \verb+manager = import_manager_t(hashdb_dir, cmd)+
\item \verb+manager.insert_source_name(file_binary_hash, repository_name, filename)+
\item \verb+manager.insert_source_data(file_binary_hash, filesize, file_type,+\\
\verb+zero_count, nonprobative_count)+
\item \verb+manager.insert_hash(binary_hash, file_binary_hash, file_offset,+\\
\verb+entropy, block_label)+
\item \verb+error_message = manager.import_json(json_string)+\\
Import hash or source, return \verb+error_message+ or "" for no error.
\item \verb+data_sizes = manager.size()+
Return number of entries in LMDB databases.
\item \verb+size_t size_hashes()+\\
Return number of hash records in the database, which is typically more than the number of source, offset pairs in the database.
\item \verb+size_t size_sources()+\\
Return number of sources.
\end{itemize}

\subsection{Scan}
To scan for hashes, open a scan manager, for example\\
\verb+manager = scan_manager_t("hashdb.hdb")+. Then use functions to find hash and source information. Functions that return less information run faster than functions that return more. Scan functions provide read-only access to hash and data stores.

\begin{itemize}
\item \verb+scan_manager = scan_manager_t(hashdb_dir)+
\item \verb+json_hash_string = scan_manager.find_expanded_hash_json(binary_hash)+\\
Find and return JSON text about the hash and its sources. Hash and source information is not returned twice. For example on a second hash match, the returned JSON will only contain the \verb+block_hash+ field. Returns "" on no match.
\item \verb+bool find_hash(binary_hash, &entropy, &block_label, &source_offset_pairs_t)+\\
C++ only. Find hash, obtain fields related to hash on match.
\item \verb+json_text = export_hash_json(binary_hash)+\\
Export hash information for the given binary hash else "" if not there.
\item \verb+json_text= export_source_json(file_binary_hash)+\\
Export source information for the given source else "" if not there.
\item \verb+count = scan_manager.find_hash_count(binary_hash)+\\
Return the source, offset pair count for the hash.
\item \verb+json_text = scan_manager.find_hash_count_json(binary_hash)+\\
Export the hash count in JSON format for the given binary hash else "" if not there.
\item \verb+approximate_count = scan_manager.find_approximate_hash_count(binary_hash)+\\
This is the fastest scan function. It returns an approximate source, offset pair count for the hash, and can be wrong.
\item \verb+json_text = scan_manager.find_approximate_hash_count_json(binary_hash)+\\
Export the approximate hash count in JSON format for the given binary hash else "" if not there.
\item \verb+has_source_data = scan_manager.find_source_data(file_binary_hash,+\\
\verb+filesize, file_type, zero_count, nonprobative_count)+\\
C++ syntax. Return information about the source.
\item \verb+has_source_data, filesize, file_type, zero_count, nonprobative_count =+\\
\verb+scan_manager.find_source_data(file_binary_hash)+\\
Python syntax. Return information about the source.
\item \verb+has_source_names = scan_manager.find_source_names(file_binary_hash,+\\
\verb+&source_names_t)+\\
C++ only. Retrieve the source names for this source or "" on no match.
\item \verb+first_binary_hash = scan_manager.first_hash()+
\item \verb+next_binary_hash = scan_manager.next_hash(binary_hash)+
\item \verb+first_file_binary_hash = scan_manager.first_source()+
\item \verb+file_binary_hash = scan_manager.next_source(file_binary_hash)+
\item \verb+db_sizes = scan_manager.size()+\\
Return sizes of internal data stores in JSON format.
\item \verb+size_hashes = scan_manager.size_hashes()+\\
Return the number of hash records in the database, which is typically more than the number of source, offset pairs in the database.
\item \verb+size_sources = scan_manager.size_sources()+\\
Return the number of sources in the database.
\end{itemize}

\subsection{Scan Stream}
The scan stream interface is provided to allow rapid multi-threaded scans of lists of hashes. The interface accepts long binary strings of unscanned data and returns long binary strings of scanned data. The user must encode and decode this packed data. The user may wish to embed this stream inside a custom socket layer.

\begin{itemize}
\item \verb+scan_stream = scan_stream_t(scan_manager, hash_size, scan_mode)+\\
Open a scan stream interface.
\item \verb+scan_stream.put(unscanned_data)+\\
Submit unscanned data for scanning.
\item \verb+scanned_data = scan_stream.get()+\\
Retrieve scanned data else "" if data is currently not available.
\item \verb+is_empty = scan_stream.empty()+\\
Return true if there is no scanned data available to retrieve, no unscanned data scheduled for scanning, and the scanner threads are not busy.
\end{itemize}

\subsection{Settings}
Holds \hdb settings.

\begin{itemize}
\item \verb+settings = settings_t()+\\
Obtain default settings. Configurable setting parameters are: \verb+settings_version,+\\
\verb+byte_alignment, block_size, max_source_offset_pairs, hash_prefix_bits,+\\
\verb+hash_suffix_bytes+.
\item \verb+settings_string = settings.settings_string()+\\
Return setting values in JSON format.
\end{itemize}

\subsection{Timestamp}
Provide timestamp support.

\begin{itemize}
\item \verb+timestamp = timestamp_t()+\\
Create a timestamp object.
\item \verb+timestamp_string = stamp(text)+\\
Create a named timestamp and provide time and delta from the last stamp time in JSON format.
\end{itemize}

\subsection{Support Functions}
Support functions provide miscellaneous support and are not part of a class.
\begin{itemize}
\item \verb+version = version()+\\
Return the \hdb version.
\item \verb+version = hashdb_version()+\\
Return the \hdb version, same as \verb+version+.
\item \verb+error_message = create_hashdb(hashdb_dir, settings, command_string)+\\
Create a hash database given settings. Return "" else reason for failure.
\item \verb+error_message = read_settings(hashdb_dir, &settings)+\\
Query settings else false and reason for failure.
\item \verb+binary_string = hex_to_bin(hex_string)+
\item \verb+hex_string = bin_to_hex(binary_string)+
\item \verb+error_message = ingest(hashdb_dir, ingest_path, step_size, repository_name,+\\
\verb+whitelist_dir, disable_recursive_processing, disable_calculate_entropy,+\\
\verb+disable_calculate_labels, command_string)+\\
Calculate and import hashes from path to \hdb. Can disable recursive processing, calculating entropy, and calculating labels.
\item \verb+error_message = scan_image(hashdb_dir, media_image_file, step_size,+\\
\verb+disable_recursive_processing)+\\
Scan the media image for matches, writing match data to \verb+stdout+.
\item \verb+error_message = read_bytes(media_image_file, forensic_path, count, &bytes)+\\
C++ syntax.  Read bytes at numeric offset from a media image file.
\item \verb+error_message, bytes_read = read_bytes(media_image_file, forensic_path, count)+\\
Python syntax. Read bytes at forensic path from a media image file, for example \verb+1000+ or \verb+1000-zip-0+.
\item \verb+error_message = read_bytes(media_image_file, offset, count, &bytes)+\\
C++ syntax.  Read bytes at numeric offset from a media image file.
\item \verb+error_message, bytes_read = read_bytes(media_image_file, forensic_path, count)+\\
Python syntax. Read bytes at forensic path from a media image file, for example \verb+1000+ or \verb+1000-zip-0+.
\end{itemize}

\subsubsection{Data Types}

The following data types are defined for the C++ interface:

\begin{itemize}
\item \verb+source_offset_pair_t: typedef pair<file_binary_hash, file_offset> id_offset_pair_t+
\item \verb+source_offset_pairs_t: typedef set<source_offset_pair_t> source_offset_pairs_t+
\item \verb+source_name_t: typedef pair<repository_name, fillename> source_name_t+
\item \verb+source_names_t: typedef set<source_name_t> source_names_t+
\end{itemize}

\section{LMDB Data Stores}
\label{LMDBDataStores}
This section provides details of how LMDB data stores are managed within a \hdb database. This technical information is provided to give context behind the optimization settings and options provided by \hdb.\\

\subsection{LMDB Hash Store}
The \textit{LMDB Hash Store} is a highly compressed optimized store of all the block hashes in the database.  When scanning for a hash, if it is not in this store, then it is not in the database.  Because of the degree of optimization, there can be false positives.  To compensate, when a hash is found in the \textit{LMDB Hash Store}, \hdb reads the \textit{LMDB Hash Information Store} to be sure the hash actually exists.\\

The \textit{LMDB Hash Store} is a hybrid B-Tree and List store:
\begin{itemize}
\item The \verb+key+ portion consists of the first few bits of the hash, with enough bits so that the \verb+value+ portion is not too large.
\item The \verb+value+ portion consists of a list of the last few bytes of the hash, with enough bytes so that the bits of the hash in the \verb+key+ and the bytes of the hash in the \verb+value+ collectively identify the hash as present with a sufficiently low false positive rate.
\end{itemize}

The \textit{LMDB Hash Store} uses two tuning parameters, hash prefix bits and hash suffix bytes, to minimize data store size and maximize lookup speed.  These parameters are described in \textbf{\Autoref{Settings}}.\\


\subsection{LMDB Hash Information Store}
The \textit{LMDB Hash Information Store} is a store of all hashes and their associated data and source information:

\begin{itemize}
\item The \verb+key+ portion consists of a block hash, in binary.
\item The \verb+value+ portion contains metadata fields about the hash plus a list of locations in sources where the block was identified.  Specifically, the \verb+value+ portion consists of:
  \begin{itemize}
  \item \verb+entropy+ The calculated entropy for the block.
  \item \verb+block_label+ A label identifying information about the block.  Users may wish to examine \verb+entropy+ and \verb+block_label+ together to estimate that a block might be nonprobative.
  \item A list of source IDs and file offset pairs indicating places where the block was identified.
  \end{itemize}
\end{itemize}

The \textit{LMDB Hash Data Store} uses two tuning parameters, \verb+max_id_offset_pairs+ and \verb+block_size+ to minimize data store size.  These parameters are described in \textbf{\Autoref{Settings}}.\\

\subsection{LMDB Source ID Store}
The \textit{LMDB Source ID Store} maps source file hash values to source IDs.  Although the user never sees source IDs, we use source IDs in the \textit{LMDB Source ID Store}, \textit{LMDB Source Data Store}, and the \textit{LMDB Hash Information Store} because they are significantly shorter than source file hashes.  We wouldn't need source IDs if we didn't make this optimization.

\begin{itemize}
\item The \verb+key+ is the \verb+file_binary_hash+.
\item The \verb+value+ is the \verb+source_id+.
\end{itemize}

\subsection{LMDB Source Data Store}
The \textit{LMDB Source Data Store} holds all the metadata about sources:
\begin{itemize}
\item The \verb+key+ is the \verb+source_id+.
\item The \verb+value+ consists of these fields:
  \begin{itemize}
  \item \verb+file_binary_hash+ The source file hash associated with this source ID, in binary.
  \item \verb+filesize+ The size of the source file, in bytes.
  \item \verb+file_type+ A label indicating the type of the file, user defined.
  \item \verb+zero_count+ The number of blocks in the source that have all bytes in the block equal to zero.
  \item \verb+nonprobative_count+ The number of block hashes stored for this source which are considered to be nonprobative.  Users may wish to set the \verb+nonprobative_count+ value based on the \verb+entropy+ and \verb+block_label+ values of each block in the source.
  \end{itemize}
\end{itemize}

\subsection{LMDB Source Name Store}
The \textit{LMDB Source Name Store} multimap maps source IDs to source names.  This store allows us to not re-import hashes from the same source and also allows us to see the list of source names that are of the same source.
\begin{itemize}
\item The \verb+key+ is the \verb+source_id+.
\item The \verb+value+ is a name pair of:
  \begin{itemize}
  \item \verb+repository_name+ A label indicating the source repository.
  \item \verb+filename+ The path to this source.
  \end{itemize}
\end{itemize}

\bibliographystyle{acm} 
\bibliography{references}

\newpage
\appendix
\appendixpage

\section{\hdb Quick Reference}
\label{QuickReference}
\input hashdb_quick_reference_text.tex
\newpage

\section{Output of the \hdb Help Command}
\label{HelpOutput}
\begingroup
\footnotesize
{
\fontfamily{courier}\selectfont
\verbatiminput{hashdb_usage.txt}
}
\endgroup


\section{\hdb C++ API: \texttt{hashdb.hpp}}
\label{hashdbapi}
\lstset{language=C++}
\lstset{basicstyle=\footnotesize}
\lstset{breaklines=true}
\lstset{breakatwhitespace=true}
\lstinputlisting{../../src_libhashdb/hashdb.hpp}

\end{document}
