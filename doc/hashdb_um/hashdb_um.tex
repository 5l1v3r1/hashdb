\documentclass[11pt,fleqn]{article} % Default font size and left-justified equations

%\usepackage{standalone}

\usepackage{todonotes}
\usepackage{color}
% use \todo{note} OR \missingfigure{Add my picture here}

\include{structure}

\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}
 
%\usepackage{arabtex}

\usepackage{verbatim}

\raggedbottom

\begin{document}

%define macros for commonly used terms that require special formatting
\newcommand \hdb {\textit{hashdb}\xspace}
\newcommand \sscope {\textit{SectorScope}\xspace}
\newcommand \aut {\textbf{Autopsy}\xspace}
\newcommand \bulk {\textbf{bulk\_extractor}\xspace}

\hypersetup{%
    pdfborder = {0 0 0}
}

\lstdefinestyle{customfile}{
basicstyle=\footnotesize\ttfamily, frame=single, float=htpb}

\input{./title.tex}

\pagenumbering{roman}
\setlength{\parindent}{0pt} %remove indenting from whole document
\newpage
\thispagestyle{empty}
\mbox{}
\newpage

\section*{One Page Quickstart for Windows Users}
 This page provides a very brief introduction to downloading, installing and running \hdb on Windows systems. 
\begin{enumerate}
\item Download the windows installer for the latest version of \hdb. It can be obtained from \url{http://digitalcorpora.org/downloads/hashdb}. The file is named \texttt{hashdb-x.y.z-windowsinstaller.exe} where x.y.z is the latest version. 

\item Run the installer file. This will automatically install \hdb on your machine.

\item Navigate to the directory where you would like to create a hash database. Then, to run \hdb from the command line, type the following instructions: 
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb create sample.hdb}
\end{Verbatim} 

In the above instructions, \texttt{\textbf{sample.hdb}} is the empty database that will be created with default database settings.

\item Next, import data into the database. In this example, import
block hashes from files under directory \texttt{blacklist\_files}.
type the following instructions from the directory where you created the database:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb import sample.hdb blacklist_files}
\end{Verbatim} 
This command, if executed successfully, will print statistics about hash values inserted. For example: 
\begingroup
\footnotesize
\begin{Verbatim}[fontfamily=courier]
# hashdb changes:
#     hash_data_data_inserted: 128
#     hash_data_source_inserted: 128
#     hash_prefix_inserted: 128
#     hash_suffix_inserted: 128
#     source_data_inserted: 1
#     source_data_changed: 1
#     source_id_inserted: 1
#     source_id_already_present: 129
#     source_name_inserted: 1

\end{Verbatim}
\endgroup
\item Additionally, the file \texttt{log.txt} contained in the directory \textit{sample.hdb} will be updated with change statistics. It will include the number of hash values that have been inserted [see \textbf{\Autoref{ScanServices}} for more information on the change statistics tracked in the log file].
 

\end{enumerate}

\newpage
\section*{One Page Quickstart for Linux and Mac Users}
This page provides a very brief introduction to downloading, installing and running \hdb (creating a database and populating it) on Linux and MacOS systems. 
\begin{enumerate}
\item Download the latest version of \hdb. It can be obtained from \url{http://digitalcorpora.org/downloads/hashdb}. The file is called \texttt{hashdb-x.y.z.tar.gz} where x.y.z is the latest version. 

\item Un-tar and un-zip the file.  In the newly created \textit{hashdb-x.y.z} directory, run the following commands:

\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{./configure}
\verbbf{make}
\verbbf{sudo make install}
\end{Verbatim}
Note, users will likely need to first download and install dependent library files. Instructions are outlined in the referenced section.  [Refer to \textbf{\Autoref{Installation}}].

\item Navigate to the directory where you would like to create a hash database. Then, to run \hdb from the command line, type the following instructions: 
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb create sample.hdb}
\end{Verbatim} 

In the above instructions, \texttt{\textbf{sample.hdb}} is the empty database that will be created with default database settings. 

\item Next, import data into the database. In this example, import
block hashes from files under directory \texttt{blacklist\_files}.
type the following instructions from the directory where you created the database:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb import sample.hdb blacklist_files}
\end{Verbatim} 
This command, if executed successfully, will print statistics about hash values inserted. For example: 
\begingroup
\footnotesize
\begin{Verbatim}[fontfamily=courier]
# hashdb changes:
#     hash_data_data_inserted: 128
#     hash_data_source_inserted: 128
#     hash_prefix_inserted: 128
#     hash_suffix_inserted: 128
#     source_data_inserted: 1
#     source_data_changed: 1
#     source_id_inserted: 1
#     source_id_already_present: 129
#     source_name_inserted: 1
\end{Verbatim}
\endgroup
\item Additionally, the file \texttt{log.txt} contained in the directory \textit{sample.hdb} will be updated with change statistics. It will include the number of hash values that have been inserted [see \textbf{\Autoref{ScanServices}} for more information on the change statistics tracked in the log file].
 
\end{enumerate}
\newpage


\tableofcontents
\newpage
\pagenumbering{arabic}





\newpage

\section{Introduction}
\subsection {Overview of \hdb}
\hdb is a tool that can be used to find data in raw media using cryptographic hashes calculated from blocks of data. It is a useful forensic investigation tool for tasks such as malware detection, child exploitation detection or corporate espionage investigations. The tool provides several capabilities that include:
\begin{itemize}
\item Creating hash databases of MD5 block hashes, as opposed to file hashes.
\item Importing block hash values.
\item Scanning the hash database for matching hash values.
\item Providing the source information for hash values. 
\end{itemize}

Using \hdb, a forensic investigator can take a known set of blacklisted media and generate a hash database. The investigator can then use the hash database to search against raw media for blacklisted information. For example, given a known set of malware, an investigator can generate a sector hash database representing that malware. The investigator can then search a given corpus for fragments of that malware and identify the specific malware content in the corpus.\\

\hdb relies on block hashing rather than full file hashing. Block hashing provides an alternative methodology to file hashing with a different capability set. With file hashing, the file must be complete to generate a file hash, although a file carver can be used to pull together a file and generate a valid hash.  File hashing also requires the ability to extract files, which requires being able to understand the file system used on a particular storage device. Block hashing, as an alternative, does not need a file system or files. Artifacts are identified at the block scale (usually 512 bytes) rather than at the file scale. While block hashing does not rely on the file system, artifacts do need to be sector-aligned for \hdb to find hashes \cite{hashEncoding}.\\

\hdb provides an advantage when working with hard disks and operating systems that fragment data into discontiguous blocks yet still sector-align media. This is because scans are performed along sector boundaries. Because \hdb works at the block resolution, it can find part of a file when the rest of the file is missing, such as with a large video file where only part of the video is on disk. \hdb can also be used to analyze network traffic (such as that captured by \textbf{tcpflow}).  Finally, \hdb can identify artifacts that are sub-file, such as embedded content in a \texttt{.pdf} document.\\

\hdb stores cryptographic hashes (along with their source information) that have been calculated from hash blocks. It also provides the capability to scan other media for hash matches.
This manual describes uses cases for the \hdb tools, including usage with \aut, \sscope, \bulk, and the \hdb Python and C++ libraries, and demonstrates how users can take full advantage of all of its capabilities.

\subsection{Purpose of this Manual}
This Users Manual is intended to be useful to new, intermediate and experienced users of \hdb. It provides an in-depth review of the functionality included in \hdb and shows how to access and utilize features through command line operation of the tool. This manual includes working examples with links to the input data used, giving users the opportunity to work through the examples and utilize all aspects of the system. 

\subsection{Conventions Used in this Manual}
This manual uses standard formatting conventions to highlight file names, directory names and example commands. The conventions for those specific types are described in this section. \\

Names of programs including the post-processing tools native to \hdb and third-party tools are shown in \textbf{bold}, as in \textbf{bulk\_extractor}.\\

File names are displayed in a fixed width font. They will appear as \texttt{filename.txt} within the text throughout the manual.\\

Directory names are displayed in italics. They appear as \textit{directoryname/} within the text. The only exception is for directory names that are part of an example command. Directory names referenced in example commands appear in the example command format.\\

Database names are denoted with bold, italicized text. They are always specified in lower-case, because that is how they are referred in the options and usage information for \hdb. Names will appear as \textbf{\textit{databasename}}.\\

This manual contains example commands that should be typed in by the user. A command entered at the terminal is shown like this: \begin{Verbatim}[commandchars=\\\{\}]
\verbbf{command}
\end{Verbatim}

The first character on the line is the terminal prompt, and should not be typed. The black square is used as the standard prompt in this manual, although the prompt shown on a users screen will vary according to the system they are using.\\


\section{How \hdb Works}
The \hdb tool provides capabilities to create, edit, access and search databases of cryptographic hashes created from hash blocks. The cryptographic hashes are imported into a database from a directory, another database, \textbf{bulk\_extractor} or JSON data, or trough the \hdb API.
Once a databases is created, \hdb provides users with the capability to scan the database for matching hash values and identify matching content. Hash databases can also be exported, added to, subtracted from and shared.\\


Figure \ref{fig:overviewFigure} provides an overview of the capabilities included with the \hdb tool. \hdb populates databases from whitelist source files
or other media provided in JSON format or through the API.
Users can also add or remove data from the database after it is created.
Once the database is populated, \hdb can export content from the database in JSON format. It also provides an API that can be used by third party tools (as it is used in the \bulk program) to create, populate and access hash databases. Finally, \hdb allows users to scan the hash database for matching hash values.\\

\begin{figure}
	\center
	\includegraphics[scale=.45]{drawings/hashdb_system_overview}
	\caption{Overview of the \hdb system}
	\label{fig:overviewFigure}
\end{figure}

\subsection{Blacklist Data}
Blacklist data is the data we scan against to determine whether forensic data
contains probative artifact.
We build a hash database of blacklist data by importing block hashes
from blacklist files, copying from other hash databases,
or importing from other sources using data prepared in JSON format.

\subsection{Forensic Data}
Forensic data is the data we scan to see if it contains artifact matching
that in our hash database.
Note that just having matches is not sufficiently probative.
Some matches are common to many files.
\hdb tracks entropy and data information to automate the process
of eliminating many false positives.
Direct analysis such as that provided by the \sscope tool
may be used to see the exact content at that location.
\sscope is available at \url{https://github.com/NPS-DEEP/NPS-SectorScope/wiki}.

\subsection{Hash Blocks}
\label{HashBlocks}
\hdb works by matching blocks.
\hdb is different from tools that match files because it can find matches
even when part of a file is missing or changed.
\hdb stores and scans for hashes created from contiguous blocks of data.
We call the size of the block hashed the \textit{block size}.
\hdb stores and scans for hashes aligned along sector boundaries.
We call this alignment the \textit{sector size}.
Although larger values take up more space, we have found that the smallest
granularity used by file systems and storage devices is 512 bytes,
so this is the default value for both.

\subsection{Building a \hdb Database}
There are several ways to populate a database:

\begin{itemize}
\item Using the \hdb \verb+import+ command.
\item Importing from correctly formatted JSON data.
\item Importing from another database.
\item Using the \bulk \hdb scanner import function.
\item Using the \hdb library through the Python or C++ interface.
\end{itemize}

A database may contain blacklist hashes from multiple source domains,
where a domain is called a \textit{repository}.
The repository name indicates the provenance of the dataset.
It is its description information, such as ``Company X’s intellectual property files''.
Multiple source files can actually be the same file but with a different name.
To manage this, \hdb tracks sources by source hash,
and a source hash can be referred to by multiple filenames and repository names.\\

Hashes are matched with source, offset pairs.  The source is tracked by the source hash.  The offset is the file offset, in bytes, into the file where the block hash was calculated.\\

\subsection{Scanning}
There are multiple ways users can scan for matches in a block hash database:

\begin{itemize}
\item Using one of the \hdb tool scan commands to scan from a media image,
list, stream, or specific hash.
\item Using one of the \hdb API scan interfaces to scan at a particular
level of detail.
\item Using the \bulk \hdb scanner Scan function.
\end{itemize}

\subsection{Contents of a Hash Database}
Each \hdb database is contained in a directory called \textit{$<$databasename$>$.hdb} and contains a number of files. These files are:
\begin{verbatim}
lmdb_hash_data_store/data.mdb
lmdb_hash_data_store/lock.mdb
lmdb_hash_store/data.mdb
lmdb_hash_store/lock.mdb
lmdb_source_data_store/data.mdb
lmdb_source_data_store/lock.mdb
lmdb_source_id_store/data.mdb
lmdb_source_id_store/lock.mdb
lmdb_source_name_store/data.mdb
lmdb_source_name_store/lock.mdb
log.txt
settings.json
\end{verbatim}

These files include several data store directories and files, a settings file, and a log file.
Each is described briefly:

\begin{itemize}
\item Data store files \\
The data store files encode all the block hashes, source files, and related information that are in the database.


\item \texttt{log.txt} \\
Every time a command is run that changes the content of the database, this file is replaced with a log of the run.
The log includes the command name, information about \hdb including the command typed and how \hdb was compiled, information about the operating system \hdb was just run on, timestamps indicating how much time the command took, and the specific \hdb changes applied, described in more detail in \textbf{\autoref{Running}}.
\item \texttt{settings.json} \\
This file contains the settings requested by the user when the block hash database was created, see \hdb settings and Bloom filter settings options.
This file also contains internal \hdb settings version used to help \hdb identify whether a database is compatible with this version of \hdb.
\end{itemize}




\subsection{Using the Hash Databases}
\label{usingSection}
\hdb provides the capability for users to scan the database for matching hash blocks. Users can also query for hash source information and information about the hash database itself. \hdb provides an API to access the import and scan capabilities.  The import capability allows third party tools to create a new database at a specified directory, import an array of hashes with source information and write changes to the \texttt{log.xml} file. The scan capability provided by the API allows third party tools to open an existing database and perform a scan. Most importantly, the \bulk \textit{hashdb} scanner uses the \hdb API to provide users with the capability to create databases from disk images or scan digital media and find matching hash blocks within the data \bulk is processing. In later sections, this manual describes the methods for using \bulk together with the \hdb tool. 


\section{Installation Guide}
\hdb is a command line tool that can be run on Linux, MacOS or Windows systems. Here we describe the installation procedures for those systems.
Steps include how to install the required dependencies as well as download \hdb and compile the release or run the executable. 
\label{Installation}

\subsection{Installing on Linux or Mac}

Before compiling \hdb for your platform, you may need to install other packages on your system which \hdb requires to compile cleanly and with a full set of capabilities.\\

\textbf{Dependencies for Linux}\\
The following commands should add the appropriate packages:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{sudo yum update}
\verbbf{sudo yum groupinstall development-tools}
\verbbf{sudo yum install gcc-c++}
\verbbf{sudo yum install libxml2-devel openssl-devel tre-devel boost-devel}
\end{Verbatim}

\textbf{Dependencies for Mac Systems}\\
Mac users must first install Apple's Xcode development system. Other components should be downloaded using the MacPorts system. If you do not have MacPorts, go to the App store and download and install it. It is free. Once it is installed, try:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{sudo port install autoconf automake libxml2} 
\end{Verbatim}

\textbf{Download and Install \hdb}\\
Next, download the latest version of \hdb. The software can be downloaded from \url{http://digitalcorpora.org/downloads/hashdb/}. The file to download is \texttt{hashdb-x.y.z.tar.gz} where x.y.z is the latest version. As of publication of this manual, the latest version of \hdb is 1.1.1.\\


After downloading the file, un-tar it by either right-clicking on the file and choosing ``extract to...' or typing the following at the command line:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{tar -xvf hashdb-x.y.z.tar.gz}
\end{Verbatim}

Then, in the newly created \textit{hashdb-x.y.z} directory, run the following commands to install \hdb in \textit{/usr/local/bin} (by default):

\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{./configure}
\verbbf{make}
\verbbf{sudo make install}
\end{Verbatim}
\hdb is now installed on your system and can be run from the command line. \\

Note: sudo is not required. If you do not wish to use sudo,  build and install \hdb and \bulk in your own space at ``\$HOME/local'' using the following commands:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{./configure --prefix=$HOME/local/ --exec-prefix=$HOME/local CPPFLAGS=-}
\textbf{                                 I$HOME/local/include/ LDFLAGS=-L$HOME/local/lib/}
\verbbf{make}
\verbbf{make install}
\end{Verbatim}

\subsection{Installing on Windows}
\label{InstallingOnWindows}
Windows users should download the Windows Installer for \hdb. The file to download is located at \url{http://digitalcorpora.org/downloads/hashdb} and is called \texttt{hashdb-x.y.} \texttt{z-windowsinstaller.exe} where x.y.z is the latest version number (1.1.1 as of publication of this manual).\\

You should close all Command windows before running the installation executable. Windows will not be able to find the \hdb tools in a Command window if any are open during the installation process. If you do not do this before installation, simply close all Command windows after installation. When you re-open, Windows should be able to find \hdb.\\


 Next run the \texttt{hashdb-x.y.z-windowsinstaller.exe} file. This will automatically install \hdb on your machine. Some Windows safeguards may try to prevent you from running it. Figure \ref{fig:windowsWarning} shows the message Windows 8 displays when trying to run the installer. To run anyway, click on ``More info'' and then select ``Run Anyway.'' \\
\begin{figure}
	\center
	\includegraphics[scale=.5]{windowsWarning.png}
	\caption{Windows 8 warning when trying to run the installer. Select ``More Info'' and then ``Run Anyway.''}
	\label{fig:windowsWarning}
\end{figure}

When the installer file is executed, the installation will begin and show a dialog like the one shown in Figure \ref{fig:windowsInstaller}.  Users should select the 64-bit configuration to install \hdb and select \verb+Add to path+ to add hashdb to \verb+PATH+. \hdb is now installed on your system can be run from the command line.\\

\begin{figure}
	\center
	\includegraphics[scale=.8]{WindowsInstaller.png}
	\caption{Dialog appears when the user executes the Windows Installer. Select the default configuration.}
	\label{fig:windowsInstaller}
\end{figure}

\subsection{Installing Other Related Tools}
Please see \textbf{\Autoref{OtherTools}} for information on installing related tools.

\section {Running the \hdb Tool}
\label{Running}
The core capabilities provided by \hdb involve creating and maintaining a database of hash values and scanning media for those hash values. To perform those tasks, \hdb users need to start by building a database (if an existing database is not available for use).
Users then import hashes using \hdb commands, the \hdb \bulk scanner, or the \hdb API, and then possibly merge or subtract hashes to obtain the desired set of hashes to scan against.
Users then scan for hashes that match.
Additional commands are provided to support statistical analysis, performance tuning and performance analysis.\\

This section describes \hdb commands, along with examples, for performing these tasks.
For more examples of command usage, please see \textbf{\autoref{UseCases}}.
For a \hdb quick reference summary, please see \textbf{\autoref{QuickReference}}
or \url{http://digitalcorpora.org/downloads/hashdb/hashdb_quick_reference.pdf}.

\subsection{Creating a Hash Database}
\label{Creating}
A hash database must be created before hashes can be added to it.
The command to create a hash database is shown in Table \ref{tab:createDatabase}.
\begin{table}[!ht]
\centering
\caption{Command for Creating Hash Databases}
\label{tab:createDatabase}
\begin{tabular}{|p{2.5 cm}|p{7 cm}|p{4 cm}|}
\hline \hline
\textbf{Command} & \textbf{Usage} & \textbf{Description} \\
\hline
\textbf{create} & \verb+create [-b <block size>]+ \verb+[-b <sector size>]+ \verb+[-m <max ID offset pairs>]+ \verb+[-t <hash prefix bits:hash+ \verb+suffix bytes>]+ \verb+<hashdb.hdb>+ & Creates a new hash database with the given configuration settings.\\
\hline
\end{tabular}
\end{table}

Table \ref{tab:hashDBSettings} shows the configurable database settings.\\

\begin{table}[!ht]
\centering
\caption{Settings for New Databases}
\label{tab:hashDBSettings}
\begin{tabular}{|p{1.5 cm}|p{8 cm}|p{4 cm}|}
\hline \hline
\textbf{Option} & \textbf{Verbose Option} & \textbf{Specification} \\
\hline
\textbf{\texttt{-b}} & \verb+--block_size=+\textit{block\_size} & Specifies the block size in bytes used to generate the hashes that will be stored in the database. Default is 512 bytes.  \\
\hline
\textbf{\texttt{-s}} & \verb+--sector_size=+\textit{sector\_size} & Specifies the sector size in bytes used to calculate hashes along. Default is 512 bytes.  \\
\hline
\textbf{\texttt{-m}} & \verb+--max_source_offset_pairs=+\textit{max source offset pairs} & Specifies the maximum number of source, source offset pairs allowed. 0 value indicates there is no limit. Default is 0.\\
\hline
\textbf{\texttt{-t}} & \verb+--tuning=+\textit{prefix bits:suffix bytes} & Specifies the number of prefix bits and suffix bytes to use for compacting the database.\\
\hline
\end{tabular}
\end{table}

see \textbf{\Autoref{Settings}} for detailed description of these settings.\\

\textbf{Example}\\
To create an (empty) hash database named \textbf{\texttt{mock\_video.hdb}}, type the following command:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb create mock_video.hdb}
\end{Verbatim}
The above command will create a database with all of the default hash database settings. Most users will not need to change these settings.
Users can specify either the option and value or the verbose option value for each parameter along with the create command, as in:\\
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb create --max\_source\_offset\_pairs=20 mock_video.hdb}
\verbbf{hashdb create -m 20 mock_video.hdb}
\end{Verbatim}
The above two commands produce identical results, creating the database \texttt{mock\_video.hdb} that will accept a maximum of 20 source, offset pairs per hash.\\

\subsection{Importing and Exporting}
Hash databases may be imported to in several ways, and may be exported in JSON format.
Commands to import and export hashes are shown in Table \ref{tab:importExport}.\\
\begin{table}[!ht]
\centering
\caption{Commands for Importing into and Exporting Hash Databases}
\label{tab:importExport}
\begin{tabular}{|p{2.5 cm}|p{7 cm}|p{4 cm}|}
\hline \hline
\textbf{Command} & \textbf{Usage} & \textbf{Description} \\
\hline
\textbf{import\_dir} & \verb+import_dir [-r <repository name>]+ \verb+[-w <whitelist.hdb>]+ \verb+<hashdb.hdb>+ \verb+<import directory>+& Imports hashes from files under the import directory.\\
\hline
\textbf{import\_tab} & \verb+import_tab [-r <repository name>]+ \verb+<hashdb.hdb>+ \verb+<tab.txt>+& Imports values from the tab-delimited file into the hash database.\\
\hline
\textbf{import} & \verb+import <hashdb.hdb>+ \verb+<hashdb.json>+& Imports values from the JSON file into the hash database.\\
\hline
\textbf{export} & \verb+export <hashdb.hdb>+ \verb+<hashdb.json>+& Exports the hash database to the JSON file\\
\hline
\end{tabular}
\end{table}
Note that there are other ways to populate a database besides these listed here, including using other hash databases (discussed in \textbf{\autoref{updateSection}}),
by using the \bulk \hdb scanner (discussed in \textbf{\autoref{bulkextractorSection}}),
and through the use of the import capability provided by the \hdb library API (discussed in \textbf{\autoref{APISection}}).\\

The tab-delimited import file consists of hash lines separated by carriage returns, where each line consists of a filename followed by a tab followed by the file hash followed by a sector index that starts at 1.  Comment lines are allowed by starting them with the \texttt{\#} character.
An example tab-delimited file is shown in Listing \ref{importTabFile}.

\lstset{style=customfile}
\begin{lstlisting}[float, caption={Example content of a tab-delimited import file}, label=importTabFile]
# tab-delimited import file
file1	3b6b477d391f73f67c1c01e2141dbb17	1
file1	89a170b6b9a948d21d1d6ee1e7cdc467	2
file1	f58a09656658c6b41e244b4a6091592c	3
\end{lstlisting}


To import block hashes from a directory of blacklist sources, type the following command:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb import_dir -r mock_video_repository mock_video.hdb mock_video_dir}
\end{Verbatim}
In the above command the option \textbf{-r} is used along with the repository name \texttt{mock\_video\_repository} to indicate the repository source of the block hashes being imported into the database. The repository name is used to keep track of the sources of hashes. Hash blocks contained in one database often originate from many different sources and the fileme may be the same. For example, if we add two separate but similar databases with partial overlap to a database, this will result in some duplicate hashes from multiple sources with the same filename. The repository name can be used with those duplicates to allow users to track all hashes back to their original sources. By default, the repository name used is the text \texttt{repository\_} with the filename of the file being imported from appended after it.\\

The \textbf{import} command in the above example imports block hashes from files in the \texttt{mock\_video\_dir} directory into the database \texttt{mock\_video.hdb}. \hdb prints the following to the command line to indicate that the hashes have been inserted into the database successfully: 

\begingroup
\footnotesize
\begin{Verbatim}[fontfamily=courier]
zzz replace with correct data
#    hashes inserted: 2595
\end{Verbatim}
\endgroup
The database \texttt{\textbf{mock\_video.hdb}} now contains these block hashes.

The file \texttt{log.txt} shows that a set of hash blocks have just been inserted. Listing \ref{logexcerpt} shows the excerpt of the log file that tracks this statistic.
\lstset{style=customfile}
\begin{lstlisting}[float, caption=Excerpt of the \texttt{log.xml} indicating hash blocks were inserted, label=logexcerpt]
zzz replace with correct data
...   
   <repository_name>mock_video_repository</repository_name>
    <timestamp name='begin import' delta='0.024016' total='0.024016'/>
    <timestamp name='end import' delta='0.015009' total='0.039025'/>
    <hashdb_changes>
      <hashes_inserted>2595</hashes_inserted>      
    </hashdb_changes>
...
\end{lstlisting}
Users may prefer to run statistical commands such as this to get information about the contents of the database (and confirm that values were inserted):
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb size mock_video.hdb}
\end{Verbatim}

\subsection{Database Manipulation}
\label{DatabaseManipulation}
Databases may need to be merged together or common hash values may need to be subtracted out in order to produce a specific set of blacklist data to scan against.
Commands that manipulate hash databases are outlined in Table \ref{tab:databaseManipulation}.
Destination databases are created if they do not exist yet.

\begin{table}[!ht]
\centering
\caption{Commands to Manipulate Hash Databases}
\label{tab:databaseManipulation}
\begin{tabular}{|p{3.5 cm}|p{6 cm}|p{4 cm}|}
\hline \hline
\textbf{Command} & \textbf{Usage} & \textbf{Description} \\
\hline
\textbf{add} & \verb+add <source db>+ \verb+<destination db>+ & Copies all of the hashes from \textit{source db} to \textit{destination db}\\
\hline
\textbf{add\_multiple} &  \verb+add_multiple <source db1>+ \verb+<source db2>+ \verb+...+ \verb+<destination db>+ & Adds databases \textit{source db1}, \textit{source db2}, etc.\ to \textit{destination db}\\
\hline
\textbf{add\_repository} & \verb+add_repository <source db1>+ \verb+<destination db2>+ \verb+<repository name>+ & Adds \textit{source db1} to \textit{destination db2} but only when the repository name matches\\
\hline
\textbf{intersect} & \verb+intersect <source db1>+ \verb+<source db2> <destination db>+ &   Copies hash values common to both \textit{source db1} and \textit{source db2} into \textit{destination db} where sources match\\
\hline
\textbf{intersect\_hash} & \verb+intersect_hash <source db1>+ \verb+<source db2> <destination db>+ &   Copies hash values common to both \textit{source db1} and \textit{source db2} into \textit{destination db} even if their sources are different.\\
\hline
\textbf{subtract} & \verb+subtract <source db1>+ \verb+<source db2> <destination db>+&   Copies hash values found in \textit{source db1} but not in \textit{source db2} into \textit{destination db} where sources match\\
\hline
\textbf{subtract\_hash} & \verb+subtract <source db1>+ \verb+<source db2> <destination db>+&   Copies hash values found in \textit{source db1} but not in \textit{source db2} into \textit{destination db} even if their sources are different.\\
\hline
\textbf{subtract \_repository} & \verb+subtract_repository+ \verb+<source db1>+ \verb+<destination db2>+ \verb+<repository namedb>+ & Adds \textit{source db1} to \textit{destination db2} unless the repository name matches\\
\hline
\textbf{deduplicate} & \verb+deduplicate <source db>+ \verb+<destination db>+ &   Copies hash values that have only one source, offset pair associated with them from \textit{source db} into \textit{destination db}\\
\hline
\end{tabular}
\end{table}

\subsubsection{Tracking Changes in Hash Databases}
Statistics about hash database changes are reported on the console and to the log file inside the hash database.
These statistics show specific changes made to stores within the hash database
and also changes not made because conditions were not met.
These statistics are shown in Table \ref{tab:changeStatistics}.
\begin{table}[!ht]

\centering
\caption{Database Changes Resulting from Commands that Manipulate Hash Databases}
\label{tab:changeStatistics}
\begin{tabular}{|p{5 cm}|p{8.8 cm}|}
\hline \hline
\textbf{Statistic} & \textbf{Meaning} \\
\hline

% hash_data
\verb+hash_data_data_inserted+ &  Number of hash data records inserted.\\
\hline
\verb+hash_data_data_changed+ &  Number of hash data records changed.\\
\hline
\verb+hash_data_data_same+ &  Number of hash data records provided but same.\\
\hline
\verb+hash_data_source_inserted+ &  Number of hash data source, offset pairs inserted.\\
\hline
\verb+hash_data_source_already_+ \verb+present+ &  Number of hash data source, offset pairs provided but same\\
\hline
\verb+hash_data_source_at_max+ &  Number of hash data source, offset pairs dropped because of max pair limit.\\
\hline

% hash
\verb+hash_prefix_inserted+ &  Number of hash prefix keys inserted.\\
\hline
\verb+hash_suffix_inserted+ &  Number of hash suffix values inserted.\\
\hline
\verb+hash_count_changed+ &  Number of hash count changes were applied.\\
\hline
\verb+hash_not_changed+ &  Number of hash and count values provided but same.\\
\hline

% source data
\verb+source_data_inserted+ &  Number of source data records inserted.\\
\hline
\verb+source_data_changed+ &  Number of source data records changed.\\
\hline
\verb+source_data_same+ &  Number of source data records provided but same.\\
\hline

% source_id
\verb+source_id_inserted+ &  Number of source ID records inserted.\\
\hline
\verb+source_id_already_present+ &  Number of source ID records provided but already present.\\
\hline

% source_name
\verb+source_name_inserted+ &  Number of source names inserted.\\
\hline
\verb+source_name_already_+ \verb+present+ &  Number of source names provided but already present.\\
\hline
\end{tabular}
\end{table}

\subsection{Scan Services}
\label{ScanServices}
\hdb can be used to determine if a file, directory or disk image has content that matches previously identified content. This capability can be used, for example, to determine if a set of files contains a specific file excerpt or if a media image contains a video fragment. Forensic investigators can use this feature to search for blacklisted content.
Scan services are shown in Table \ref{tab:scanServices}. \\

\begin{table}[!ht]
\centering
\caption{Commands that Provide Scan Services}
\label{tab:scanServices}
\begin{tabular}{|p{3.5 cm}|p{6 cm}|p{4 cm}|}
\hline \hline
\textbf{Command} & \textbf{Usage} & \textbf{Description} \\
\hline
\textbf{scan} & \verb+scan <path>+ \verb+<DFXML file>+ & Scans the hashdb for hashes that match hashes in the DFXML file and prints out matches\\
\hline
\textbf{scan\_hash} & \verb+scan_hash <path>+ \verb+<hash value>+ & Scans the hashdb for the specified hash value and prints out whether it matches\\
\hline
\textbf{scan\_expanded} & \verb+scan_expanded [-m <number>]+ \verb+<hashdb> <DFXML file>+ & Scans the hashdb for hashes that match hashes in the DFXML file and unless more than max, prints out the repository name, filename, and file offset for each hash that matches\\
\hline
\textbf{scan\_expanded\_} \textbf{hash} & \verb+scan_expanded_hash+ \verb+[-m <number>] <hashdb>+ \verb+<hash value>+ & Scans the hashdb for the specified hash value and unless more than max, prints out the repository name, filename, and file offset for each hash that matches\\
\hline
\end{tabular}
\end{table}

To scan, first identify the media that you would like to scan. For this example, we download and use video file \texttt{mock\_video\_redacted\_image} available at \url{http://digitalcorpora.org/downloads/hashdb/demo}.\\

Then identify the existing hash database that will be used to search for hash value matches. We'll use the database \texttt{\textbf{mock\_video.hdb}} that we created in the previous section. That database contains all of the block hash values from a media image. \\

Finally, run the \hdb scan command to scan for blocks in the media that match block hashes in the database:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb scan_image mock_video.hdb mock_video_redacted_image > matches.json}
\end{Verbatim}
This command tells \hdb to scan \textbf{mock\_video\_redacted\_image} and try to match the values found in the local database \texttt{\textbf{mock\_video.hdb}}, putting match data in file matches.json.\\

Listing \ref{bulkHashScan} shows the output printed to the command line as a result of the above \bulk \hdb scan command. \\

\lstset{style=customfile}
\begin{lstlisting}[float, caption=Output from \bulk \hdb scan, label=bulkHashScan]
zzzzzzzzzzzzzzz
Total email features found: 0
\end{lstlisting}

All hash block matches discovered in the hash database are listed in the output.
zzzzzzzzz\\


The second column of the \texttt{identified\_blocks.txt} file shows the actual block hash value. The final column is the number of times this block hash value has been added to the hash database. It is a count of hash duplicates. Hash duplicates occur when the hash value is the same but any part of the source information including repository name, filename or offset, is unique. In this case, each hash values shown has only been added to the database once.\\

\lstset{style=customfile}
\begin{lstlisting}[float, caption={The \texttt{identified\_blocks.txt} file produced by \bulk's \textit{hashdb} scanner. First column is the forensic path, second is the hash value, and third is the number of times the hash value occurs in the database}, label=identifiedBlocks]
# BANNER FILE NOT PROVIDED (-b option)
# BULK_EXTRACTOR-Version: 1.5.5-dev ($Rev: 10844 $)
# Feature-Recorder: identified_blocks
# Filename: /home/bdallen/demo8/demo_video_redacted_image
# Feature-File-Version: 1.1
12452352	3b6b477d391f73f67c1c01e2141dbb17	{"count":1}
12456448	89a170b6b9a948d21d1d6ee1e7cdc467	{"count":1}
\end{lstlisting}

\begin{lstlisting}[float, caption={Two lines from the \texttt{identified\_sources.txt} file produced by post-processing the \texttt{identified\_blocks.txt} file. The third column is replaced with expaned information}, label=identifiedSourceLine]
12452352	3b6b477d391f73f67c1c01e2141dbb17	[{"count":1},{"sou
rce_list_id":2844319735, "sources":[{"source_id":1,"file_offset":10485760,
"repository_name":"temp1","filename":"\/home\/bdallen\/demo8\/demo_video.m
p4","filesize":10630146,"file_hashdigest":"a003483521c181d26e66dc09740e939
d"}]}]
12456448	89a170b6b9a948d21d1d6ee1e7cdc467	[{"count":1},{"sou
rce_list_id":2844319735, "sources":[{"source_id":1,"file_offset":10489856}
]}]
\end{lstlisting}

Users may be put off by the quantity of matches incurred by low-entropy data in their databases such as blocks of zeros or metadata header blocks from files that are otherwise unique. Database manipulation commands,
\textbf{\autoref{DatabaseManipulation}}, can mitigate this, for example:
\begin{itemize}
\item Use the ``subtract'' command to remove known whitelist data created from sources such as ``brand new'' operating system images and the NSRL.
\item Alternatively, use the ``deduplicate'' command to copy all hash values that have been imported exactly once.
\end{itemize}
These commands are provided to manage false positives.

\subsection{Statistics}
Various statistics are available about a given hash database including the size of a database, where its hashes were sourced from, a histogram of its hashes, and more.
Table \ref{tab:statistics} describes available statistics.\\

\begin{table}[!ht]
\centering
\caption{Commands that provide Statistics about Hash Databases}
\label{tab:statistics}
\begin{tabular}{|p{3.5 cm}|p{6 cm}|p{4 cm}|}
\hline \hline
\textbf{Command} & \textbf{Usage} & \textbf{Description} \\
\hline
\textbf{size} & \verb+size <hashdb>+ & Prints out size information relating to the database\\
\hline
\textbf{sources} & \verb+sources <hashdb>+ & Provides a top-level view of the repository names and filenames in the database. It prints out all repositories and files that have contributed to this database\\
\hline
\textbf{histogram} & \verb+histogram <hashdb>+ &  Prints a hash distribution for the hashes in the \textit{hashdb}\\
\hline
\textbf{duplicates} & \verb+duplicates <hashdb> <number>+ &  Prints out hashes in the database that are sourced the given number of times\\
\hline
\textbf{hash\_table} & \verb+hash_table <hashdb> <hex file hash>+ &  Prints hashes associated with the specified source identified by the source file hexdigest\\
\hline
\end{tabular}
\end{table}

To find sizes of various data stores in hash database \texttt{zzzz.hdb},
type the following:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb size}
\end{Verbatim}
The above command prints sizes of data stores in JSON format.

To obtain a list of hashes associated with a particular source that has hexcode \texttt{16d75027533b0a5ab900089a244384a0}, type the following:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb hash_table 16d75027533b0a5ab900089a244384a0}
\end{Verbatim}

\begin{lstlisting}[float, caption={The \texttt{identified\_hashes\_and\_sources.txt} file produced by post-processing the \texttt{identified\_blocks.txt} file using the \texttt{explain\_identified\_blocks} command}, label=explainHashesAndSources]
 hashdb-Version: 2.0.0
# explain_identified_blocks-command-Version: 3
# command_line: hashdb explain_identified_blocks temp1.hdb temp2/identifie
d_blocks.txt
# hashes
["0c5c611edc8dfd34f85c6cbf88702e51",{"count":1},[{"source_id":1,"file_offs
et":10530816}]]
["16d75027533b0a5ab900089a244384a0",{"count":1},[{"source_id":1,"file_offs
et":10547200}]]
["cf49adf3285b983d9f8d60497290bfd2",{"count":1},[{"source_id":1,"file_offs
et":10522624}]]
["f58a09656658c6b41e244b4a6091592c",{"count":1},[{"source_id":1,"file_offs
et":10493952}]]
# sources
{"source_id":1,"repository_name":"temp1","filename":"\/home\/bdallen\/demo
8\/demo_video.mp4","filesize":10630146,"file_hashdigest":"a003483521c181d2
6e66dc09740e939d"}
\end{lstlisting}

\subsection{Performance Analysis}
Performance analysis commands for analyzing \hdb performance are available, see Table \ref{tab:analysis}.

\begin{table}[!ht]
\centering
\caption{Commands that Support \hdb Performance Analysis}
\label{tab:analysis}
\begin{tabular}{|p{3.5 cm}|p{6 cm}|p{4 cm}|}
\hline \hline
\textbf{Command} & \textbf{Usage} & \textbf{Description} \\
\hline
\textbf{add\_random} & \verb+add_random+ \verb+-r [<repository name>]+ \verb+<hashdb.hdb> <count>+ & Adds count random hashes to the given database, creating timing data in the \texttt{log.xml} file\\
\hline
\textbf{scan\_random} & \verb+scan_random <hashdb.hdb>+ & Scans random hashes in the given database, creating timing data in the \texttt{log.xml} file\\
\hline
\textbf{add\_same} & \verb+add_same+ \verb+-r [<repository name>]+ \verb+<hashdb.hdb> <count>+ & Adds count same hashes to the given database, creating timing data in the \texttt{log.xml} file\\
\hline
\textbf{scan\_same} & \verb+scan_same<hashdb.hdb>+ & Scans count same hashes in the given database, creating timing data in the \texttt{log.xml} file\\
\hline
\end{tabular}
\end{table}

\section{Tools that use \hdb}
\label{OtherTools}
Several tools use \hdb.

\subsection{The \sscope \aut Plug-in}
\sscope provides an \aut plug-in for scanning for fragments of previously identified files. \aut is currently only available on Windows systems. This section describes how to set up the \sscope \aut plug-in.

\subsubsection{Installing the \sscope Plug-in}
The \sscope Windows installer installs the requisite \verb+.nbm+ Autopsy plug-in module onto the desktop. Please follow these steps to install this module:

\begin{enumerate}
\item Open \aut. From the Autopsy menu, select \verb+Tools | Plugins+.
\item Open the \verb+Downloaded+ tab and click the \verb+Add Plugins...+ button.
\item From the \verb+Add Plugins+ window, navigate to the \verb+.nbm+ module file that was installed onto the desktop, and open it.
\item Click \verb+Install+ and follow the wizard. Please note that it may be difficult to replace an old module of NPS-Autopsy-hashdb already installed in Autopsy. In the unlikely case that error \verb+Some plugins require plugin org.jdesktop.beansbinding to be installed+ appears, it may be necessary to uninstall and reinstall Autopsy.
\end{enumerate}

\subsubsection{Configuring the \sscope Plug-in}
The path to the \hdb database to scan against must be configured:

\begin{enumerate}
\item Start a new case, \verb+File | New Case...+, fill in the Case Information fields, and click \verb+Next+.
\item Fill in Case Information and click \verb+Finish+.
\item For Add Data Source (1 of 3), put in a media image for Autopsy to process and click \verb+Next+.
\item For Add Data Source (2 of 3), select checkboxes as desired, then click on \verb+NPS-SectorScope+ text to configure the path to your \hdb database to scan against. Currently a file chooser is not available, so please type in the full path, for example: \verb+C:\Users\me\my_hashdb.hdb+. Click Next.
\item For Add Data Source (3 of 3) click \verb+Finish+. When the NPS-SectorScope module begins processing, Autopsy will display "NPS-SectorScope ..." as \bulk runs, which may take up to several hours. Unfortunately, \bulk progress is not currently indicated. For diagnostics: please see if progress text is appearing in the generated \verb+bulk_extractor\report.xml+ file and in the generated log file or try running the scan manually.
\end{enumerate}

\subsection{\sscope}
The \sscope tool provides a GUI for analyzing data associated with block hash matches found on a media image. An example screenshot is shown in Figure \ref{fig:SectorScope_main}. \sscope also provides GUI interfaces for building and scanning \hdb databases. Please see \url{https://github.com/NPS-DEEP/NPS-SectorScope/wiki} for more information on \sscope.

\begin{figure}
	\center
	\includegraphics[scale=.45]{drawings/SectorScope_main}
	\caption{Overview of the \hdb system}
	\label{fig:SectorScope_main}
\end{figure}

\subsection{\bulk}
\bulk is an open source digital forensics tool that extracts features such as email addresses, credit card numbers, URLs and other types of information from digital evidence files. It operates on disk images, files or a directory of files and extracts useful information without parsing the file system or file system structures.  For more information on how to use \bulk for a wide variety of applications, refer to the separate publication \textit{The \bulk Users Manual} available at \url{http://digitalcorpora.org/downloads/bulk_extractor/BEUsersManual.pdf} \cite{beusersguide}.\\

In particular, a \hdb \bulk scanner is available which may be used to import block hashes into a new hash database and to scan for hashes against an existing hash database.
Currently, \hdb requires a newer build of \bulk than is available on the \bulk site.
Please see the \hdb Wiki page at \url{https://github.com/NPS-DEEP/hashdb/wiki}
for information on obtaining a version of \bulk that is compatible with the current version of \hdb.\\

Options that control the hashdb
scanner are provided to \bulk using "-S name=value" parameters.
Example syntax for the \bulk \hdb scanner is shown in Table \ref{tab:hashdbScanner}.
Scanner options are described in Table \ref{tab:hashdb_be_usage} and shown in Listing \ref{BEUsage}.

\begin{table}[!ht]
\centering
\caption{\bulk \hdb Scanner Commands}
\label{tab:hashdbScanner}
\begin{tabular}{|p{3.5 cm}|p{6 cm}|p{4 cm}|}
\hline \hline
\textbf{Mode} & \textbf{Example} & \textbf{Description} \\
\hline
\textbf{import files} & \verb+bulk_extractor -E hashdb+ \verb+-S hashdb_mode=import+ \verb+-o outdir1 -R my_directory+ & Import hashes from directory into outdir1/hashdb.hdb\\
\hline
\textbf{import image} & \verb+bulk_extractor -E hashdb+ \verb+-S hashdb_mode=import+ \verb+-o outdir1 my_image1+ & Import hashes from image into outdir1/hashdb.hdb\\
\hline
\textbf{scan image} & \verb+bulk_extractor -E hashdb+ \verb+-S hashdb\_mode=scan+ \verb+-S hashdb_scan_path+ \verb+=outdir1/hashdb.hdb -o outdir2+ \verb+my_image+ & Scan image for hashes matching hashes in outdir1/hashdb.hdb\\
\hline
\end{tabular}
\end{table}

\begin{table}[!ht]

\centering
\caption{\bulk \hdb Scanner Options}
\label{tab:hashdb_be_usage}
%\begin{tabular}{|p{3.5 cm}|p{6 cm}|p{4 cm}|}
\begin{tabular}{|p{5 cm}|p{2.0 cm}|p{6.5 cm}|}
\hline \hline
\textbf{Option} & \textbf{Default} & \textbf{Specification} \\
\hline
\verb+hashdb_mode+ & \verb+none+ & The mode for the scanner, one of \verb+[none|import|scan]+. For ``none'', the scanner is active but performs no action. For ``import'', the scanner imports block hashes. For ``scan'', the scanner scans for matching block hashes.\\
\hline
\verb+hashdb_block_size+ &512 & Block size, in bytes, used to generate hashes.\\
\hline
\verb+hashdb_sector_size+ &512 & Sector size, in bytes.  Scans and imports along sector boundaries.\\
\hline
\verb+hashdb_scan_path+ & & The file path to a hash database to scan against.  Valid only in scan mode. No default provided. Value must be specified if in scan mode.\\
\hline
\verb+hashdb_import_repository_+ \verb+name+ & \verb+default_+ \verb+repository+ &Selects the repository name to attribute the import to.  Valid only in import mode.\\
\hline
\verb+hashdb_max_feature_file_+ \verb+lines+ & 0 &The maximum number of feature lines to record or 0 for no limit.  Valid only in scan mode.\\
\hline
\end{tabular}
\end{table}

\lstset{basicstyle=\footnotesize}
\lstset{breaklines=true}
\lstset{breakatwhitespace=true}
\lstinputlisting[float, caption={\bulk options}, label=BEUsage, style=customfile]{hashdb_be_usage.txt}

\section{\hdb Data Types}
\label{DataTypes}
Many of the \hdb commands and API interfaces require or emit data
in JSON format.  This section describes the syntax used and required by \hdb
commands and API interfaces.

\subsection{Source Data}
\hdb import and export commands and interfaces accept and emit source data in JSON format.  Source data defines information about a source. Source data is identified by the file hash of the source.  An example source data line is shown in Listing \ref{JSONSourceData}. Fields are described in Table \ref{tab:JSONSourceData}.\\

\lstset{style=customfile}
\begin{lstlisting}[float, caption={Example JSON source data}, label=JSONSourceData]
{
  "file_hash":"b9e7...",
  "filesize":8000,
  "file_type":"exe",
  "nonprobative_count":4,
  "name_pairs":["repository1","filename1","repo2","f2"]
}
\end{lstlisting}

\begin{table}[!ht]

\centering
\caption{Fields used in JSON source data}
\label{tab:JSONSourceData}
\begin{tabular}{|p{5 cm}|p{8.8 cm}|}
\hline \hline
\textbf{Field} & \textbf{Meaning} \\
\hline
\verb+file_hash+ & The hexdigest of the source file containing the block hash\\
\hline
\verb+filesize+ & The size, in bytes, of the source file\\
\hline
\verb+file_type+ & A classification of what type of file the source file is\\
\hline
\verb+nonprobative_count+ & The number of blocks in the source that are considered to be nonprobative\\
\hline
\verb+name_pairs+ & An array of source name, filename pairs associated with this source\\
\hline
\end{tabular}
\end{table}

\subsection{Block Hash Data}
\hdb import and export commands and interfaces also accept and emit block hash data in JSON format.  Block hash data defines information about a block hash. Block hash data is identified by the file block hash.  An example block hash line is shown in Listing \ref{JSONBlockHashData}. Fields are described in Table \ref{tab:JSONBlockHashData}.\\

\lstset{style=customfile}
\begin{lstlisting}[float, caption={Example JSON block hash data}, label=JSONBlockHashData]
{
  "block_hash":"a7df...",
  "entropy":8,
  "block_label":"W",
  "source_offset_pairs":["b9e7...", 4096]
}
\end{lstlisting}

\begin{table}[!ht]

\centering
\caption{Fields used in JSON block hash data}
\label{tab:JSONBlockHashData}
\begin{tabular}{|p{5 cm}|p{8.8 cm}|}
\hline \hline
\textbf{Field} & \textbf{Meaning} \\
\hline
\verb+block_hash+ & A block hash hexdigest\\
\hline
\verb+entropy+ & The calculated entropy, scaled between 0 and 100, using an entropy calculation TO BE DETERMINEDzzzzzzz\\
\hline
\verb+block_label+ & A label describing the type of data within the block. The block label may include information that it matched a whitelist database during import. The entropy and block label fields may be used together to estimate that a block might be nonprobative\\
\hline
\verb+source_offset_pairs+ & An array of source file hash, source file offset pairs associated with this block hash\\
\hline
\end{tabular}
\end{table}

\subsection{Scan Data}
The \hdb scan command and scan API emits hash and source information relating to hashes matched while scanning. An example scan match is shown in Listing \ref{JSONScanData}. Fields are described in Table \ref{tab:JSONScanData}.\\

\lstset{style=customfile}
\begin{lstlisting}[float, caption={Example JSON block hash data}, label=JSONBlockHashData]
{
  "entropy": 8,
  "block_label": "W",
  "source_list_id": 57,
  "sources": [{
    "file_hash": "f7035a...",
    "filesize": 800,
    "file_type": "exe",
    "nonprobative_count": 2,
    "name_pairs": ["repository1", "filename1", "repo2", "f2"]
  }],
  "source_offset_pairs": ["f7035a...", 0, "f7035a...", 512]
}
\end{lstlisting}

\begin{table}[!ht]

\centering
\caption{Fields used in JSON scan data}
\label{tab:JSONScanData}
\begin{tabular}{|p{5 cm}|p{8.8 cm}|}
\hline \hline
\textbf{Field} & \textbf{Meaning} \\
\hline
\verb+entropy+ & The calculated entropy integer, scale and algorithm TBD.zzzz\\
\hline
\verb+block_label+ & A label describing the type of data within the block\\
\hline
\verb+source_list_id+ & A source list ID which is a CRC of the associated source file hashes associated with the block hash\\
\hline
\verb+sources+ & An array of source data for each source matching the hash\\
\hline
\verb+file_hash+ & The hexdigest of the source file containing the block hash\\
\hline
\verb+filesize+ & The size, in bytes, of the source file\\
\hline
\verb+file_type+ & A classification of what type of file the source file is\\
\hline
\verb+nonprobative_count+ & The number of blocks in the source that are considered to be nonprobative\\
\hline
\verb+name_pairs+ & An array of repository name, filename pairs associated with this source\\
\hline
\verb+source_offset_pairs+ & An array of the source file hash, source file offset pairs associated with this block hash\\
\hline
\end{tabular}
\end{table}

\subsection{Size Data}
The \hdb size command and size API returns size information about internal data structures in JSON format. The size of the \verb+source_id_store+ indicates the number of sources. The size of the \verb+hash_store+ is greater than or equal to the number of hashes stored, and is not exact because of how data is stored. Although for internal use, these fields can give some sense of the size of a \hdb database. An example output is shown in Listing \ref{JSONSize}.\\

\lstset{style=customfile}
\begin{lstlisting}[float, caption={Example JSON output of database size values}, label=JSONSize]
{
zzzz
  "hash_data_store":0,
  "hash_store":0,
  "source_data_store":0,
  "source_id_store":0,
  "source_name_store":0
}
\end{lstlisting}

\subsection{Timing Data}

\hdb provides timing data in JSON format for use with timing analysis. Python scripts may use this output to produce performance plots. An example timestamp entry is shown in Listing \ref{JSONTimingData}. Fields are described in Table \ref{tab:JSONTimingData}.\\

\lstset{style=customfile}
\begin{lstlisting}[float, caption={Example JSON timestamp format}, label=JSONTimingData]
{
  "name":"begin",
  "delta":"0.000396",
  "total":"0.000396"
}
\end{lstlisting}

\begin{table}[!ht]

\centering
\caption{Fields used in JSON timing data}
\label{tab:JSONTimingData}
\begin{tabular}{|p{5 cm}|p{8.8 cm}|}
\hline \hline
\textbf{Field} & \textbf{Meaning} \\
\hline
\verb+name+ & The name of the timestamp\\
\hline
\verb+delta+ & The delta time since the previous timestamp. In this example, the delta is from the time the timestamping started\\
\hline
\verb+total+ & The total time since timestamping started\\
\hline
\end{tabular}
\end{table}

\section{\hdb Configuration Settings}
\label{Settings}
Function and performance of a hashdb database is defined using hashdb configuration settings. These are the settings:

\textbf{Block size}\\
The size of data blocks the database expects to store. Block hashes are calculated from data of this size. The default is 512.\\

\textbf{Sector size}\\
The resolution of sectors that may reside on media. The operating system and/or storage device will align data on sector boundaries. Block hashes are calculated on aligned sector boundaries. The default is 512.\\

\textbf{Max ID offset pairs}\\
The maximum number of source, offset pairs to store for a given hash, default 100,000. Each pair identifies a location in a source where this hash is located. If source information is not interesting when there are many sources, offset pairs associated with a hash, use a low value to prevent storing extra entries.\\

\textbf{Hash prefix bits}\\
The number of bits of the hash prefix to use as the key in the store. The idea is to select a value given the size of the database so that the average number of hashes with this prefix is slightly greater than 0, for example 20. For example if you expect 5 billion hashes, you might select 28 because $5/2^{28}=18.6$, which is near 20.\\

\textbf{Hash suffix bytes}\\
The number of bytes of the hash suffix to store in the set of values of hash suffixes for this hash key. The idea is to store as few bytes as possible while minimizing false positives. For example if you expect 5 billion hashes, you might select 28 prefix bits and 3 suffix bytes because $5 / 2^{((28) + 8*(3))} = 5 / 2^{52} = 0.00011\%$ false positive rate, about 1 in 1 million.\\

\section{Using the \hdb Library APIs}
\label{APIs}
\hdb provides C++ and Python interfaces for importing, scanning, and working with block hashes.

\subsection{Python}
Install Python module \verb+hashdb+ to use the \hdb Python interfaces.

\subsubsection{Import}
To import hash and source data, open an import manager, for example\\
\verb+{manager = import_manager_t("hashdb.hdb", "create my DB")+. Then use import functions to add data.

\begin{itemize}
\item \verb+manager = import_manager_t(hashdb_dir, cmd)+
\item \verb+manager.insert_source_name(file_binary_hash, repository_name, filename)+
\item \verb+manager.insert_source_data(file_binary_hash, filesize, file_type,+\\
\verb+nonprobative_count)+
\item \verb+manager.insert_hash(binary_hash, file_binary_hash, file_offset,+\\
\verb+entropy, block_label)+
\item \verb+error_message = manager.insert_json(json_string)+\\
Insert hash or source. Return "" on success.
\item \verb+ data_sizes = manager.sizes()+
\end{itemize}

\subsubsection{Scan}
To scan for hashes, open a scan manager, for example\\
\verb+manager = scan_manager_t("hashdb.hdb")+. Then use functions to find hash and source information. Functions that return less information run faster than functions that return more.

\begin{itemize}
\item \verb+scan_manager = scan_manager_t(hashdb_dir)+
\item \verb+expanded_text = scan_manager.find_expanded_hash(binary_hash)+\\
Find and return JSON text about the hash and its sources. Hash and source information is not returned twice. For example on a second hash match, the returned JSON will just be {}.
\item \verb+json_hash_string = scan_manager.find_hash_json(binary_hash)+
\item \verb+approximate_count = scan_manager.find_approximate_hash_count(binary_hash)+\\
This is the fastest scan function. It returns an approximate source, offset pair count for the hash and can be wrong.
\item \verb+count = scan_manager.find_hash_count(binary_hash)+\\
Return the source, offset pair count for the hash.
\item \verb+json_source_string = scan_manager.find_source_json(file_binary_hash)+
\item \verb+first_binary_hash = scan_manager.first_hash()+
\item \verb+next_binary_hash = scan_manager.next_hash(binary_hash)+
\item \verb+first_file_binary_hash = scan_manager.first_source()+
\item \verb+file_binary_hash = scan_manager.next_source(file_binary_hash)+
\item \verb+db_sizes = scan_manager.sizes()+\\
Return sizes of internal data stores in JSON format.
\item \verb+size_hashes = scan_manager.size_hashes()+\\
Return the number of hash records in the database, which is typically more than the number of source, offset pairs in the database.
\item \verb+size_sources = scan_manager.size_sources()+\\
Return the number of sources in the database.
\end{itemize}

\subsubsection{Settings}
Holds \hdb settings.

\begin{itemize}
\item \verb+settings = settings_t()+\\
Obtain default settings. Configurable setting parameters are: \verb+settings_version,+\\
\verb+sector_size, block_size, max_source_offset_pairs, hash_prefix_bits,+\\
\verb+hash_suffix_bytes+.
\item \verb+settings_string = settings.settings_string()+\\
Return setting values in JSON format.
\end{itemize}

\subsubsection{Timestamp}
Provide timestamp support.

\begin{itemize}
\item \verb+timestamp = timestamp_t()+\\
Create a timestamp object.
\item \verb+timestamp_string = stamp(text)+\\
Create a named timestamp and provide time and delta from the last stamp time in JSON format.
\end{itemize}

\subsubsection{Support Functions}
Support functions provide miscellaneous support and are not part of a class.
\begin{itemize}
\item \verb+version = version()+\\
Return the \hdb version.
\item \verb+error_message = create_hashdb(hashdb_dir, settings)+\\
Create a hash database given settings. Return "" else reason for failure.
\item \verb+error_message = read_settings(hashdb_dir, &settings)+\\
Query settings else false and reason for failure.
\item \verb+binary_string = hex_to_bin(hex_string)+
\item \verb+hex_string = bin_to_hex(binary_string)+
\item \verb+import_recursive_dir(hashdb_dir, whitelist_dir, min_entropy,+\\
\verb+max_entropy, repository_name, import_path)+\\
Import from path to \hdb. zzz NOT AVAILABLE IN ALPHA.
\end{itemize}

\subsection{C++}
To use C++ interfaces, include interface file \verb+hashdb.hpp+ and link \hdb library \verb+libhashdb+. \hdb interfaces use the \hdb namespace. Interfaces can assert on unexpected error. For more detail, please see \hdb header file \texttt{hashdb.hpp} at \url{https://github.com/NPS-DEEP/hashdb/blob/new_hashdb_3.0/src_libhashdb/hashdb.hpp}.

\subsubsection{Import}
Import hashes. Interfaces use lock for DB safety. The destructor appends changes to change log.

\begin{itemize}
\item \verb+import_manager_t(hashdb_dir, cmd)+
\item \verb+void insert_source_name(file_binary_hash, repository_name, filename)+
\item \verb+void insert_source_data(file_binary_hash, filesize, file_type, nonprobative_count)+\\
Change if different.
\item \verb+void insert_hash(binary_hash, file_binary_hash, file_offset, entropy, block_label)+\\
Change if different
\item \verb+std::string insert_json(json_string)+\\
Insert hash or source, return \verb+error_message+ or "" for no error.
\item \verb+std::string sizes()+\\
Return number of entries in LMDB databases.
\item \verb+~import_manager_t()+\\
Append changes to change log at \verb+hashdb_dir/log.dat+.
\end{itemize}

\subsubsection{Scan}
Scan for hashes. Provide read-only access to hash and source data stores.

\begin{itemize}
\item \verb+scan_manager_t(hashdb_dir)+
\item \verb+std::string find_expanded_hash(binary_hash)+\\
Find and return JSON text else "" on no match.
\item \verb+bool find_hash(binary_hash, &entropy, &block_label, &source_offset_pairs_t)+\\
\item \verb+std::string find_hash_json(binary_hash)+\\
Return \verb+json_hash_string+ else "" on no match.
\item \verb+size_t find_approximate_hash_count(binary_hash)+\\
Return approximate source, offset pair count for the hash, and can be wrong.
\item \verb+size_t find_hash_count(binary_hash)+\\
Return the source, offset pair count for the hash.
\item \verb+bool find_source_data(file_binary_hash, &filesize, &file_type,+\\
\verb+&nonprobative_count)+
False on no source ID.
\item \verb+bool find_source_names(file_binary_hash, &source_names_t)+\\
False on no source ID.
\item \verb+std::string find_source_json(file_binary_hash)+\\
Return \verb+json_source_string+ else "" on no match.
\item \verb+std::string first_hash()+
\item \verb+std::string next_hash(binary_hash)+
\item \verb+std::string first_source()+
\item \verb+std::string next_source(file_binary_hash)+
\item \verb+string sizes()+\\
Return number of entries in LMDB databases.
\item \verb+size_t size_hashes()+\\
Return number of hash records in the database, which is typically more than the number of source, offset pairs in the database.
\item \verb+size_t size_sources()+\\
Return number of sources.
\end{itemize}

\subsubsection{Settings}
Hold \hdb settings.

\begin{itemize}
\item \verb+settings_t()+\\
Create a settings object with default values.
\item \verb+std::string settings_string()+\\
Return setting values in JSON format.
\end{itemize}

\subsubsection{Timestamp}
Provide timestamp support.

\begin{itemize}
\item \verb+timestamp_t()+
\item \verb+std::string stamp(text)+\\
Return stamp in JSON format.
\end{itemize}

\subsubsection{Support Functions}

\begin{itemize}
\item \verb+extern C char* version()+\\
Return the \hdb version.
\item \verb+bool create_hashdb(hashdb_dir, settings, &error_message)+\\
Create a hash database given settings. Return true and "" else false and reason for failure.
\item \verb+bool read_settings(hashdb_dir, &settings, &error_message)+\\
Query settings else false and reason for failure.
\item \verb+print_enviornment(command_line_string, &ostream)+\\
Print command and environment to output stream. - zzzz this interface may be removed.
\item \verb+import_recursive_dir(hashdb_dir, whitelist_dir, min_entropy,+\\
\verb+max_entropy, repository_name, import_path)+\\
Import from path to \hdb.
\end{itemize}

\subsubsection{Data Types}

The following data types are defined.

\begin{itemize}
\item \verb+source_offset_pair_t: typedef pair<file_binary_hash, file_offset> id_offset_pair_t+
\item \verb+source_offset_pairs_t: typedef set<source_offset_pair_t> source_offset_pairs_t+
\item \verb+source_name_t: typedef pair<repository_name, fillename> source_name_t+
\item \verb+source_names_t: typedef set<source_name_t> source_names_t+
\end{itemize}






\section{Use Cases for \hdb}
\label{UseCases}
There are many different ways to utilize the functionality provided by the \hdb tool. In this section, we highlight some of the most common uses of the system.

\subsection{Querying for Source or Database Information}
 Users can scan a hash database directly using various querying commands. Those commands are outlined in Table \ref{tab:scanServices}.  The ``scan'' command allows users to search for hash blocks.\\

\subsection{Writing Software that works with \hdb}
\label{APISection}
\hdb provides Python and C++ APIs that can manage all aspects of a hash database
including importing and scanning [see \textbf{\Autoref{APIs}} for information on using these APIs].
Other software programs can use these APIs to access database capabilities. The file \texttt{hashdb.hpp} found in the \textit{src} directory contains the complete specification of the API. That complete file is also contained in Appendix \ref{hashdbapi} of this document.  The two key features provided by the API include the ability to import values into a hash database and the ability to scan media for any values matching those in a given hash database.  The \bulk program uses the \hdb API to implement both of these capabilities.\\

\subsection{Scanning or Importing to a Database Using \bulk}
\label{bulkextractorSection}
The \bulk \textit{hashdb} scanner allows users to query for fragments of previously encountered hash values and populate a hash database with hash values. Options that control the \textit{hashdb} scanner are provided to \bulk using the ``-S name=value'' command line parameters. When \bulk executes, the parameters are sent directly to the scanner.\\

For example, the following command runs the \bulk \textit{hashdb} scanner in import mode and adds hash values calculated from the disk image \texttt{my\_image} to a hash database:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{bulk_extractor -e hashdb -o outputDir -S hashdb_mode=import my_image}
\end{Verbatim}
Note, \bulk will place feature file and other output not relevant to the \hdb application in the ``outputDir'' directory. When using the import command, the output directory will contain a newly created hash database called \texttt{hashdb.hdb}. That database can then be copied or added to a hash database in another location.


\subsection{Updating Hash Databases}
\label{updateSection}
\hdb provides users with the ability to manipulate the contents of hash databases. The specific command line options for performing these functions are described in Table \ref{tab:databaseManipulation}. \hdb databases are treated as sets with the add, subtract and intersect commands basically using add, subtract and intersect set operations. For example, the following command will  copy all non-duplicate values from \texttt{mock\_video.hdb} into \texttt{mock\_video\_dedup.hdb} :
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb deduplicate mock_video.hdb mock_video_dedup.hdb}
\end{Verbatim}
Whenever a database is created or updated, \hdb updates the file \texttt{log.xml}, found in the database's directory with information about the actions performed.\\

After each command to change a database, statistics about the changes are writen in the \texttt{log.xml} file and to \texttt{stdout}. Table \ref{tab:changeStatistics} shows all of the statistics tracked in the log file along with their meaning. The value of each statistic is the number of times the event happened during the command. For example, if 280 hashes are removed, the statistic ``hashes\_removed'' will be marked with a value of 280. \\

\subsubsection{Update Commands and ``Duplicate'' Hashes}
Commands that add or import hashes of the same value will result in hash duplicates if the source information is unique. If hash and source values are identical (including repository name), no hash values are added with the \textbf{add} or \textbf{import} commands. The \textbf{intersect} and \textbf{subtract} commands do not require source information to match. An intersection occurs when hatches match, regardless of whether the source information matches. Similarly, hash values are also subtracted from the database regardless of whether or not their source information matches.  The update statistics specified in the log file (shown in \ref{tab:changeStatistics}) will specify the results of each of these commands to help users track changes. \\

As discussed previously, users can only specify the repository name with the \textbf{import} command. As databases become large, the repository name for each hash value will help identify important source information. Users should plan on importing data with specific repository names whenever possible to avoid source confusion later.\\

Finally, we provide two philosophies for mitigating duplicate hash bloat:
\begin{itemize}
 \item If you know you have imported the same blacklist data twice,
  and you do not want to manage a 'whitelist' database,
  \texttt{deduplicate} is a quick and easy way to get rid of low-entropy noise.
  \item If your database has blacklist data from more than one source
  or you just want tighter control about what you want to remove
  and are willing to use a 'whitelist' database
  to remove hashes to improve lookup speed
  or to reduce noise about uninteresting hashes found,
  use \texttt{subtract}.
\end{itemize}







\subsection{Optimizing a Hash Database}
\label{optimizing}
 
For large databases, it takes a bit of time to look up a hash value to determine whether it is in the database. This time adds up when scanning for millions of hash values. As an optimization, \hdb provides tuning parameters for tuning the initial hash lookup when checking to see if a hash value is present in the database [see \textbf{\Autoref{Creating}} for information on using these tuning parameters].

\subsection{Exporting Hash Databases}
Users can export hashes from a hash database to a JSON export file using the ``export'' command [see \textbf{\Autoref{DataTypes}} for information on JSON syntax].  For example, the following command will export the \texttt{mock\_video.hdb} database to the file \texttt{demoVideoHashes.json}:
\begin{Verbatim}[commandchars=\\\{\}]
\verbbf{hashdb export mock_video.hdb demoVideoHashes.xml}
\end{Verbatim}



\section{Troubleshooting}
\label{DebuggingHashdb}
All \hdb users should join the \bulk users Google group for more information and help with any issues encountered. To join, send an email to \textbf{bulk\_extractor-users+subscribe@} \textbf{googlegroups.com}.  \\

\section{Related Reading}
There are other articles related to block hashing, and its practical and research applications. Some of those articles are specifically cited throughout this manual. Other useful references include but are not limited to:
\begin{itemize}
\item Michael McCarrin, Bruce Allen. Rapid Recognition of Blacklisted Files and Fragments. Naval Postgraduate School. \url{http://www.osdfcon.org/presentations/2015/McCarrin-Allen_osdfcon.pdf}.
\item Jim Jones, Tahir Khan, Kathryn Laskey, Alex Nelson, Mary Laamanen, Doug White.  Inferring Past Activity from Partial Digital Artifacts. George Mason University, National Institute of Standards and Technology. \url{http://www.osdfcon.org/presentations/2015/Jim-Jones_EtAl-Release.pdf}.
\item Simson Garfinkel, Michael McCarrin. Hash-based Carving: Searching media for complete files and file fragments with sector hashing and hashdb. DFRWS 2015 USA. \url{http://www.sciencedirect.com/science/article/pii/S1742287615000468}
\item Joel Young, Kristina Foster, Simson Garfinkel, Kevin Fairbanks. Distinct Sector Hashes for Target File Detection. \url{http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&arnumber=6311397}.
\item Garfinkel, Simson, Alex Nelson, Douglas White and Vassil Rousseve. Using purpose-built functions and block hashes to enable small block and sub-file forensics. Digital Investigation. Volume 7. 2010. Page S13--S23. \url{http://www.dfrws.org/2010/proceedings/2010-302.pdf}.
\item Foster, Kristina. Using Distinct Sectors in Media Sampling and Full Media Analysis to Detect Presence of Documents From a Corpus. Naval Postgraduate School Masters Thesis, September 2012. \url{http://calhoun.nps.edu/public/handle/10945/17365}.
\end{itemize}

\bibliographystyle{acm} 
\bibliography{references}

\newpage
\appendix
\appendixpage

\section{\hdb Quick Reference}
\label{QuickReference}
\input ../hashdb_quick_reference/hashdb_quick_reference_text.tex
\newpage

\section{Output of \hdb Help Command}
\label{HelpOutput}
\begingroup
\footnotesize
{
\fontfamily{courier}\selectfont
\verbatiminput{hashdb_usage.txt}
}
\endgroup


\section{\hdb C++ API: \texttt{hashdb.hpp}}
\label{hashdbapi}
\lstset{language=C++}
\lstset{basicstyle=\footnotesize}
\lstset{breaklines=true}
\lstset{breakatwhitespace=true}
\lstinputlisting{../../src_libhashdb/hashdb.hpp}


%\section{\bulk \textit{hashdb} Scanner Usage Options}
%\label{scannerOptionsAppendix}
%The \bulk hashdb scanner provides two capabilities: 1) scanning
%a hash database for fragments of previously encountered hash values,
%and 2) importing block hashes into a new hash database.
%Options that control the hashdb
%scanner are provided to \bulk using "-S name=value" parameters
%when \bulk is invoked.  Available options are: 
%
%\begingroup
%\footnotesize
%{
%\fontfamily{courier}\selectfont
%\verbatiminput{hashdb_be_usage.txt}
%}
%\endgroup

\end{document}
