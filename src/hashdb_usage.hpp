// Author:  Bruce Allen <bdallen@nps.edu>
// Created: 2/25/2013
//
// The software provided here is released by the Naval Postgraduate
// School, an agency of the U.S. Department of Navy.  The software
// bears no warranty, either expressed or implied. NPS does not assume
// legal liability nor responsibility for a User's use of the software
// or the results of such use.
//
// Please note that within the United States, copyright protection,
// under Section 105 of the United States Code, Title 17, is not
// available for any work of the United States Government and/or for
// any works created by United States Government employees. User
// acknowledges that this software contains work which was created by
// NPS government employees and is therefore in the public domain and
// not subject to copyright.
//
// Released into the public domain on February 25, 2013 by Bruce Allen.

/**
 * \file
 * Provides usage and detailed usage for the hashdb tool.
 */

#ifndef HASHDB_USAGE_HPP
#define HASHDB_USAGE_HPP

#include <config.h>

// Standard includes
#include <cstdlib>
#include <cstdio>
#include <string>
#include <sstream>
#include <iostream>
#include <algorithm>
#include <vector>
#include <cassert>
#include <boost/lexical_cast.hpp>
#include <getopt.h>
#include "hashdb_settings.hpp"
#include "hashdb_runtime_options.hpp"

// approximate bloom conversions for k=3 and p false positive = ~ 1.1% to 6.4%
uint64_t approximate_M_to_n(uint32_t M) {
  uint64_t m = (uint64_t)1<<M;
  uint64_t n = m * 0.17;
//std::cout << "Bloom filter conversion: for M=" << M << " use n=" << n << "\n";
  return n;
}

// approximate bloom conversions for k=3 and p false positive = ~ 1.1% to 6.4%
uint32_t approximate_n_to_M(uint64_t n) {
  uint64_t m = n / 0.17;
  uint32_t M = 1;
  // fix with actual math formula, but this works
  while ((m = m/2) > 0) {
    M++;
  }
//std::cout << "Bloom filter conversion: for n=" << n << " use M=" << M << "\n";
  return M;
}

void usage() {
  hashdb_settings_t s;
  hashdb_runtime_options_t o;

  // print usage
  std::cout
  << "hashdb Version " << PACKAGE_VERSION  << "\n"
  << "Usage: hashdb -h | -H | -V | <command>\n"
  << "    -h, --help    print this message\n"
  << "    -H            print detailed help including usage notes and examples\n"
  << "    --Version     print version number\n"
  << "\n"
  << "The hashdb tool supports the following <command> options:\n"
  << "\n"
  << "create [<hashdb settings>]+ [<bloom filter settings>]+ <hashdb>\n"
  << "    Create a new hash database.\n"
  << "\n"
  << "    Options:\n"
  << "    Please see <hashdb settings> and <bloom filter settings> for settings\n"
  << "    and default values.\n"
  << "\n"
  << "    Parameters:\n"
  << "    <hashdb>   the file path to the new hash database to create\n"
  << "\n"
  << "import [-r <repository name>] <DFXML file> <hashdb>\n"
  << "    Import hashes from file <DFXML file> into hash database <hashdb>.\n"
  << "\n"
  << "    Options:\n"
  << "    -r, --repository=<repository name>\n"
  << "        The repository name to use for the set of hashes being imported.\n"
  << "        (default is \"repository_\" followed by the <DFXML file> path).\n"
  << "\n"
  << "    Parameters:\n"
  << "    <DFXML file>   the hash database to import hashes from\n"
  << "    <hashdb>       the hash database to insert the imported hashes into\n"
  << "\n"
  << "export <hashdb> <DFXML file>\n"
  << "    Export hashes from the <hashdb> to a <DFXML file>.\n"
  << "\n"
  << "    Parameters:\n"
  << "    <hashdb>       the hash database whose hash values are to be exported\n"
  << "    <DFXML file>   the DFXML file of exported hash values\n"
  << "\n"
  << "copy <source hashdb> <destination hashdb>\n"
  << "    Copies hashes from the <source hashdb> to the <destination hashdb>.\n"
  << "\n"
  << "    Parameters:\n"
  << "    <source hashdb>       the hash database to copy hashes from\n"
  << "    <destination hashdb>  the hash database to copy hashes to\n"
  << "\n"
  << "merge <source hashdb 1> <source hashdb 2> <destination hashdb>\n"
  << "    Copies hashes from <source hashdb 1> and <source hashdb 2> into\n"
  << "    the <destination hashdb>.  The merge command can be faster than the\n"
  << "    copy command.\n"
  << "\n"
  << "    Parameters:\n"
  << "    <source hashdb 1>     a hash database to copy hashes from\n"
  << "    <source hashdb 2>     a hash database to copy hashes from\n"
  << "    <destination hashdb>  the hash database to copy hashes into\n"
  << "\n"
  << "remove <hashdb 1> <hashdb 2>\n"
  << "    Remove hashes in <hashdb 1> from <hashdb 2>, leaving <hashdb 1> unchanged.\n"
  << "\n"
  << "    Parameters:\n"
  << "    <hashdb 1>     the input hash database containing hashes to be removed\n"
  << "    <hashdb 2>     the hash database to remove matching hashes from\n"
  << "\n"
  << "remove_dfxml <DFXML file> <hashdb>\n"
  << "    Remove all hashes in the <DFXML file> from the <hashdb>, if present.\n"
  << "\n"
  << "    Parameters:\n"
  << "    <DFXML file>   the DFXML file containing hashes to be removed\n"
  << "    <hashdb>       the hash database to remove matching hashes from\n"
  << "\n"
  << "deduplicate <hashdb>\n"
  << "    Remove all hashes from <hashdb> that appear in the database more than\n"
  << "    once.\n"
  << "\n"
  << "    Parameters:\n"
  << "    <hashdb>       the hash database to entirely remove duplicated hashes\n"
  << "                   from\n"
  << "\n"
  << "rebuild_bloom [<bloom filter settings>]+ <hashdb>\n"
  << "    Rebuilds the bloom filters in the <hashdb> hash database based on the\n"
  << "    <bloom filter settings> provided.\n"
  << "\n"
  << "    Options:\n"
  << "    <bloom filter settings>\n"
  << "        Please see <bloom filter settings> for settings and default values.\n"
  << "\n"
  << "    Parameters:\n"
  << "    <hashdb>       a hash database for which the bloom filters will be rebuilt\n"
  << "\n"
  << "server <hashdb> <socket>\n"
  << "    Starts a query server service for <hashdb> at <socket> for\n"
  << "    servicing hashdb queries.\n"
  << "\n"
  << "    Parameters:\n"
  << "    <hashdb>       the hash database that the server service will use\n"
  << "    <socket>       the TCP socket to make available for clients,\n"
  << "                   for example 'tcp://localhost:14500'.\n"
  << "\n"
  << "query_hash <path or socket> <DFXML file>\n"
  << "    Queries the hashdb service provided at the specified <path or socket>\n"
  << "    for hash values in the <DFXML file> and prints out matches.\n"
  << "\n"
  << "    Parameters:\n"
  << "    <path or socket>   the path or socket providing the lookup service\n"
  << "                       Please see <path or socket> for more information\n"
  << "\n"
  << "get_hash_source <path or socket> <identified_blocks.txt> <output text file>\n"
  << "    Use the lookup service at <path or socket> to expand the\n"
  << "    <identified_blocks.txt> file generated by bulk_extractor into\n"
  << "    <output text file> to indicate where the hash values are sourced from.\n"
  << "\n"
  << "    Parameters:\n"
  << "    <identified_blocks.txt>   the input text file containing hashes whose\n"
  << "                              sources will be obtained.\n"
  << "    <output text file>        the output text file containing hashes\n"
  << "                              and where they are sourced from\n"
  << "    <path or socket>   the path or socket providing the lookup service\n"
  << "                       Please see <path or socket> for more information\n"
  << "\n"
  << "get_hashdb_info <path or socket>\n"
  << "    Print out information about the hashdb service.\n"
  << "\n"
  << "    Parameters:\n"
  << "    <path or socket>   the path or socket providing the lookup service\n"
  << "                       Please see <path or socket> for more information\n"
  << "\n"
  << "<hashdb settings> establish the settings of a new hash database:\n"
  << "    -p, --hash_block_size=<hash block size>\n"
  << "        <hash block size>, in bytes, used to generate hashes (default " << s.hash_block_size << ")\n"
  << "\n"
  << "    -m, --max_duplicates=<maximum>\n"
  << "        <maximum> number of hash duplicates allowed, or 0 for no limit\n"
  << "        (default " << s.maximum_hash_duplicates << ")\n"
  << "\n"
  << "    -t, --storage_type=<storage type>\n"
  << "        <storage type> to use in the hash database, where <storage type>\n"
  << "        is one of: btree | hash | red-black-tree | sorted-vector\n"
  << "        (default " << s.map_type << ")\n"
  << "\n"
  << "    -a, --algorithm=<hash algorithm>\n"
  << "        <hash algorithm> in use for the hash database, where <hash algorithm>\n"
  << "        is one of: md5 | sha1 | sha256\n"
//  << "        (default " << s.map_type << ")\n"
  << "        (default md5)\n"
  << "\n"
  << "    -i, --bits=<number of index bits>\n"
  << "        <number of index bits> to use for the source lookup index, between\n"
  << "        32 and 40 (default " << (uint32_t)s.number_of_index_bits << ")\n"
  << "        The number of bits used for the hash block offset value is\n"
  << "        (64 - <number of index bits>).\n"
  << "\n"
  << "<bloom filter settings> tune performance during hash queries:\n"
  << "    --b1 <state>\n"
  << "        sets bloom filter 1 <state> to enabled | disabled (default " << bloom_state_to_string(s.bloom1_is_used) << ")\n"
  << "    --b1n <n>\n"
  << "        expected total number <n> of unique hashes (default " << approximate_M_to_n(s.bloom1_M_hash_size) << ")\n"
  << "    --b1kM <k:M>\n"
  << "        number of hash functions <k> and bits per hash <M> (default <k>=" << s.bloom1_k_hash_functions << "\n"
  << "        and <M>=" << s.bloom1_M_hash_size << " or <M>=value calculated from value in --b1n)\n"
  << "    --b2 <state>\n"
  << "        sets bloom filter 1 <state> to enabled | disabled (default " << bloom_state_to_string(s.bloom2_is_used) << ")\n"
  << "    --b2n <total>\n"
  << "        expected total number <n> of unique hashes (default " << approximate_M_to_n(s.bloom2_M_hash_size) << ")\n"
  << "    --b2kM <k:M>\n"
  << "        number of hash functions <k> and bits per hash <M> (default <k>=" << s.bloom2_k_hash_functions << "\n"
  << "        and <M>=" << s.bloom2_M_hash_size << " or <M>=value calculated from value in --b2n)\n"
  << "\n"
  << "<path or socket> selects how to connect to the hashdb server service.\n"
  << "        Valid paths are filesystem paths.\n"
  << "        Valid sockets are TCP sockets, for example 'tcp://localhost:14500'.\n"
  << "        Note: socket selections may not be available in early beta versions.\n"
  ;
}

void detailed_usage() {
  hashdb_settings_t s;
  hashdb_runtime_options_t o;

  // print usage notes and examples
  std::cout
  << "\n"
  << "Notes:\n"
  << "Using the md5deep tool to generate hash data:\n"
  << "hashdb imports hashes from DFXML files that contain cryptographic\n"
  << "hashes of hash blocks.  These files can be generated using the md5deep tool\n"
  << "or by exporting a hash database using the hashdb \"export\" command.\n"
  << "When using the md5deep tool to generate hash data, the \"-p <partition size>\"\n"
  << "option must be set to the desired hash block size.  This value must match\n"
  << "the hash block size that hashdb expects or else no hashes will be\n"
  << "copied in.  The md5deep tool also requires the \"-d\" option in order to\n"
  << "instruct md5deep to generate output in DFXML format.\n"
  << "\n"
  << "Selecting an optimal hash database storage type:\n"
  << "The storage type option, \"-t\", selects the storage type to use in the\n"
  << "hash database.  Each storage type has advantages and disadvantages:\n"
  << "    btree           Provides fast build times, fast access times, and is\n"
  << "                    fairly compact.\n"
  << "                    Currently, btree may have threading issues and may\n"
  << "                    crash when performing concurrent queries.\n"
  << "\n"
  << "    hash            Provides fastest query times and is very compact,\n"
  << "                    but is very slow during building.  We recommend\n"
  << "                    building a hash database using the btree storage type,\n"
  << "                    and, once built, copying it to a new hash database\n"
  << "                    using the hash storage type option.\n"
  << "\n"
  << "    red-black-tree  Similar in performance to btree, but not as fast or\n"
  << "                    compact.\n"
  << "\n"
  << "    sorted-vector   Similar in performance to hash.\n"
  << "\n"
  << "Selecting the hash algorithm:\n"
  << "The hash alogirthm option, \"-a\", selects the algorighm in use for the hash\n"
  << "database.  Available hash algorithms are md5, sha1, and sha256.\n"
  << "\n"
  << "Improving query speed by using Bloom filters:\n"
  << "Bloom filters can speed up performance during hash queries by quickly\n"
  << "indicating if a hash value is not in the hash database.  When the Bloom\n"
  << "filter indicates that a hash value is not in the hash database, an actual\n"
  << "hash database lookup is not required, and time is saved.  If the Bloom\n"
  << "filter indicates that the hash value may be in the hash database, a hash\n"
  << "database lookup is required and no time is saved.\n"
  << "\n"
  << "Bloom filters can be large and can take up lots of disk space and memory.\n"
  << "A Bloom filter with a false positive rate between 1\% and 10\% is effictive.\n"
  << "If the false-positive rate is low, the Bloom filter is unnecessarily large,\n"
  << "and it could be smaller.  If the false-positive rate is too high, there\n"
  << "will be so many false positives that hash database lookups will be required\n"
  << "anyway, defeating the value of the bloom filter.\n"
  << "\n"
  << "Up to two Bloom filters may be used.  The idea of using two is that the\n"
  << "first would be smaller and would thus be more likely to be fully cached\n"
  << "in memory.  If the first Bloom filter indicates that the hash may be present,\n"
  << "then the second bloom filter, which should be larger, is checked.  If the\n"
  << "second Bloom filter indicates that the hash may be present, then a hash\n"
  << "database lookup is required to be sure.\n"
  << "\n"
  << "Performing hash queries using the hashid scanner with bulk_extractor:\n"
  << "bulk_extractor may be used to scan the hash database for matching\n"
  << "cryptographic hashes if the hashid scanner is configured and enabled.\n"
  << "The hashid scanner runs either as a client with hashdb running as\n"
  << "a server to perform hash queries, or loads the hash database drectly and\n"
  << "performs queries directly.  The hashid scanner takes parameters from\n"
  << "bulk_extractor using bulk_extractor's \"-S name=value\" control parameter.\n"
  << "hashid accepts the following parameters:\n"
  << "\n"
  << "   -S path_or_socket=<path or socket>   server service selection\n"
  << "   -S action=<action>    Select the scanner action, where action is one of\n"
  << "      'query_hash', or 'import'\n"
  << "   -S algorithm=<hash algorithm>   cryptographic hash to use, one of md5,\n"
  << "      sha1, or sha256\n"
  << "   -S hash_block_size=4096    Hash block size, in bytes, used to generate\n"
  << "      cryptographic hashes\n"
  << "   -S sector_size=512    Sector size, in bytes\n"
  << "      Hashes are generated on each sector_size boundary\n"
  << "   -S repository_name=<repository name> repository name used when importing\n"
  << "\n"
  << "Improving startup speed by keeping a hash database open:\n"
  << "In the future, a dedicated provision may be created for this, but for now,\n"
  << "the time required to open a hash database may be avoided by keeping a\n"
  << "persistent hash database open by starting a hash database query server\n"
  << "service and keeping it running.  Now this hash database will open quickly\n"
  << "for other query services because it will already be cached in memory.\n"
  << "Caution, though, do not change the contents of a hash database that is\n"
  << "opened by multiple processes because this will make the copies inconsistent.\n"
  << "\n"
/* no, the user will only need to be aware of cryptographic hashes.
  << "Overloaded uses of the term \"hash\":\n"
  << "The term \"hash\" is overloaded and can mean any of the following:\n"
  << "   The cryptographic hash value being recorded in the hash database, such as\n"
  << "   an MD5 hash.\n"
  << "   The hash storage type, such as a B-Tree,  used for storing information in\n"
  << "   the hash database.\n"
  << "   The hash that the unordered map hash storage type uses in order to map\n"
  << "   a cryptographic hash record onto a hash storage slot.\n"
  << "   The hash that the Bloom filter uses to map onto a specific bit within\n"
  << "   the Bloom filter.\n"
  << "\n"
*/
  << "Log files:\n"
  << "Commands that create or modify a hash database produce a log file in the\n"
  << "hash database directory called \"log.xml\".  Currently, the log file is\n"
  << "replaced each time.  In the future, log entries will append to existing\n"
  << "content.\n"
  << "\n"
/* no, fixed in 0.9.1.
  << "Known bugs:\n"
  << "Performing hash queries in a threaded environment using the btree storage\n"
  << "type causes intermittent crashes.  This was observed when running the\n"
  << "bulk_extractor hashid scanner when bulk_extractor was scanning recursive\n"
  << "directories.  This bug will be addressed in a future release of boost\n"
  << "btree.\n"
  << "\n"
*/
  << "Examples:\n"
  << "This example uses the md5deep tool to generate cryptographic hashes from\n"
  << "hash blocks in a file, and is suitable for importing into a hash database\n"
  << "using the hashdb \"copy\" command.  Specifically:\n"
  << "\"-p 4096\" sets the hash block partition size to 4096 bytes.\n"
  << "\"-d\" instructs the md5deep tool to produce output in DFXML format.\n"
  << "\"my_file\" specifies the file that cryptographic hashes will be generated\n"
  << "for.\n"
  << "The output of md5deep is directed to file \"my_dfxml_file\".\n"
  << "    md5deep -p 4096 -d my_file > my_dfxml_file\n"
  << "\n"
  << "This example uses the md5deep tool to generate hashes recursively under\n"
  << "subdirectories, and is suitable for importing into a hash database using\n"
  << "the hashdb \"copy\" command.  Specifically:\n"
  << "\"-p 4096\" sets the hash block partition size to 4096 bytes.\n"
  << "\"-d\" instructs the md5deep tool to produce output in DFXML format.\n"
  << "\"-r mydir\" specifies that hashes will be generated recursively under\n"
  << "directory mydir.\n"
  << "The output of md5deep is directed to file \"my_dfxml_file\".\n"
  << "    md5deep -p 4096 -d -r my_dir > my_dfxml_file\n"
  << "\n"
  << "This example creates a new hash database named my_hashdb with default settings:\n"
  << "    hashdb create my_hashdb\n"
  << "\n"
  << "This example imports hashes from DFXML input file my_dfxml_file to hash\n"
  << "database my_hashdb, categorizing the hashes as sourced from repository\n"
  << "\"my repository\":\n"
  << "    hashdb import -r \"my repository\" my_dfxml_file my_hashdb\n"
  << "\n"
  << "This example exports hashes in my_hashdb to new DFXML file my_dfxml:\n"
  << "    hashdb export my_hashdb my_dfxml\n"
  << "\n"
  << "This example copies hashes from hash database my_hashdb1 to hash database\n"
  << "my_hashdb2.\n"
  << "    hashdb copy my_hashdb1 my_hashdb2\n"
  << "\n"
  << "This example merges my_hashdb1 and my_hashdb2 into new hash database\n"
  << "my_hashdb3:\n"
  << "    hashdb merge my_hashdb1 my_hashdb2 my_hashdb3\n"
  << "\n"
  << "This example removes hashes in my_hashdb1 from my_hashdb2:\n"
  << "    hashdb remove my_hashdb1 my_hashdb2\n"
  << "\n"
  << "This example removes hashes in my_dfxml from my_hashdb:\n"
  << "    hashdb remove_dfxml my_dfxml my_hashdb\n"
  << "\n"
  << "This example removes all hashes in my_hashdb that appear more than once:\n"
  << "    hashdb deduplicate my_hashdb\n"
  << "\n"
  << "This example rebuilds the Bloom filters for hash database my_hashdb to\n"
  << "optimize it to work well with 50,000,000 different hash values:\n"
  << "    hashdb rebuild_bloom --b1n 50000000 my_hashdb\n"
  << "\n"
  << "This example starts hashdb as a server service for the hash database at\n"
  << "path my_hashdb at socket endpoint \"tcp://*:14500\":\n"
  << "    hashdb server my_hashdb tcp://*:14500\n"
  << "\n"
  << "This example searches the hashdb server service available at socket\n"
  << "tcp://localhost:14500 for hashes that match those in DFXML file my_dfxml\n"
  << "and directs output to stdout:\n"
  << "    hashdb query_hash tcp://localhost:14500 my_dfxml\n"
  << "\n"
  << "This example searches the hashdb server service at file path my_hashdb\n"
  << "for hashes that match those in DFXML file my_dfxml and directs output\n"
  << "to stdout:\n"
  << "    hashdb query_hash my_hashdb my_dfxml\n"
  << "\n"
  << "This example uses the hashdb server service at file path my_hashdb\n"
  << "and input file identified_blocks.txt to generate output file\n"
  << "my_identified_blocks_with_source_info.txt:\n"
  << "to stdout.\n"
  << "    hashdb get_hash_source my_hashdb identified_blocks.txt\n"
  << "           my_identified_blocks_with_source_info.txt\n"
  << "\n"
  << "This example prints out information about the hash database provided as\n"
  << "a hashdb server service at file path my_hashdb:\n"
  << "    hashdb get_hashdb_info my_hashdb\n"
  << "\n"
  << "This example creates new hash database my_hashdb using various tuning\n"
  << "parameters.  Specifically:\n"
  << "\"-p 512\" specifies that the hash database will contain hashes for data\n"
  << "hashed with a hash block size of 512 bytes.\n"
  << "\"-m 2\" specifies that when there are duplicate hashes, only the first\n"
  << "two hashes of a duplicate hash value will be copied.\n"
  << "\"-t hash\" specifies that hashes will be recorded using the \"hash\" storage\n"
  << "type algorithm.\n"
  << "\"-a sha1\" specifies that SHA1 hashes will be stored in the hash database.\n"
  << "type algorithm.\n"
  << "\"-n 4\" specifies that, internal to the hash database, hash values will be\n"
  << "sharded across four files.\n"
  << "\"-i 34\" specifies that 34 bits are allocated for the source lookup index,\n"
  << "allowing 2^34 entries of source lookup data.  Note that this leaves 2^30\n"
  << "entries remaining for hash block offset values.\n"
  << "\"--b1 enabled\" specifies that Bloom filter 1 is enabled.\n"
  << "\"--b1n 50000000\" specifies that Bloom filter 1 should be sized to expect\n"
  << "50,000,000 different hash values.\n"
  << "\"--b2 enabled\" specifies that Bloom filter 2 is enabled.\n"
  << "\"--b2kM 4:32 enabled\" specifies that Bloom filter 2 will be configured to\n"
  << "have 4 hash functions and that the Bloom filter hash function size will be\n"
  << "32 bits, consuming 512MiB of disk space.\n"
  << "    hashdb create -p 512 -m 2 -t hash -a sha1 -n 4 -i 34 --b1 enabled\n"
  << "                --b1n 50000000 --b2 enabled --b2kM 4:32 my_hashdb1 my_hashdb2\n"
  << "\n"
  << "This example rebuilds the Bloom filters for hash database my_hashdb to\n"
  << "optimize it to work well with 50,000,000 different hash values:\n"
  << "    hashdb rebuild_bloom --b1n 50000000 my_hashdb\n"
  << "\n"
  << "This example uses bulk_extractor to run the hashid scanner to scan for\n"
  << "hash values in a media file where the hash queries are performed\n"
  << "locally from a hashdb database that is opened by the hashid scanner.\n"
  << "Parameters to bulk_extractor for this example follow:\n"
  << "\"-S path_or_socket=my_hashdb\" tells the scanner to perform hash queries\n"
  << "using hash database at local file path my_hashdb.\n"
  << "\"-S action=query_hash\" tells the scanner to scan for matching hash values.\n"
  << "\"-S hash_block_size=4096\" tells the scanner to create cryptographic hashes\n"
  << "on 4096-byte chunks of data.\n"
  << "\"-S sector_size=512\" tells the scanner to create cryptographic hashes at\n"
  << "every 512-byte sector boundary.\n"
  << "\"-o scanner_output\" tells bulk_extractor to put scanner output into the\n"
  << "scanner_output directory.\n"
  << "File \"my_imagefile\" is the name of the image file that the scanner will use.\n"
  << "Specifically, the scanner will create hashes from hash blocks at each\n"
  << "sector boundary.\n"
  << "                   bulk_extractor\n"
  << "                   -S path_or_socket=my_hashdb\n"
  << "                   -S action=query_hash\n"
  << "                   -S hash_block_size=4096\n"
  << "                   -S sector_size=512\n"
  << "                   -o scanner_output\n"
  << "                   my_imagefile\n"
  << "\n"
  << "This example uses bulk_extractor to run the scan_hashid scanner to scan\n"
  << "for hash values in a media file where the hash queries are performed\n"
  << "remotely using a hash database query server service available at socket\n"
  << "endpoint tcp://localhost:14500.\n"
  << "This example is similar to the previous example.\n"
  << "\"-S socket=tcp://localhost:14500\" sets the socket so that queries use a\n"
  << "hashdb query server at socket endpoint \"tcp://localhost:14500\".\n"
  << "hashdb must be running and available at\n"
  << "socket endpoint \"tcp://*:14500\" or else this example will fail because\n"
  << "a server service is not available.  Please see the example for starting\n"
  << "hashdb as a server query service.\n"
  << "                   bulk_extractor\n"
  << "                   -S path_or_socket=tcp://*:14500\n"
  << "                   -S action=query_hash\n"
  << "                   -S hash_block_size=4096\n"
  << "                   -S sector_size=512\n"
  << "                   -o scanner_output\n"
  << "                   my_imagefile\n"
  << "\n"
  << "This example uses bulk_extractor to import hashes from image file\n"
  << "my_imagefile into hash database my_hashdb.\n"
  << "\"-S action=import\" tells the scanner to import hashes.\n"
  << "\"-S repository_name=my_repository\" names the repository for the set of\n"
  << "hashes being imported.\n"
  << "                   bulk_extractor\n"
  << "                   -S path_or_socket=my_hashdb\n"
  << "                   -S action=import\n"
  << "                   -S hash_block_size=4096\n"
  << "                   -S repository_name=my_repository\n"
  << "                   -o scanner_output\n"
  << "                   my_imagefile\n"
  ;
}

#endif

